---
title: Joint Distributions
author: "CDS DS-122<br>Boston University"
format: 
    revealjs:
        math: true
        css: 
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true 
---


## Dating Pool Illusion
:::{style="font-size: .8em"}
Imagine evaluating people in a dating pool based on two traits:

- Kindness (nice vs. mean)
- Appearance (attractive vs. unattractive)

You might notice that the more attractive someone is, the less kind they seem.
Or vice versa: the kinder someone is, the less attractive they appear.<br>
<br>

_Does this mean that being attractive causes someone to be unkind?_ 

<br>

People are evenly distributed across both kindness and appearance. However, in practice, you only notice people who are either kind or attractive, or both. 

:::

## Dating Pool Illusion
:::{.center-text}
```{python}
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Create a figure and axis
fig, ax = plt.subplots(figsize=(6, 6))

# Draw the full grid (Great Square)
ax.add_patch(patches.Rectangle((0, 0), 1, 1, edgecolor='black', facecolor='lightgray', label='Complete Pool'))

# Draw the triangle of acceptable individuals (those who are nice, attractive, or both)
triangle = patches.Polygon([[0, 1], [1, 1], [1, 0]], closed=True, color='skyblue', label='Observed Subset')
ax.add_patch(triangle)

# Add labels to the axes
ax.set_xticks([0, 1])
ax.set_xticklabels(['Unattractive', 'Attractive'])
ax.set_yticks([0, 1])
ax.set_yticklabels(['Mean', 'Nice'])
plt.yticks(fontsize=18)
plt.xticks(fontsize=18)

# Add grid lines
ax.grid(True, linestyle='--', alpha=0.5)

# Add title and legend
# ax.set_title("Berkson's Fallacy: Selection Bias in Dating Pool")
# ax.legend(loc='upper left')
plt.legend(loc='best', prop={'size': 18})

# Set limits and aspect ratio
ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_aspect('equal')

# Save the figure
plt.savefig("berksons_fallacy_diagram.png")
plt.show()
```
:::




## Learning Objectives

:::{style="font-size: .8em"}

* Joint distributions
* Marginal distributions
* Indepent random variables
* Covariance 
* Correlation coefficient

:::


## Like Father, Like Son?

:::{.center-text}
```{python}
# Import necessary library
import pandas as pd

# Load the dataset
df = pd.read_csv("data/father_son_heights.csv")

# Display the first few rows
print(df.head(6))
```
:::

:::{style="font-size: .8em"}
The properties of the dataset: <br>

- 100 rows/observations,
- 2 columns/variables,
- Heights are measured in inches

__Question__: Explore the relationship between father and son heights. Do taller fathers usually have taller sons?
:::


## Joint Distributions
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Joint Distribution </span>
        <p>
       A joint distribution describes the probability of two or more random variables taking on specific values at the same time.
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

__Example__: We analyze a student population using two categorical variables:

- _StudyEffort_: {low, high}  
- _Grade_: {A, B, C}
<!-- 
To understand how these attributes relate, we define their _joint distribution_. -->

| |low|high|
|---|---|---|
|__A__| 0.07| 0.18|
|__B__| 0.28| 0.09|
|__C__|0.35| 0.03|

:::

## Marginal Distributions
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Marginal Distribution </span>
        <p>
       The marginal distribution of a discrete random variable is the probability distribution of that variable considered on its own. It is obtained by summing over the joint distribution with respect to the other variable(s).
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
Mathematically, we write: $\small{P(X) = \sum_y P(X, Y = y).}$

__Example__:
 <!-- We could also be interested in each characteristic on its own. For example, we might be interested in the distribution of _StudyEffort_ across our population. -->

<!-- This is called a _marginal_ distribution. -->

<!-- It gets this name because it corresponds to the __marginal sums__ of the probability distribution.   -->



| |low|high|marginal|
|---|---|---|---|
|__A__| 0.07| 0.18|0.25|
|__B__| 0.28| 0.09|0.37|
|__C__|0.35| 0.03|0.38 |
|__marginal__| 0.70 | 0.30| |
:::

## Marginal Distributions
:::{style="font-size: .8em"}
__Example__: A fair coin is tossed three times. Let $X$ denote the number of heads on the first toss and $Y$ the total number of heads. The sample space is then equal to

$$\Omega = \{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}.$$

From the sample space we see that the joint probability mass (or frequency) function of $X$ and $Y$ is as given in the following table:

| | $y = 0$ |$y = 1$| $y = 2$| $y = 3$|
| :---: | :---: | :---: | :---: | :---: |
| $x = 0$ | $\frac{1}{8}$ | $\frac{2}{8}$ | $\frac{1}{8}$ | 0 |
| $x = 1$ | 0  | $\frac{1}{8}$ | $\frac{2}{8}$ | $\frac{1}{8}$ |

<span style="color:rgb(1, 180, 180);">What is the marginal PMF of $Y$?</span>
:::

## Marginal Distributions
:::{style="font-size: .8em"}
Here is a set of observations of two random variables, $X_1$ and $X_2$:
:::
:::{.center-text}
```{python}
import seaborn as sns
import numpy as np
import pandas as pd

np.random.seed(4)
from scipy.stats import multivariate_normal
df1 = pd.DataFrame(multivariate_normal.rvs(mean = np.array([1, 1]), cov = np.array([[1, 0.8],[0.8, 1]]), 
                                           size = 600),
                  columns = ['X1', 'X2'])

plt.figure(figsize = (4.5,4.5))
sns.scatterplot(data = df1, x = 'X1', y = 'X2')
plt.xlabel(r'$X_1$', fontsize=18);
plt.ylabel(r'$X_2$', fontsize=18);
plt.yticks(fontsize=18)
plt.xticks(fontsize=18)
plt.axis('square');
plt.tight_layout()
```
:::
:::{style="font-size: .8em"}
We can calculate the marginals by summing over the joint probabilities.
:::

## Marginal Distributions
:::{.center-text}
```{python}
#| echo: true
g = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 4.5)
g.plot(sns.histplot, sns.histplot)
g.ax_joint.set_xlabel(r'$X_1$', fontsize=16)
g.ax_joint.set_ylabel(r'$X_2$', fontsize=16);
```
:::

## Independence
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Independent Random Variables </span>
        <p>
       Random variables \(X\) and \(Y\) are independent if their joint cdf \(F\) factors into the product of their marginal cdfâ€™s, \(F_{X}\) and \(F_{Y}\):

       $$\small{F(x,y) = F_{X}(x)F_{Y}(y)}$$ 

       or equivalently,

       $$\small{P(X\leq x,Y \leq y) = P(X\leq x)P(Y \leq y).}$$ 
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Remark__: Two random variables $X$ and $Y$, forming a discrete random vector, are independent if and only if
$$p_{XY}(x,y) = p_X(x)p_Y(y),$$
where $p_{XY}$ is their joint PMF and $p_{X}$ and $p_{Y}$ are their marginal PMFs.
:::

## Independence
:::{style="font-size: .8em"}
__Example__: Let us investigate whether _Grade_ and _StudyEffort_ are independent.
:::
:::{style="font-size: .7em"}

Starting from the marginals 

|low|high|
|---|---|
| 0.70| 0.30|

and

|A|B|C|
|---|---|---|
| 0.25| 0.37| 0.38|

we compute the new distribution based on the independence assumption:

| |low|high|
|---|---|---|
|A| 0.175| 0.075|
|B| 0.259| 0.111|
|C|0.266| 0.114|
:::

## Independence
:::{style="font-size: .8em"}
__Example (continued)__:
Since the resulting ditribution is not the same as the original joint distribution,  _Grade_ and _StudyEffort_ are not independent.

__Example__: 

:::{.center-text}
```{python}
plt.figure(figsize = (4.5,4.5))
sns.scatterplot(data = df1, x = 'X1', y = 'X2')
plt.xlabel(r'$X_1$', fontsize=18);
plt.ylabel(r'$X_2$', fontsize=18);
plt.axis('square');
```
<span style="color:rgb(1, 180, 180);">Are $X_1$ and $X_2$ independent?</span>
:::

:::

## Independence
:::{style="font-size: .8em"}
__Example__: The datasets below have approximately the same marginals. 

:::{.columns}
::: {.column width="50%"}
```{python}
np.random.seed(4)
from scipy.stats import multivariate_normal
df2 = pd.DataFrame(multivariate_normal.rvs(mean = np.array([1, 1]), cov = np.array([[1, 0],[0, 1]]), 
                                           size = 600),
                  columns = ['X1', 'X2'])
#plt.figure(figsize=(4.5, 4.5))
g = sns.JointGrid(data=df2, x='X1', y='X2', height=5)
g.plot(sns.scatterplot, sns.histplot)
g.ax_joint.set_xlabel(r'$X_1$', fontsize=18)
g.ax_joint.set_ylabel(r'$X_2$', fontsize=18)
g.figure.subplots_adjust(top=0.9)
plt.suptitle('$X_1$ and $X_2$ Independent', fontsize=20)
plt.show()

```
:::

::: {.column width="50%"}
```{python}
g = sns.JointGrid(data = df1, x = 'X1', y = 'X2', 
                   height = 5)
g.plot(sns.scatterplot, sns.histplot)
g.ax_joint.set_xlabel(r'$Y_1$', fontsize=18)
g.ax_joint.set_ylabel(r'$Y_2$', fontsize=18)
g.figure.subplots_adjust(top=0.9) # Reduce plot to make room 
plt.suptitle('$Y_1$ and $Y_2$ Not Independent', fontsize=20);
plt.show()
```
:::
:::
:::

## Covariance
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Covariance </span>
        <p>
       The covariance of \(X_1\) and \(X_2\), denoted \(\operatorname{Cov}(X_1, X_2)\) is defined as:
    
        $$\small{ \operatorname{Cov}(X_1, X_2) = E[(X_1 - \overline{X_1})(X_2 - \overline{X_2})] }$$

        where \(\overline{X_1}\) and \(\overline{X_2}\) are the means of \(X_1\) and \(X_2.\)
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Notation__: We often denote $\operatorname{Cov}(X,Y)$ as $\sigma_{XY}$.

__Remark__: The covariance of two continuous random variables is outside the scope of this course.

We know how to find the expected value of a random variable. But how exactly do we compute 

$$\operatorname{Cov}(X_1, X_2) = E[(X_1 - \overline{X_1})(X_2 - \overline{X_2})]?$$
:::

## Covariance
:::{style="font-size: .8em"}
For discrete $X_1$ and $X_2$ with joint probability $P(X_1=x_1, X_2=x_2) = p(x_1,x_2)$ the covariance is computed as

$$ \operatorname{Cov}(X_1, X_2) = \sum_{(x_1,x_2)\in S} (x_1 - \overline{X_1})(x_2 - \overline{X_2}) p(x_1,x_2). $$

Here, $S$ is the _support_ of $X_1$ and $X_2$.

__Example__: Suppose that $X$ and $Y$ have the joint probability mass function given below.

| | $x = 5$ |$x = 6$| $x = 7$| 
| :---: | :---: | :---: | :---: | 
| $y = 8$ | 0 | 0.4 | 0.1 |
| $y = 9$ | 0.3  | 0 | 0.2 | 
:::

## Covariance
:::{style="font-size: .8em"}
__Example (continued)__: The support $S$ is equal to the set $\{(5,8),(6,8),(7,8), (5,9),(6,9),(7,9)\}.$

The marginal probabilities are given by

| | $x = 5$ |$x = 6$| $x = 7$| 
| :---: | :---: | :---: | :---: | 
| $p_X(x)$ | 0.3 | 0.4 | 0.3 |

| | $y = 8$ |$y = 9$| 
| :---: | :---: | :---: | 
| $p_Y(y)$ | 0.5 | 0.5 | 

The mean of $X$ can be computed using $p_X(x)$ as follows

$$\overline{X} = 5 \cdot 0.3 + 6 \cdot 0.4 + 7 \cdot 0.3 = 6.$$

Similarly, we find that $\overline{Y} = 8.5.$

:::

## Covariance
:::{style="font-size: .8em"}
__Example (continued)__: Now it remains to compute the covariance itself

$$\small{\operatorname{Cov}(X,Y) = (5-6)(8-8.5)(0) + (6-6)(8-8.5)(0.4) + (7-6)(8-8.5)(0.1)}$$

$$\small{+ (5-6)(9-8.5)(0.3) + (6-6)(9-8.5)(0) + (7-6)(9-8.5)(0.2) = -0.1.}$$

We typically "normalize" covariance by the standard deviations of the random variables. The result is known as "correlation coefficient" or "Pearson's correlation coefficient" or "Pearson's $\rho.$"

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Properties of Covariance </span>
        <p>
       $$\operatorname{Cov}(X,Y) = \operatorname{Cov}(Y,X),$$
       $$\operatorname{Var}(X) = \operatorname{Cov}(X,X).$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Correlation
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Correlation </span>
        <p>
       If \(X\) and \(Y\) are jointly distributed random variables and the variances and covariances of both \(X\) and \(Y\) exist and the variances are nonzero, then the correlation of \(X\) and \(Y\) , denoted by \( \rho\), is

       $$\rho(X_1,X_2) = \frac{E\left[(X_1-\overline{X_1})(X_2-\overline{X_2})\right]}{\sigma_{X_1} \sigma_{X_2}}.$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

Because it is normalized, $\rho(X_1, X_2)$ takes on values between -1 and 1.   

If $\rho(X_1, X_2) = 0$ then $X_1$ and $X_2$ are _uncorrelated_.   

If $\rho(X_1, X_2) > 0$ then $X_1$ and $X_2$ are _positively correlated_, and if $\rho(X_1, X_2) < 0$ they are _negatively correlated_ or (_anticorrelated_).
:::

## Correlation
:::{style="font-size: .8em"}
If $X_1$ and $X_2$ are uncorrelated, are they independent?

In general, __NO__.

If $X_1$ and $X_2$ are uncorrelated, that means the relationship between them can't be predicted by _a linear model._

However, this does not mean that they are necessarily independent!
:::

## Like Father, Like Son?

:::{.center-text}
```{python}
#| echo: true
# Compute covariance between Father_Height and Son_Height
covariance = df['Father_Height'].cov(df['Son_Height'])
print(f"The covariance between the heights is {covariance:.4f}")

# Compute correlation between Father_Height and Son_Height
correlation = df['Father_Height'].corr(df['Son_Height'])
print(f"The correlation between the heights is {correlation:.4f}")
```
:::

:::{style="font-size: .8em"}
The positive covariance indicates that taller fathers tend to have taller sons.

The correlation of 0.42 suggests a moderate positive linear relationship between father and son heights.
:::

## Group Question 1

:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<img src="images/joint distributions/PMF_table.png" width=900/><br>

\(X\) and \(Y\) are discrete random variables. Both \(X\) and \(Y\) can assume values 0, 1, and 2. The incomplete table above is meant to depict the joint distribution of \(X\) and \(Y\), and the marginal distributions of \(X\) and \(Y.\)<br>

a. Complete the table. <br>
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

## Group Question 1 

:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
b. Find the expected value of \(X\) and the expected value of \(Y\).<br>
    <br>
    <br>
    <br>
c. Find the covariance of \(X\) and \(Y\). <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
d. Are \(X\) and \(Y\) independent? Explain your answer.
    <br>
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
This is quesion 8 from the practice exam for Fall 24.
:::