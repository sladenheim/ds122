---
title: Classification & Naive Bayes
author: "CDS DS-122<br>Boston University"
format:
    revealjs:
        math: true
        css:
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import norm, multivariate_normal

def update(distribution, likelihood):
    '''Standard Bayesian update function'''
    distribution['probs'] = distribution['probs'] * likelihood
    prob_data = distribution['probs'].sum()
    distribution['probs'] = distribution['probs'] / prob_data
    return distribution

```

## Learning Objectives
:::{style="font-size: .8em"}

- Explain what a **classification problem** is 
- Implement a **Naive Bayes classifier** using Bayesian updating
- Understand **the "naive" assumption** and why it matters
- Start using **continuous distributions for likelihood**


:::
## <span style="font-size: 0.85em">Warm-Up: Bird Watcher's Dilemma</span>

:::{style="font-size: .6em"}

You spotted a bird on the coast. It's either a **Puffin**, **Tern**, or **Gull**.

**Observation 1:** The bird dove underwater

- Puffins dive often: P(dive | Puffin) = 0.9
- Terns dive sometimes: P(dive | Tern) = 0.6
- Gulls rarely dive: P(dive | Gull) = 0.3

**Observation 2:** The bird has a colorful beak

- Puffins have colorful beaks: P(colorful beak | Puffin) = 0.8
- Terns have plain beaks: P(colorful beak | Tern) = 0.2
- Gulls have plain beaks: P(colorful beak | Gull) = 0.2

**Our task:**

- Starting with equal priors (1/3 each), identify the bird.
- Did the order of our observations matter?


:::


## What is Classification?

:::{style="font-size: .8em"}

**Classification:** Predicting which category/class an observation belongs to

**Examples:**

- What type of email is this: spam, promotional, social, or important?
- Is this tumor benign or malignant?
- Is this transaction fraudulent?


**Classification is one of the most common applications of Bayesian methods!**

:::

## Penguin Classification

:::{style="font-size: .8em"}

We'll work with something cuter - classifying penguins into three species using physical measurements.

::: {.columns}
::: {.column width="33%"}
![**Adélie Penguin** - Smallest, classic "tuxedo" look](images/classification/adelie.jpg){width=200}
:::
::: {.column width="33%"}
![**Chinstrap Penguin** - Named for black facial stripe](images/classification/chinstrap.jpg){width=200}
:::
::: {.column width="33%"}
![**Gentoo Penguin** - Largest, orange beak](images/classification/gentoo.png){width=200}
:::
:::

:::

## The Palmer Penguins Dataset

:::{style="font-size: .8em"}

Data on 344 penguins found on the Palmer Archipelago in the Antarctic, including flipper and culmen (beak) length:


```{python}
# Load the Palmer Penguins dataset
# You can install with: pip install palmerpenguins
try:
    from palmerpenguins import load_penguins
    df = load_penguins()
    # Rename columns to match our convention
    df = df.rename(columns={
        'flipper_length_mm': 'Flipper Length (mm)',
        'bill_length_mm': 'Culmen Length (mm)',
        'species': 'Species'
    })
    df['Species'] = df['Species'].str.capitalize()
except ImportError:
    # Fallback: load from local CSV if palmerpenguins not installed
    df = pd.read_csv('images/classification/penguins.csv')
    df['Species'] = df['Species'].str.split(' ').str[0]

# Show first few rows
df[['Species', 'Flipper Length (mm)', 'Culmen Length (mm)']].head(7)
```

:::

## Visualizing the Data

How can we classify these?  (What do you think?)

:::{style="font-size: .7em"}

```{python}
#| echo: false
#| fig-align: center

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Flipper length by species
for species in df['Species'].unique():
    data = df[df['Species'] == species]['Flipper Length (mm)'].dropna()
    axes[0].hist(data, alpha=0.6, label=species, bins=15)
axes[0].set_xlabel('Flipper Length (mm)', size=12)
axes[0].set_ylabel('Count', size=12)
axes[0].set_title('Flipper Length by Species', size=14)
axes[0].legend()

# Culmen length by species
for species in df['Species'].unique():
    data = df[df['Species'] == species]['Culmen Length (mm)'].dropna()
    axes[1].hist(data, alpha=0.6, label=species, bins=15)
axes[1].set_xlabel('Culmen Length (mm)', size=12)
axes[1].set_ylabel('Count', size=12)
axes[1].set_title('Culmen Length by Species', size=14)
axes[1].legend()

plt.tight_layout()
plt.show()
```

:::

## The Bayesian Approach

:::{style="font-size: .8em"}

We're going to build a Naive Bayes Classifier using our usual approach:


1. **Prior:** What do we believe before seeing any data?
   - P(Adélie), P(Chinstrap), P(Gentoo)


2. **Likelihood:** How likely is the observed data under each hypothesis?
   - P(flipper length = 193 mm | Adélie)
   - P(culmen length = 48 mm | Chinstrap)
   - etc.


3. **Posterior:** Update beliefs using Bayes' theorem
   - P(Adélie | data), P(Chinstrap | data), P(Gentoo | data)


4. **Decision:** Choose the class with highest posterior probability

:::

## Step 1: The Prior

:::{style="font-size: .8em"}

Without any measurements, all species are equally likely.

```{python}
# Create prior distribution
species_list = df['Species'].unique()
prior = pd.DataFrame(species_list, columns=['Species'])
prior['probs'] = 1/len(species_list)
prior
```


**Question: When might we want a non-uniform prior?**

If we knew we were in a region where Gentoo penguins are rare, we might set P(Gentoo) lower!


:::

## Alternative: Empirical Bayes

:::{style="font-size: .8em"}

Instead of a uniform prior, we could use the data itself to inform our prior!

```{python}
#| echo: true
species_counts = df['Species'].value_counts()
species_proportions = species_counts / species_counts.sum()
species_proportions
```


**Empirical Bayes approach:** Use these proportions as priors

```{python}
# Create empirical prior
prior_empirical = pd.DataFrame(species_list, columns=['Species'])
prior_empirical['probs'] = [species_proportions[sp] for sp in species_list]
prior_empirical
```

:::

## <span style="font-size: 0.85em">The Empirical Bayes Controversy</span>

:::{style="font-size: .7em"}

**The concern:** We're using the same data to set the prior AND update it!


**Less controversial when:**

- Using **past data** to inform priors
- **Large datasets** so prior has low impact
- **Domain knowledge** supports the prior



**Our approach:** We'll use a uniform prior for this lecture (more conservative), but empirical Bayes is common in practice when you have lots of historical data!

:::

## <span style="font-size: 0.85em">Step 2: The Likelihood - Single Feature</span>

:::{style="font-size: .8em"}

For each species, we'll model flipper length as **normally distributed**.


**Key assumption:**
$$\text{Flipper length} \mid \text{Species} \sim \text{Normal}(\mu_{\text{species}}, \sigma_{\text{species}})$$

**How do we get** $\mu$ **and** $\sigma$**?** We estimate them from our data!

- $\mu_{\text{species}}$ = sample mean of flipper lengths for that species
- $\sigma_{\text{species}}$ = sample standard deviation for that species


```{python}
def make_norm_map(df, colname, by='Species'):
    """Create normal distribution for each species based on data."""
    norm_map = {}
    grouped = df.groupby(by)[colname]
    for species, group in grouped:
        mean = group.mean()
        std = group.std()
        norm_map[species] = norm(mean, std)
    return norm_map

# Create distributions for flipper length
flipper_map = make_norm_map(df, 'Flipper Length (mm)')
```



:::

## <span style="font-size: 1.0em">Visualizing the Likelihood</span>

:::{style="font-size: .7em"}

$$\text{Flipper length} \mid \text{Species} \sim \text{Normal}(\mu_{\text{species}}, \sigma_{\text{species}})$$

- For continuous variables, we use the **probability density function (PDF)** instead of a probability mass function. 
- The PDF value is NOT a probability! But it still measures "relative likelihood"

```{python}
#| fig-width: 10
#| fig-height: 4
#| fig-align: center

x = np.linspace(170, 240, 300)
plt.figure(figsize=(10, 4))

for species in flipper_map.keys():
    y = flipper_map[species].pdf(x)
    plt.plot(x, y, label=species, linewidth=2)

plt.xlabel('Flipper Length (mm)', size=12)
plt.ylabel('Likelihood (PDF)', size=12)
plt.title('Flipper Length Distributions by Species', size=14)
plt.legend()
plt.grid(alpha=0.3)
plt.show()
```

:::

## <span style="font-size: 1.0em">Likelihood for One Observation</span>

:::{style="font-size: .8em"}

**Observation:** Flipper length = 193 mm

```{python}
observed_flipper = 193

# Calculate likelihood for each species
likelihood = [flipper_map[species].pdf(observed_flipper)
              for species in species_list]

likelihood_df = pd.DataFrame({
    'Species': species_list,
    'Likelihood': likelihood
})
likelihood_df
```


Adélie and Chinstrap are most likely, Gentoo is very unlikely!

:::

## Step 3: The Update

:::{style="font-size: .8em"}

Apply Bayes' theorem to get the posterior:

$$P(\text{Species} \mid \text{data}) = \frac{P(\text{data} \mid \text{Species}) \cdot P(\text{Species})}{P(\text{data})}$$

```{python}
# Create comprehensive update table
update_table = pd.DataFrame({
    'Species': species_list,
    'Prior': prior['probs'].values,
    'Likelihood': likelihood,
})

# Calculate unnormalized posterior
update_table['Prior × Likelihood'] = update_table['Prior'] * update_table['Likelihood']

# Normalize to get posterior
normalizer = update_table['Prior × Likelihood'].sum()
update_table['Posterior'] = update_table['Prior × Likelihood'] / normalizer

update_table
```

:::

## Adding a Second Feature

:::{style="font-size: .8em"}

Flipper length alone doesn't distinguish Adélie from Chinstrap well.

Let's also use **culmen (beak) length**!

**Key idea:** The posterior becomes the prior for the second update!

```{python}
# Create distributions for culmen length
culmen_map = make_norm_map(df, 'Culmen Length (mm)')

# Observed culmen length
observed_culmen = 48

# Calculate likelihood for culmen length
likelihood_culmen = [culmen_map[species].pdf(observed_culmen)
                     for species in species_list]

# Create comprehensive update table
# The "Prior" here is actually the posterior from flipper length!
update_table_2 = pd.DataFrame({
    'Species': species_list,
    'Prior (from flipper)': update_table['Posterior'].values,
    'Likelihood (culmen)': likelihood_culmen,
})

# Calculate unnormalized posterior
update_table_2['Prior × Likelihood'] = update_table_2['Prior (from flipper)'] * update_table_2['Likelihood (culmen)']

# Normalize to get posterior
normalizer = update_table_2['Prior × Likelihood'].sum()
update_table_2['Posterior'] = update_table_2['Prior × Likelihood'] / normalizer

update_table_2
```


**Now we're confident:** It's a Chinstrap!

**Question** We updated flipper, then culmen. What if we reversed the order?

:::

## <span style="font-size: 1.0em">Sequential vs Joint Updates</span>

:::{style="font-size: .7em"}

We updated **sequentially** but we could have done it in **one joint update**!

**Sequential updating:**
$$P(\text{species} \mid \text{flipper, culmen}) \propto \underbrace{(\text{Prior} \times P(\text{flipper} \mid \text{species}))}_{\text{posterior after flipper}} \times P(\text{culmen} \mid \text{species})$$

The old posterior becomes the new prior for the next update.


**Joint updating:**
$$P(\text{species} \mid \text{flipper, culmen}) \propto \text{Prior} \times \underbrace{P(\text{flipper, culmen} \mid \text{species})}_{\text{joint likelihood}}$$

**Key insight - these are the SAME when features are independent!**

$$P(\text{species} \mid \text{flipper, culmen}) \propto \text{Prior} \times \underbrace{P(\text{flipper} \mid \text{species}) \times P(\text{culmen} \mid \text{species})}_{\text{splits due to independence}}$$


:::

## The "Naive" Assumption

:::{style="font-size: .7em"}

We just used two features **independently**:

1. Updated based on flipper length
2. Updated based on culmen length


**This assumes:** Flipper length and culmen length are **independent** given species

$$P(\text{flipper}, \text{culmen} \mid \text{species}) = P(\text{flipper} \mid \text{species}) \cdot P(\text{culmen} \mid \text{species})$$


**Is this true?**

Probably not! Larger penguins likely have both longer flippers AND longer beaks.

<br>

**So what happens if we DON'T make this assumption?**


:::

## Accounting for Correlations

:::{style="font-size: .8em"}

Instead of treating features independently, we can use a **multivariate normal distribution**.


**For each species:**

- **Mean vector:** $\boldsymbol{\mu} = [\mu_{\text{flipper}}, \mu_{\text{culmen}}]$
- **Covariance matrix:** $\boldsymbol{\Sigma}$ captures variances AND correlations


$$\begin{bmatrix} \text{Flipper Length} \\ \text{Culmen Length} \end{bmatrix} \mid \text{Species} \sim \text{MVN}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$$

:::

## <span style="font-size: 1.0em">Reminder: Bivariate Normal</span>

:::{style="font-size: .6em"}

```{python}
#| echo: false
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Bivariate Normal Distribution</span>
        <p>
        A <b>bivariate normal</b> models two correlated variables simultaneously.
        <br><br>
        <b>The PDF (matrix form):</b>
        $$f(\mathbf{x}) = \frac{1}{2\pi|\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)$$
        where:
        <ul>
        <li>\(\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\) (the observed values)</li>
        <li>\(\boldsymbol{\mu} = \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}\) (mean vector)</li>
        <li>\(\boldsymbol{\Sigma} = \begin{bmatrix} \sigma_1^2 & \rho\sigma_1\sigma_2 \\ \rho\sigma_1\sigma_2 & \sigma_2^2 \end{bmatrix}\) (covariance matrix, where \(\rho\) = correlation)</li>
        </ul>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Visualizing the Covariance

:::{style="font-size: .8em"}

```{python}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

# Plot scatter for each species
fig, ax = plt.subplots(figsize=(10, 6))

for species in species_list:
    species_data = df[df['Species'] == species]
    ax.scatter(species_data['Flipper Length (mm)'],
               species_data['Culmen Length (mm)'],
               label=species, alpha=0.6, s=50)

ax.set_xlabel('Flipper Length (mm)', size=12)
ax.set_ylabel('Culmen Length (mm)', size=12)
ax.set_title('Flipper vs Culmen Length (Notice the correlation!)', size=14)
ax.legend()
ax.grid(alpha=0.3)
plt.show()
```


Notice: Longer flippers tend to go with longer beaks!


:::

## <span style="font-size: 1.0em">Computing Mean and Covariance</span>

:::{style="font-size: .8em"}

```{python}
# For Chinstrap penguins
chinstrap_data = df[df['Species'] == 'Chinstrap'][['Flipper Length (mm)',
                                                      'Culmen Length (mm)']].dropna()

# Mean vector
mean_chinstrap = chinstrap_data.mean()
print("Mean vector:")
print(mean_chinstrap)
print()

# Covariance matrix
cov_chinstrap = chinstrap_data.cov()
print("Covariance matrix:")
print(cov_chinstrap)
```


:::

## <span style="font-size: 0.85em">Defining our Multivariate Normals</span>

:::{style="font-size: .8em"}

```{python}
#| echo: true
def make_multinorm_map(df, colnames):
    multinorm_map = {}
    grouped = df.groupby('Species')
    for species, group in grouped:
        features = group[colnames].dropna()
        mean = features.mean()
        cov = features.cov()
        multinorm_map[species] = multivariate_normal(mean, cov)
    return multinorm_map

# Create multivariate normals
features = ['Flipper Length (mm)', 'Culmen Length (mm)']
multinorm_map = make_multinorm_map(df, features)
```


:::

## <span style="font-size: 0.85em">Classification with Multivariate Normal</span>

:::{style="font-size: .8em"}

Now we update **once** with both features together:
```{python}
#| echo: false
# Reset prior
prior = pd.DataFrame(species_list, columns=['Species'])
prior['probs'] = 1/len(species_list)
```

```{python}
#| echo: true
observed_data = [193, 48]
# Calculate likelihood for each species
# !!! One combined likelihood, not separated by feature
likelihood_mv = [multinorm_map[species].pdf(observed_data)
                 for species in species_list]
# Update
posterior_mv = prior.copy()
update(posterior_mv, likelihood_mv)
posterior_mv
```


Still clearly Chinstrap, but probabilities are different!

:::


## <span style="font-size: 0.85em">Visualizing Decision Boundaries</span>

:::{style="font-size: .8em"}

```{python}
#| fig-width: 10
#| fig-height: 6
#| echo: false
#| fig-align: center

def classify_naive_bayes(row, flipper_map, culmen_map, species_list):
    """Classify using naive Bayes."""
    # Start with uniform prior
    probs = np.ones(len(species_list)) / len(species_list)

    # Update with flipper length
    flipper_likes = [flipper_map[sp].pdf(row['Flipper Length (mm)'])
                     for sp in species_list]
    probs = probs * flipper_likes

    # Update with culmen length
    culmen_likes = [culmen_map[sp].pdf(row['Culmen Length (mm)'])
                   for sp in species_list]
    probs = probs * culmen_likes

    # Normalize and return most likely species
    probs = probs / probs.sum()
    return species_list[np.argmax(probs)]

def classify_multivariate(row, multinorm_map, species_list):
    """Classify using multivariate normal."""
    data = [row['Flipper Length (mm)'], row['Culmen Length (mm)']]
    probs = [multinorm_map[sp].pdf(data) for sp in species_list]
    return species_list[np.argmax(probs)]

# Remove rows with missing data
df_clean = df.dropna(subset=['Flipper Length (mm)', 'Culmen Length (mm)'])

# Create a grid of points
flipper_range = np.linspace(df['Flipper Length (mm)'].min() - 5,
                            df['Flipper Length (mm)'].max() + 5, 100)
culmen_range = np.linspace(df['Culmen Length (mm)'].min() - 2,
                           df['Culmen Length (mm)'].max() + 2, 100)
flipper_grid, culmen_grid = np.meshgrid(flipper_range, culmen_range)

# Classify each grid point with multivariate normal
predictions = np.zeros(flipper_grid.shape)
for i in range(flipper_grid.shape[0]):
    for j in range(flipper_grid.shape[1]):
        data = [flipper_grid[i,j], culmen_grid[i,j]]
        probs = [multinorm_map[sp].pdf(data) for sp in species_list]
        predictions[i,j] = np.argmax(probs)

# Plot
fig, ax = plt.subplots(figsize=(10, 6))
ax.contourf(flipper_grid, culmen_grid, predictions, alpha=0.3, levels=2)

# Overlay actual data
for idx, species in enumerate(species_list):
    species_data = df_clean[df_clean['Species'] == species]
    ax.scatter(species_data['Flipper Length (mm)'],
               species_data['Culmen Length (mm)'],
               label=species, s=50, edgecolors='black', linewidths=0.5)

ax.set_xlabel('Flipper Length (mm)', size=12)
ax.set_ylabel('Culmen Length (mm)', size=12)
ax.set_title('Decision Boundaries (Multivariate Normal Classifier)', size=14)
ax.legend()
ax.grid(alpha=0.3)
plt.show()
```

:::



## <span style="font-size: 0.85em">Naive Bayes vs Multivariate Normal</span>

:::{style="font-size: .8em"}

```{python}
#| echo: false

comparison = pd.DataFrame({
    'Species': species_list,
    'Naive Bayes': [0.003455, 0.001246, 0.995299],
    'Multivariate Normal': posterior_mv['probs'].values
})
print(comparison.to_string(index=False))
```


**Performance on full dataset:**

- Naive Bayes: **94.7% accuracy**
- Multivariate Normal: **95.3% accuracy**

Multivariate normal is only slightly better! The correlation structure doesn't help much for this problem.


:::

## Why "Naive" Bayes Works

:::{style="font-size: .7em"}

 Naive Bayes performed nearly as well multivariate normal (94.7% vs 95.3%)!

Even though the independence assumption is violated, Naive Bayes often works well because:


- **We only care about which class is most likely**, not exact probabilities
- Even if correlations exist, they may be **similar correlations across classes**
- **Less prone to overfitting** than complex models


**Real-world success:**

- 95%+ accuracy in spam detection
- 94.7% accuracy on our penguin dataset
- Used in customer segmentation, sentiment analysis, many business applications


:::

## When to Use Which?

:::{style="font-size: .8em"}

::: {.columns}
::: {.column width="50%"}
**Naive Bayes**

- Many features, including discrete
- Limited data
- Need speed/simplicity
:::

::: {.column width="50%"}
**Multivariate Normal**

- Few, continuous features
- Enough data for covariance
- Need probability calibration
:::
:::

Start with Naive Bayes! It's surprisingly effective and much simpler.

:::

## <span style="font-size: 0.85em">Spam Filtering: The Original Application</span>

:::{style="font-size: .8em"}

In the 1990s, Bayesian classification revolutionized spam filtering and is still used today!

**The Goal:** Calculate P(spam | words in email)

**The Intuition:**

- Words like "FREE", "WINNER", "VIAGRA" increase probability of spam
- Words like "meeting", "deadline", "attached" decrease it

Let's walk through a concrete example to see how this works.

:::

## <span style="font-size: 0.85em">Step 1: Learning from Historical Data</span>

:::{style="font-size: .8em"}

First, we analyze thousands of emails to learn word frequencies.

```{python}
#| echo: false

# Example word probabilities from training data
spam_data = {
    'Word': ['FREE', 'WINNER', 'meeting', 'deadline', 'viagra'],
    'P(word|spam)': [0.65, 0.58, 0.12, 0.08, 0.82],
    'P(word|ham)': [0.03, 0.02, 0.45, 0.38, 0.001]
}
df_spam_probs = pd.DataFrame(spam_data)
print(df_spam_probs.to_string(index=False))
```

**Interpretation:**

- 65% of spam emails contain "FREE" vs. only 3% of legitimate emails
- 45% of legitimate emails contain "meeting" vs. only 12% of spam

:::

## <span style="font-size: 0.85em">Step 2: Setting Our Prior</span>

:::{style="font-size: .8em"}

Before looking at any words, what's the baseline probability of spam?

```{python}
#| echo: true
# Suppose 30% of emails are spam (from historical data)
prior_spam = pd.DataFrame({
    'Class': ['Spam', 'Ham'],
    'Prior P': [0.30, 0.70]
})
print(prior_spam.to_string(index=False))
```

**Note:** This prior could be:

- Based on your personal inbox history
- Industry averages
- Uniform (50/50) if no information available

:::

## <span style="font-size: 0.85em">Step 3: A New Email Arrives</span>

:::{style="font-size: .8em"}

**Email text:** "WINNER! Claim your FREE prize now!"

**Words detected:** "WINNER", "FREE"

Let's update our beliefs step by step using Naive Bayes.

:::

## <span style="font-size: 0.85em">Update #1: Processing "WINNER"</span>

:::{style="font-size: .8em"}

```{python}
# Start with prior
email_class = pd.DataFrame({
    'Class': ['Spam', 'Ham'],
    'Prior': [0.30, 0.70]
})

# Get likelihoods from our learned probabilities
likelihood_winner = [0.58, 0.02]  # P(WINNER|spam), P(WINNER|ham)

# Calculate posterior
email_class['Likelihood'] = likelihood_winner
email_class['Prior × Likelihood'] = email_class['Prior'] * email_class['Likelihood']
normalizer = email_class['Prior × Likelihood'].sum()
email_class['Posterior'] = email_class['Prior × Likelihood'] / normalizer

print(email_class.to_string(index=False))
```

**After seeing "WINNER":** P(spam) jumps from 30% to 93%!

:::

## <span style="font-size: 0.85em">Update #2: Processing "FREE"</span>

:::{style="font-size: .8em"}

Now use the posterior from "WINNER" as our new prior:

```{python}
# The posterior from "WINNER" becomes our new prior
email_class_2 = pd.DataFrame({
    'Class': ['Spam', 'Ham'],
    'Prior (after WINNER)': email_class['Posterior'].values
})

# Get likelihoods for "FREE"
likelihood_free = [0.65, 0.03]  # P(FREE|spam), P(FREE|ham)

# Calculate new posterior
email_class_2['Likelihood'] = likelihood_free
email_class_2['Prior × Likelihood'] = email_class_2['Prior (after WINNER)'] * email_class_2['Likelihood']
normalizer = email_class_2['Prior × Likelihood'].sum()
email_class_2['Posterior'] = email_class_2['Prior × Likelihood'] / normalizer

print(email_class_2.to_string(index=False))
```

**After seeing "FREE" too:** P(spam) increases to 99.8%!

**Decision:** This is almost certainly spam.

:::

## <span style="font-size: 0.85em">The Naive Assumption in Spam Filtering</span>

:::{style="font-size: .8em"}

We treated "WINNER" and "FREE" as **independent** given the class.

$$P(\text{WINNER, FREE} \mid \text{spam}) = P(\text{WINNER} \mid \text{spam}) \times P(\text{FREE} \mid \text{spam})$$

**Is this realistic?**

No! Spammers who use "WINNER" are more likely to also use "FREE" - these words are correlated.

**But does it matter?**

For classification purposes, often not much! We just need to identify which class is most likely.

**Real-world success:** Bayesian spam filters achieved 95%+ accuracy and helped save email from the spam crisis of the late 1990s.

:::


## <span style="font-size: 1.0em">Group Question: Spam Detection</span>

:::{style="font-size: .6em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>The Setup:</b> You want to determine if a text you receive is spam.  You've made the following observations:</b><br><br>
• 50% of your spam texts offer you a "job opportunity" (vs 1% of your regular texts) <br>
• 50% of your spam messages ask you to "donate" to something (vs 1% of your regular texts) <br>
• 20% of your real messages mention your name (vs 1% of the spam texts) <br>
• 10% of the texts you receive are spam <br>
<br><br>
<b>Your Task:</b> You just received the text: "Hi [your name] do you want to donate to my fundraiser?"  Using a Naive Bayes approach, make your best guess as to whether the text is spam or not.
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```


:::

## <span style="font-size: 1.0em">Solution: Spam Detection</span>

:::{style="font-size: .5em"}


- $P(\text{spam}) = 0.10$, so $P(\text{real}) = 0.90$
- $P(\text{``job"} \mid \text{spam}) = 0.50$, $P(\text{``job"} \mid \text{real}) = 0.01$
- $P(\text{``donate"} \mid \text{spam}) = 0.50$, $P(\text{``donate"} \mid \text{real}) = 0.01$
- $P(\text{``name"} \mid \text{spam}) = 0.01$, $P(\text{``name"} \mid \text{real}) = 0.20$

**Naive Bayes calculation (using only features that appear):**

For **spam**:
$$P(\text{spam}) \times P(\text{name} \mid \text{spam}) \times P(\text{donate} \mid \text{spam}) = 0.10 \times 0.01 \times 0.50 = 0.0005$$

For **real**:
$$P(\text{real}) \times P(\text{name} \mid \text{real}) \times P(\text{donate} \mid \text{real}) = 0.90 \times 0.20 \times 0.01 = 0.0018$$

**Normalize:**
$$P(\text{spam} \mid \text{data}) = \frac{0.0005}{0.0005 + 0.0018} = \frac{0.0005}{0.0023} \approx 0.217 \text{ or } 21.7\%$$

$$P(\text{real} \mid \text{data}) = \frac{0.0018}{0.0023} \approx 0.783 \text{ or } 78.3\%$$

**Conclusion:** The text is most likely **REAL** (78.3% probability).

:::


## <span style="font-size: 1.0em">Group Question: Mystery Penguin!</span>

:::{style="font-size: .5em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>The Mystery:</b> You've discovered a penguin on the Palmer Archipelago with the following measurements:<br>
• Flipper length: 215 mm<br>
• Culmen (beak) length: 37 mm<br>
<br>
<b>The normal distribution parameters for each species:</b><br>
<table style="border-collapse: collapse; margin: 10px 0;">
  <tr style="border-bottom: 2px solid black;">
    <th style="padding: 5px 15px; text-align: left;">Species</th>
    <th style="padding: 5px 15px; text-align: center;">Flipper Distribution</th>
    <th style="padding: 5px 15px; text-align: center;">Culmen Distribution</th>
  </tr>
  <tr>
    <td style="padding: 5px 15px;"><b>Adélie</b></td>
    <td style="padding: 5px 15px; text-align: center;">N(μ=190.0, σ=6.5)</td>
    <td style="padding: 5px 15px; text-align: center;">N(μ=38.8, σ=2.7)</td>
  </tr>
  <tr>
    <td style="padding: 5px 15px;"><b>Chinstrap</b></td>
    <td style="padding: 5px 15px; text-align: center;">N(μ=195.8, σ=7.1)</td>
    <td style="padding: 5px 15px; text-align: center;">N(μ=48.8, σ=3.3)</td>
  </tr>
  <tr>
    <td style="padding: 5px 15px;"><b>Gentoo</b></td>
    <td style="padding: 5px 15px; text-align: center;">N(μ=217.2, σ=6.5)</td>
    <td style="padding: 5px 15px; text-align: center;">N(μ=47.5, σ=3.1)</td>
  </tr>
</table>
<br>
<b>Some potentially useful normal PDF values:</b><br>
f(215 | μ=190.0, σ=6.5) = 0.000038<br>
f(215 | μ=195.8, σ=7.1) = 0.0015<br>
f(215 | μ=217.2, σ=6.5) = 0.058<br>
f(37 | μ=38.8, σ=2.7) = 0.118<br>
f(37 | μ=48.8, σ=3.3) = 0.000202<br>
f(37 | μ=47.5, σ=3.1) = 0.000415<br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::


## <span style="font-size: 1.0em">Group Question: Mystery Penguin!</span>

:::{style="font-size: .7em"}


::: {style="text-align: center;"}
![](images/classification/mystery_penguin.png){width=200}
:::


```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>Your Task:</b> Using Naive Bayes with a uniform prior, update with flipper length and culmen length to find a posterior distribution over penguin species.<br><br>
1. Which species is this penguin most likely to be?<br>
2. Given the posterior distribution, how likely do you think it is *any* of these three?<br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## <span style="font-size: 1.0em">Mystery Penguin: Solution</span>

:::{style="font-size: .65em"}

**Step 1: Update with flipper length (215 mm)**

```{python}
#| echo: false

# Step 1: Start with uniform prior and update with flipper
species_list = ['Adelie', 'Chinstrap', 'Gentoo']
mystery_penguin = pd.DataFrame({
    'Species': species_list,
    'Prior': [1/3, 1/3, 1/3],
    'Likelihood': [0.000038, 0.0015, 0.058]
})

mystery_penguin['Prior × Likelihood'] = mystery_penguin['Prior'] * mystery_penguin['Likelihood']
normalizer = mystery_penguin['Prior × Likelihood'].sum()
mystery_penguin['Posterior'] = mystery_penguin['Prior × Likelihood'] / normalizer

print(mystery_penguin.to_string(index=False))
```



**Step 2: Update with culmen length (37 mm)**

```{python}
#| echo: false

# Step 2: Use previous posterior as new prior, update with culmen
mystery_penguin_2 = pd.DataFrame({
    'Species': species_list,
    'Prior': mystery_penguin['Posterior'].values,
    'Likelihood': [0.118, 0.000202, 0.000415]
})

mystery_penguin_2['Prior × Likelihood'] = mystery_penguin_2['Prior'] * mystery_penguin_2['Likelihood']
normalizer = mystery_penguin_2['Prior × Likelihood'].sum()
mystery_penguin_2['Posterior'] = mystery_penguin_2['Prior × Likelihood'] / normalizer

print(mystery_penguin_2.to_string(index=False))
```


:::

