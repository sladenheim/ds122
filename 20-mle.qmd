---
title: Maximum Likelihood Estimation
author: "CDS DS-122<br>Boston University"
format: 
    revealjs:
        math: true
        css: 
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true 
---

## Learning Objectives
:::{style="font-size: .8em"}

- Model fitting
- Likelihood function 
- Log-likelihood function 
- Maximum Likelihood Estimation (MLE)
- Application to the train problem

:::

## Train Problem
:::{style="font-size: .8em"}
A railroad gives each train a number starting from 1 up to some number ùëÅ. One day, you see a train with the number 60. 

:::{.center-text}
<img src="images/train/train60.jpeg" width=300/>
:::

__Question__: Estimate how many trains the railroad has using maximum likelihood estimation.
:::

## Model Fitting
:::{style="font-size: .8em"}
Assume that you know that data is drawn from a particular _**kind**_ of distribution, but you don't know the value(s) of the distribution's parameter(s). 

The table below lists some common distributions and their parameters.


| Distribution | Parameters $\theta$ |
| --- | --- | 
|Bernoulli   | $p$ |
|Binomial    | $(N,p)$ |
|Poisson     | $\lambda$ |
|Geometric   | $p$ |
|Exponential | $\lambda$ |
|Uniform     | $(a,b)$ |
|Normal      | $(\mu, \sigma)$ |
:::


## Model Fitting
:::{style="font-size: .8em"}
That is, data is drawn from a distribution
$p(x; \theta).$

The way to read this is: the probability of $x$ under a distribution having parameter(s) $\theta$.

We call $p(x; \theta)$ a __family__ of distributions because there is a different distribution for each value of $\theta$. 

<!-- The graph below illustrates the probability density functions of several normal distributions (from the same parametric family). -->

:::{.center-text}
<img src="images/parameter_estimation/normal_family.svg" width=500/>
:::
:::

## Model Fitting
:::{style="font-size: .8em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Model Fitting </span>
        <p>
        
    Model fitting is finding the parameter(s) \(\theta\) of the distribution, given some data \(x\) and assuming that a certain distribution can describe the population. 
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
In this setting, it's the parameter(s) $\theta$ that change, not the data $x.$

When treat $p(x; \theta)$ as a function of $\theta$ (instead of $x$, say) we call it a __likelihood__.

This shift in terminology highlights that we are now focusing on how the probability changes as we vary $\theta.$
:::

:::{.notes}
Subtle difference with parameter estimation: paramter estimation can be unrelated to the actual parameters, for instance it can be the range of outcomes in the normal distribution, not necessarily mu and sigma.
:::

## Model Fitting
:::{style="font-size: .8em"}
__Example__: Consider the dataset below:
```{python}
from scipy.stats import norm
from scipy.stats import uniform
from numpy.random import default_rng
import matplotlib.pyplot as plt

# specify the parameters
mu = 70 
sig = 3
#
samp_size = 30
rng = 0

fig, ax = plt.subplots(1, 1, figsize = (14,1))

# sample
samp_x = norm.rvs(size = samp_size, loc = mu, scale = sig, random_state = rng)
samp_y = [1 for x in samp_x]
ax.scatter(samp_x, samp_y, marker = 'o', facecolors='none', edgecolors='red', linewidths = 1.5, s = 48)
#ax.set_xlim(xmin, xmax)
ax.set_title('height [inches]')
ax.yaxis.set_visible(False)
# remove the "box" around the plot
for spine in [list(ax.spines.values())[i] for i in [0, 1, 3]]:
    spine.set_visible(False)
```
Assuming that this data is drawn from a normal distribution 

$$\small{p(x; \theta) = \mathcal{N}(x; \mu, \sigma^2).}$$

Then model fitting would consist of finding the $\mu$ and $\sigma$ that best match the given data $x$ shown above. In other words, _**the likelihood function gives the probability of observing the given data as a function of the parameter(s)**_. Therefore, _**the parameters can be estimated by maximizing the likelihood function**_.
:::

:::{.notes}
The same as probability


:::

## Calculating Likelihood
:::{style="font-size: .8em"}
How can we calculate the likelihood function? 

Consider a set of $m$ data items 

$$ \small X_s = (x^{(1)}, x^{(2)}, \dots, x^{(m)}), $$ 

drawn independently from the unknown distribution $p(x)$.

What is the probability of the entire dataset $X_s$?

Since we assume that the $x^{(i)}$ are independent, the probabilities multiply.

Therefore, the joint probability is

$$ \small p(X_s; \theta) =  p(x^{(1)}; \theta) \cdot  p(x^{(2)}; \theta) \cdot \ldots \cdot p(x^{(m)}; \theta) = \prod_{i=1}^m p(x^{(i)}; \theta). $$

<!-- We can use a special shorthand notation to represent products. Just like $\sum$ is shorthand for summing, $\prod$ is shortand for taking the product.

For example, the product of two numbers $a_1$ and $a_2$ (i.e., $a_1 a_2$), can be written as $\prod_{i=1}^2 a_i$. -->

<!-- The joint probability can be written as:

$$ \small p(X_s; \theta) = \prod_{i=1}^m p(x^{(i)}; \theta). $$ -->

:::

## Calculating Likelihood
:::{style="font-size: .8em"}
Note that each individual $p(x^{(i)}; \theta)$ is a value between 0 and 1.

There are $m$ of these numbers being multiplied.   So for any reasonable-sized dataset, the joint probability is going to be _**very small**_.
<br><br>

For example, if a typical probability is $1/10$, and there are 500 data items, then the joint probability will be a number on the order of $10^{-500}$.   

So the likelihood of the dataset will usually be too small to even represent in a computer using standard floating point!
:::

## Log-Likelihood
:::{style="font-size: .8em"}
Luckily, there is an excellent way to handle this problem. Instead of using likelihood, we will use the _**log of likelihood**_.  

The table below shows some of the properties of the natural logarithm.

|||
| --- | --- | 
|Product rule | $\log ab = \log a + \log b$ |
|Quotient rule | $\log \frac{a}{b} = \log a - \log b$ |
|Power rule   | $\log a^n = n \log a$ |
|Exponential\logarithmic | $\log e^x = e^{\log x} = x$ |

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    How can we rewrite \(\small{\log 6 + \log 5 - \log 4 + 2\log 2}\)? <br>
    a. \(\small{\log\frac{6}{5}}\) <br>
    b. \(\small{\log 30}\)<br>
    c. \(\small{\log 30 - 4\log2}\)
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
b
:::

## Log-Likelihood
:::{style="font-size: .8em"}
We will only be interested in the maxima of the likelihood function. Since the log function does not change those points (the log is a monotonic function), using the log of the likelihood works for us.

:::{.center-text}
<img src="images/parameter_estimation/natural_log_max.png" width=500/>
:::
:::

## Log-Likelihood
:::{style="font-size: .75em"}
So we will work with the log-likelihood:

$$\small{\log p(X_s; \theta).}$$

Which becomes:

$$\small{\log p(X_s; \theta) = \log \prod_{i=1}^m p(x_i; \theta) = \sum_{i=1}^m \log p(x_i; \theta).}$$

This way we are no longer multiplying many small numbers, and we work with values that are easy to represent.

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    Which statement about  \(\small{\log 0.05}\) is correct? <br>
    a. \(\small{\log 0.05 \geq 0}\) <br>
    b. \(\small{\log 0.05 < 0}\)<br>
    c. \(\small{\log 0.05}\) is not defined
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
Slide Type
Fragment
Answer. b (In fact,  log0.05‚âà‚àí3
 .)

Note: The log of a number less than one is negative, so log-likelihoods are always negative values.
:::

## Log-Likelihood
:::{style="font-size: .8em"}
__Example__: Suppose that $X$ is a discrete random variable with the probability mass function shown below.

| $x$ | $0$ | $1$ | $2$ | $3$ |
| :---: | :---: | :---: | :---: | :---: |
| $p(x;\theta)$ | $\frac{2}{3}\theta$ | $\frac{1}{3}\theta$| $\frac{2}{3}\left(1-\theta\right)$| $\frac{1}{3}\left(1-\theta\right)$|

Here $0\leq \theta \leq 1$ is a parameter. The following 10 independent observations were taken from this distribution:

$$\small{X_s = (3,0,2,1,3,2,1,0,2,1).}$$

We want to find the corresponding log-likelihood function.

Based on the observed data sample, the (joint) likelihood function is equal to

$$\small{p(X_s;\theta) = \prod_{i=1}^{10}  p\left(x^{(i)}; \theta\right)=p(0;\theta)^2p(1;\theta)^3p(2;\theta)^3p(3;\theta)^2.}$$
:::

## Log-Likelihood
:::{style="font-size: .8em"}
__Example (continued)__:
$$\small{p(X_s;\theta) = \prod_{i=1}^{10}  p\left(x^{(i)}; \theta\right)}\small{=\left( \frac{2}{3}\theta \right)^2 \left( \frac{1}{3}\theta \right)^3 \left(\frac{2}{3}\left(1-\theta\right)\right)^3 \left(\frac{1}{3}\left(1-\theta\right)\right)^2.}$$
<!-- $$\small{=\left( \frac{2}{3}\theta \right)^2 \left( \frac{1}{3}\theta \right)^3 \left(\frac{2}{3}\left(1-\theta\right)\right)^3 \left(\frac{1}{3}\left(1-\theta\right)\right)^2.}$$ -->

<!-- Since the likelihood function is given by

$$\small{p(X_s;\theta) = \left( \frac{2}{3}\theta \right)^2 \left( \frac{1}{3}\theta \right)^3 \left(\frac{2}{3}\left(1-\theta\right)\right)^3 \left(\frac{1}{3}\left(1-\theta\right)\right)^2,}$$ -->

Thus, the log-likelihood function can be written as

$$\small{\log p(X_s;\theta) = \log \left( \left( \frac{2}{3}\theta \right)^2 \left( \frac{1}{3}\theta \right)^3 \left(\frac{2}{3}\left(1-\theta\right)\right)^3 \left(\frac{1}{3}\left(1-\theta\right)\right)^2 \right)}$$

$$\small{=2\left(\log\frac{2}{3}+ \log \theta \right) + 3\left(\log\frac{1}{3}+ \log \theta \right) + 3\left(\log\frac{2}{3}+ \log (1-\theta) \right) + 2\left(\log\frac{1}{3}+ \log (1-\theta) \right)}$$

$$\small{= 5\log \theta + 5\log (1-\theta) + 5\log\frac{2}{9}.}$$
:::

## Log-Likelihood
:::{style="font-size: .8em"}
We can visualise the log-likelihood function by varying $\theta$.

```{python}
import numpy as np
def loglik(theta):
    return 5*np.log(theta)+5*np.log(1-theta)+5*np.log(2/9)

fig = plt.figure(figsize = (14,3))
ax = plt.axes()

x = np.linspace(0.01, 0.99, 500)
ax.plot(x, loglik(x), lw = 3, color = 'blue')
ax.set_xlabel(r'$\theta$', size = 16)
ax.set_ylabel('Log-Likelihood', size = 16);
ax.set_xticks(np.arange(0, 1, 0.1))
ax.tick_params(axis='both', which='major', labelsize=14)
plt.title(r'Log-Likelihood for considered discrete model based on sample $X_s$', size = 18);
plt.show()
```

A maximum in the plot of the log-likelihood function is somewhere between 0.4 and 0.6.  

If we use the value of $\theta$ at the maximum, we _**maximize the (log-)likelihood of the data under the model**_.
:::

## Computation of MLE
:::{style="font-size: .8em"}
Often, the Maximum Likelihood Estimator (MLE) is found using calculus by locating a critical point that is a maximum:

$$\small{\frac{\partial}{\partial \theta} \log p(X_s; \theta) = 0 \text{ and } \frac{\partial^2 }{\partial \theta^2}\log p(X_s; \theta)<0.}$$

:::{.center-text}
<img src="images/parameter_estimation/second_derivative_test.png" width=600/>
:::

If the graph of the log-likelihood shows a clear maximum, there's no need to compute second derivatives.
:::

## Computation of MLE
:::{style="font-size: .8em"}
The table below shows some useful properties of the derivatives.

|||
| --- | --- | 
|Natural logarithm | $\frac{d}{dx}\log x = \frac{1}{x}$ |
|Product rule | $\frac{d}{dx}(uv) = u\frac{dv}{dx} + v\frac{du}{dx}$ |
|Quotient rule | $\frac{d}{dx}\left(\frac{u}{v}\right) = \frac{v\:du/dx\:-\:u\:dv/dx}{v^2}$ |
|Chain rule| $\frac{d}{dx}z(y(x)) = \frac{dz}{dy}\frac{dy}{dx}$ |

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    What is the derivative of  \(\small{\log(1-x)}\)? <br>
    a. \(\small{(1-x)^{-1}}\) <br>
    b. \(\small{-\frac{1}{\log(x)}}\)<br>
    c. \(\small{-(1-x)^{-1}}\)
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::
:::{.notes}
c
:::

## Computation of MLE
:::{style="font-size: .8em"}

Typically, to find the Maximum Likelihood Estimate $\hat{\theta}$ for distribution parameter $\theta$, we perform the following steps:

 1. Compute the likehood function $p\left(X_s;\theta \right).$<br>
 2. Compute the corresponding log-likelihood function $\log p\left(X_s;\theta \right).$<br>
 3. Take the derivative of the log-likelihood function with respect to $\theta$.<br>
 4. Set the derivative to zero to find the MLE. <br>
 4. (Optional) Confirm that the obtained extremum is indeed a maximum by taking the second derivative or from the plot of the log-likelihood function. 
:::

## Computation of MLE
:::{style="font-size: .8em"}
Let us compute the MLE for the example we looked at earlier.

__Example.__ Earlier we obtained the following log-likehood function:

$$\small{5\log \theta + 5\log (1-\theta) + 5\log\frac{2}{9}}.$$

The derivative of this log-likelihood function with respect to $\theta$ is equal to

$$\small{\log p(X_s; \theta) = \frac{\partial}{\partial\theta}\log p(X_s;\theta) = \frac{5}{\theta} - \frac{5}{1-\theta}}.$$

Setting the derivative to zero leads to

$$\small{5(1-\theta) = 5\theta};$$

$$\small{2\theta=1.}$$
:::


## Computation of MLE
:::{style="font-size: .8em"}

The above computation suggests that the MLE for $\theta$ is potentially $\hat{\theta}=\frac{1}{2}.$ 
```{python}
def loglik(theta):
    return 5*np.log(theta)+5*np.log(1-theta)+5*np.log(2/9)

fig = plt.figure(figsize = (14,3))
ax = plt.axes()

x = np.linspace(0.01, 0.99, 500)
ax.plot(x, loglik(x), lw = 3, color = 'blue')
ax.set_xlabel(r'$\theta$', size = 16)
ax.set_ylabel('Log-Likelihood', size = 16);
ax.set_xticks(np.arange(0, 1, 0.1))
plt.title(r'Log-Likelihood for considered discrete model based on sample $X$', size = 18);

mle = 0.5
ymin = np.min(loglik(x))
ymax = np.max(loglik(x))
plt.vlines(x = mle, ymin = ymin, ymax = ymax, linestyles = 'dashed', color = 'g')
plt.plot(mle, ymin, 'o', color = 'g', markersize = 14, clip_on = False)
plt.text(mle+0.02, ymin+1, r'$\hat{\theta} = 0.5$', size = 16, ha = 'left', va = 'bottom')
plt.show()
```
The figure shows that $\hat{\theta}=\frac{1}{2}$ is a maximum. Therefore, the MLE of $\theta$ is indeed $\frac{1}{2}.$
:::

## Train Problem
:::{style="font-size: .8em"}

We assume:

- Each train is equally likely to be seen.
- Train numbers range from 1 to $N$.
- We observed train #60.


Let $\theta = N$. The likelihood of seeing train 60 is:

$$
p(X_s; \theta) = \frac{1}{\theta}, \quad \text{for } \theta \geq 60.
$$

This function decreases as $\theta$ increases.  
So the maximum likelihood estimate is:


$$
\hat{\theta} = 60.
$$


:::

## Group Quesiton 1
:::{style="font-size: .8em"}

| $x$ | $1$ | $2$ |
| :---: | :---: | :---: |
| $p(x;\theta)$ | $\theta$ | $1-\theta$|
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
   Suppose that \(X\) is a discrete random variable with the probability mass function given by the table above.<br>

Three independent oservations are made from this distribution:
\(x_1 = 1, x_2 = 2, x_3 = 2.\)<br>

    a. Find the likelihood function.<br>
    <br>
    <br>
    <br>
    b. Find the log-likelihood function.
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

## Group Quesiton 1
:::{style="font-size: .8em"}
```{python}
def qloglik(theta):
    return np.log(theta)+2*np.log(1-theta)

fig = plt.figure(figsize = (14,3))
ax = plt.axes()

x = np.linspace(0.01, 0.99, 500)
ax.plot(x, qloglik(x), lw = 3, color = 'blue')
ax.set_xlabel(r'$\theta$', size = 16)
ax.set_ylabel('Log-Likelihood', size = 16);
ax.set_xticks(np.arange(0, 1, 0.1))
plt.title(r'$\log \theta + 2\log (1-\theta)$', size = 18);
plt.show()
```

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
   c. Find the maximum likelihood estimate of \(\theta.\) 
       <br>
    <br>
    <br>
        <br>
    <br>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::
