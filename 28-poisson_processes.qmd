---
title: Poisson Processes
author: "CDS DS-122<br>Boston University"
format:
    revealjs:
        math: true
        css:
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson, gamma
from IPython.core.display import HTML


```

## Learning Objectives
:::{style="font-size: .8em"}

- Understanding mixture distributions
- Modeling events with Poisson processes
- Computing posterior predictive distributions
- Calculating probability of superiority

:::

## The World Cup Problem

:::{style="font-size: .7em"}

In the 2018 FIFA World Cup final, France defeated Croatia 4 goals to 2.

1. How confident should we be that France is the better team?
2. If the same teams played again, what is the chance France would win?

:::{.center-text}
<img src="images/poisson/france_croatia.jpg" width=400/>
:::

To answer these questions, we need **Poisson processes** and **mixture distributions**.

:::

## Modeling soccer goals

:::{style="font-size: .7em"}

**Goals in soccer have interesting properties:**

- **Discrete events** (0, 1, 2, 3... goals) but occur over **continuous time** (90 minutes)
- Goals can happen at any moment during the game
- Each goal happens somewhat independently of others
- Teams have a characteristic scoring rate ("goals per game")

**This matches the Poisson process framework perfectly:**

- We've seen Poisson distributions before for counting rare events
- But now we're thinking about them **per unit time** (per game, per 90 minutes)
- The rate parameter $\lambda$ captures a team's goal-scoring ability
- Perfect for modeling "how many events in a fixed time period"

:::


## Poisson Processes

:::{style="font-size: .7em"}

Remember we defined:

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Poisson Distribution </span>
        <p style="margin-bottom: 0;">
       A random variable \(X\) has a Poisson distribution with parameter \(\lambda > 0\) if it has a discrete distribution with 
       \[
       P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}, \quad k = 0, 1, 2, \dots
       \]
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

Now we can talk about the related "Poisson process":

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Poisson Process</span>
        <p style="margin-bottom: 0;">
        A Poisson process models events that occur randomly over time at a constant average rate \(\lambda\) per time period.  The number of occurrences in a fixed period of time follows a Poisson distribution.  The time between occurrences follows an exponential distribution.
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Poisson Processes

:::{style="font-size: .7em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Poisson Process</span>
        <p>
        A Poisson Process models events that occur randomly over time at a constant average rate \(\lambda\) per time period.  The number of occurrences in a fixed period of time follows a Poisson distribution.  The time between occurrences follows an exponential distribution.
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

**Assumptions:**

1. Events occur at a constant average rate $\lambda$ (events per unit time)
2. Events are equally likely at any moment
3. Events occur independently

**Examples:** Customer arrivals, radioactive decay, goals in soccer

:::


##  <span style="font-size: 0.7em">Poisson process for soccer goals</span>

:::{style="font-size: .8em"}

We will assume each team's total follows a Poisson distribution.

So we can ask questions like:

- If a team averages $\lambda = 2$ goals per game, what's the probability they score exactly 4 goals?

$$ P[k=4] = \frac{2^4 e^{-2}}{4!} = \frac{16 \times 0.135}{24} \approx 0.09 $$

The problem now is, we don't know $\lambda$!

:::

## The World Cup Problem

:::{style="font-size: .7em"}

**The Setup:**

- (Data) We have seen one game where France scored 4 goals, Croatia scored 2 goals
- (Model) Assume goals follow a Poisson process with unknown rates ($\lambda_C$, $\lambda_F$)

**Next steps:**

0. Remember we are going to **fix data** and **vary models**
1. Choose a **prior** for $\lambda_C$ and $\lambda_F$
2. Use the outcome of this game to find our **posterior** beliefs about $\lambda_C$ and $\lambda_F$
3. Calculate our **posterior predictive distribution** for future scores
4. Answer our questions (which team is better,  what will happen next time)

:::

## Prior for Goal Scoring Rate

:::{style="font-size: .6em"}

**We want a prior that can capture what we know:**

- Average goals per game in World Cup history: 1.4
- $\lambda$ must be non-negative
- Very high-scoring games are rare

**The Gamma Distribution** is a good choice:

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Gamma Distribution</span>
        <p>
        A continuous random variable \(X\) has an (unscaled) Gamma distribution with shape parameter \(\alpha > 0\) , denoted \(X \sim \text{Gamma}(\alpha, 1)\), if it has PDF:
        $$f(x) = \frac{x^{\alpha-1} e^{-x}}{\Gamma(\alpha)} , \quad x > 0.$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Prior for Goal Scoring Rate

:::{style="font-size: .6em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Gamma Distribution</span>
        <p>
        A continuous random variable \(X\) has an (unscaled) Gamma distribution with shape parameter \(\alpha > 0\) , denoted \(X \sim \text{Gamma}(\alpha, 1)\), if it has PDF:
        $$f(x) = \frac{x^{\alpha-1} e^{-x}}{\Gamma(\alpha)} , \quad x > 0.$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

Why this works for us: 

- Defined for $\lambda > 0$, decreases for large values
- $\mathbb{E}(X) = \alpha$ so can be set to have mean 1.4 ($\alpha = 1.4$)
- The $\Gamma$ function is a generalization of the factorial ($\Gamma(n+1) = n!$ for integers)
- Fun fact: The exponential distribution and chi-squared distribution are both special cases of the Gamma distribution!
- (Makes the math work out - we'll see why in a few lectures!)

:::

## The Gamma Distribution

:::{style="font-size: .7em"}

Most teams score fewer than 2 goals per game, which matches our prior knowledge.

```{python}
#| echo: false
#| fig-align: center
from scipy.stats import gamma

x_vals = np.arange(0, 8, 0.1)
# Gamma with shape parameter 1.4
gamma_vals = gamma.pdf(x_vals, 1.4)

plt.plot(x_vals, gamma_vals, color='red', lw=3)
plt.xlabel('Goal Scoring Rate λ', size=16)
plt.ylabel('Probability Density', size=16)
plt.title('Prior Distribution for λ (Gamma distribution)', size=18);
```


:::

## The World Cup Problem

:::{style="font-size: .8em"}

**The Setup:**

- Assume goals follow a Poisson process with unknown rates ($\lambda_C$, $\lambda_F$)
- We have seen one game where France scored 4 goals, Croatia scored 2 goals

**Next steps:**

1. ~~Choose a prior~~
2. Use the outcome of this game to update our beliefs about $\lambda_C$ and $\lambda_F$ (calculate a posterior)
3. Calculate our **posterior predictive distribution** for future scores
4. Answer our questions (which team is better, and what will happen in the next game)

:::

## Bayesian Update Strategy

:::{style="font-size: .7em"}

**How do we update our beliefs about $\lambda$ after seeing the game?**

1. **Create a grid:** Discretize the space of possible $\lambda$ values (e.g., 0, 0.1, 0.2, ..., 8.0)

2. **Assign prior probabilities:** For each grid point, evaluate the prior probability (from our Gamma distribution)

3. **Compute likelihoods:** For each grid point, calculate the likelihood of the observed data (4 goals for France, 2 for Croatia) using the Poisson distribution

4. **Multiply:** Prior x Likelihood for each grid point

5. **Normalize:** Divide by the sum to get probabilities that sum to 1

This gives us the posterior distribution!

:::

## Computing the Updates

:::{style="font-size: .7em"}

Using our standard Bayesian update approach with a grid:

```{python}
#| echo: true
#| fig-align: center
def likelihood_poisson(lam, data):
    '''Returns the likelihood of seeing k goals for goal scoring rate lambda'''
    return [poisson.pmf(data, l) for l in lam]

def update(distribution, likelihood):
    '''Our standard Bayesian update function'''
    distribution['probs'] = distribution['probs'] * likelihood
    prob_data = distribution['probs'].sum()
    distribution['probs'] = distribution['probs'] / prob_data
    return distribution

gamma_vals = [gamma.pdf(lam, 1.4) for lam in x_vals]
prior = pd.DataFrame({'lams': x_vals, 'probs': gamma_vals/sum(gamma_vals)})

france = prior.copy()
update(france, likelihood_poisson(france['lams'], 4));
croatia = prior.copy()
update(croatia, likelihood_poisson(croatia['lams'], 2));
```

:::

## Posteriors for Both Teams

:::{style="font-size: .8em"}

France's posterior is shifted to higher values of $\lambda$.

```{python}
#| echo: false
#| fig-align: center
plt.plot(prior['lams'], prior['probs'], color='gray', lw=3)
plt.plot(france['lams'], france['probs'], color='blue', lw=3)
plt.plot(croatia['lams'], croatia['probs'], color='red', lw=3)
plt.legend(['Prior', 'France Posterior', 'Croatia Posterior'], fontsize=14, loc='best')
plt.xlabel('Goal Scoring Rate λ', size=16)
plt.ylabel('Probability', size=16)
plt.title('Posterior Distributions after One Game', size=18);
```


:::

##  <span style="font-size: 0.7em">Point Estimates from the Posterior</span>

:::{style="font-size: .7em"}

Let's calculate our two point-estimators from the posterior:

- **MAP (Maximum A Posteriori):** The mode of the posterior (most likely value)
- **MMSE (Minimum Mean Squared Error):** The mean of the posterior (minimizes expected squared error)

```{python}
#| echo: true
# France estimates
france_map = france['lams'][france['probs'].idxmax()]
france_mmse = (france['lams'] * france['probs']).sum()

# Croatia estimates
croatia_map = croatia['lams'][croatia['probs'].idxmax()]
croatia_mmse = (croatia['lams'] * croatia['probs']).sum()

print(f"France:  MAP = {france_map:.2f},  MMSE = {france_mmse:.2f}")
print(f"Croatia: MAP = {croatia_map:.2f},  MMSE = {croatia_mmse:.2f}")
```

Note: Both estimates are pulled toward the prior (mean 1.4), not just the raw observations (4 and 2).

:::

## Credible Intervals

:::{style="font-size: .7em"}

A **credible interval** captures our uncertainty about the parameter.

A 90% credible interval contains 90% of the posterior probability.

```{python}
#| echo: true
def credible_interval(distribution, prob=0.90):
    """Compute equal-tailed credible interval."""
    cumsum = distribution['probs'].cumsum()
    lower_idx = (cumsum >= (1 - prob) / 2).idxmax()
    upper_idx = (cumsum >= prob + (1 - prob) / 2).idxmax()
    return distribution['lams'].iloc[lower_idx], distribution['lams'].iloc[upper_idx]

france_ci = credible_interval(france)
croatia_ci = credible_interval(croatia)

print(f"France 90% CI:  [{france_ci[0]:.2f}, {france_ci[1]:.2f}]")
print(f"Croatia 90% CI: [{croatia_ci[0]:.2f}, {croatia_ci[1]:.2f}]")
```


:::


## Probability of Superiority

:::{style="font-size: .8em"}

**Question 1:** How confident should we be that France is the better team?

We need: $P(\lambda_{\text{France}} > \lambda_{\text{Croatia}})$

**Method:** Enumerate all pairs of values from the two distributions, adding up the probability that France's value exceeds Croatia's.

:::

## Probability of Superiority

:::{style="font-size: .8em"}

**Simple Example:** What's the probability a six-sided die beats a four-sided die?

| Die 6 | Beats | Probability |
|-------|-------|-------------|
| 6 | 1,2,3,4 | $(1/6) \times 1$ |
| 5 | 1,2,3,4 | $(1/6) \times 1$ |
| 4 | 1,2,3 | $(1/6) \times (3/4)$ |
| 3 | 1,2 | $(1/6) \times (1/2)$ |
| 2 | 1 | $(1/6) \times (1/4)$ |
| 1 | none | $(1/6) \times 0$ |

Sum: $(1/6)(1 + 1 + 3/4 + 1/2 + 1/4) = 7/12 \approx 0.583$

:::
##  <span style="font-size: 0.7em">Computing Probability of Superiority</span>

:::{style="font-size: .8em"}

```{python}
#| echo: true
def prob_of_superiority(dist1, dist2):
    """Compute the probability that dist1 > dist2."""
    total = 0
    for index1, row1 in dist1.iterrows():
        for index2, row2 in dist2.iterrows():
            if row1['lams'] > row2['lams']:
                total += row1['probs'] * row2['probs']
    return total

prob_france_better = prob_of_superiority(france, croatia)
print(f"P(France is better) = {prob_france_better:.4f}")
```

Based on one game, we have moderate confidence (about 75%) that France is the better team.

:::

## Predicting a Rematch

:::{style="font-size: .8em"}

**Question 2:** If they played again, what's the chance France would win?

This is different! 

<br>

Question 1 asked "how likely is $\lambda_{\text{F}} > \lambda_{\text{C}}$?"

Question 2 is asking "how likely is $x_F > x_C$ where $x_F \sim \text{Poisson}(\lambda_{\text{F}})$ and $x_C \sim \text{Poisson}(\lambda_{\text{C}})$?"

<br>

We need one more tool to answer Question 2 - Mixture Distributions.

:::


## Mixture Distributions

:::{style="font-size: .8em"}

Consider this (American) football scenario:

**Scenario:** A team randomly selects between two play types:

- With probability 1/2, they call a **run play**
  - Steady gains: 0, 5, or 10 yards (equal probability)
- With probability 1/2, they call a **pass play**
  - Higher variance: -10, 0, 5, 10, 15, or 20 yards (equal probability)
  - (-10 represents an interception)

What is the distribution of yards gained?

:::

## Computing a Mixture

:::{style="font-size: .7em"}

:::{.center-text}
<img src="images/poisson/football_field.jpg" width=800/>
:::


:::


## Computing a Mixture

:::{style="font-size: .7em"}

:::{.center-text}
<img src="images/poisson/football_field.jpg" width=800/>
:::

<br><br><br>

- P(5 yards) via run: $(1/2) \times (1/3) = 1/6$
- P(5 yards) via pass: $(1/2) \times (1/6) = 1/12$
- **Total:** $1/6 + 1/12 = 1/4$

:::

## Mixture Distributions

:::{style="font-size: .8em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Mixture Distribution</span>
        <p>
        If \(f\) is the PMF of a mixture distribution, and \(f_1, f_2, \ldots, f_n\) are the component distributions, where the probability of selecting distribution \(f_i\) is \(p_i\), then:
        $$f(x) = \sum_{i=1}^{n} p_i \cdot f_i(x)$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```


:::


##  <span style="font-size: 0.9em">Calculating the mixture by hand</span>

:::{style="font-size: .8em"}


Probability of gaining 0 yards:

- Via run play: $(1/2) \times (1/3) = 1/6$
- Via pass play: $(1/2) \times (1/6) = 1/12$
- Total: $1/6 + 1/12 = 1/4$

Probability of gaining 15 yards:

- Via run play: $(1/2) \times (0) = 0$ (can't get 15 on a run)
- Via pass play: $(1/2) \times (1/6) = 1/12$
- Total: $0 + 1/12 = 1/12$

:::

## Computing Mixtures in Code

:::{style="font-size: .8em"}

```{python}
#| echo: true
# Run play: 0, 5, or 10 yards
run = pd.DataFrame(index=[0, 5, 10])
run['probs'] = 1/3

# Pass play: -10, 0, 5, 10, 15, or 20 yards
pass_play = pd.DataFrame(index=[-10, 0, 5, 10, 15, 20])
pass_play['probs'] = 1/6

def make_mixture(pmf_table, probs):
    mix = pmf_table.fillna(0)
    mix = mix.transpose() * probs
    return mix.sum(axis=1)

pmf_table = pd.concat([run.transpose(), pass_play.transpose()], ignore_index=True)
play_probs = [0.5, 0.5]

mixture = make_mixture(pmf_table, play_probs)
print(mixture.sort_index())
```


:::

## Back to Question 2

:::{style="font-size: .8em"}

**Question 2:** If they played again, what's the chance France would win?

This is different! 

<br>

Question 1 asked "how likely is $\lambda_{\text{F}} > \lambda_{\text{C}}$?"

Question 2 is asking "how likely is $x_F > x_C$ where $x_F \sim \text{Poisson}(\lambda_{\text{F}})$ and $x_C \sim \text{Poisson}(\lambda_{\text{C}})$?"

<br>

The distribution of goals is a **mixture** of Poisson distributions weighted by the **posterior** on $\lambda$.

:::

##  <span style="font-size: 0.8em">Posterior Predictive Distribution</span>

:::{style="font-size: .6em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box" style="font-size: 0.85em;">
     <span class="label">Posterior Distribution</span>
        <p>
        The <b>posterior distribution</b> is a distribution <b>over a parameter</b> where:
        $$P(\theta | \text{data}) \propto P(\text{data} | \theta) \cdot P(\theta).$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

<br>

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Posterior Predictive Distribution</span>
        <p>
        The <b>posterior predictive distribution</b> is the probability distribution <b>of the next observation</b> given the data we've seen:
        $$P(x_{\text{new}} | \text{data}) = \sum_{\theta} P(x_{\text{new}} | \theta) \cdot P(\theta | \text{data})$$

        This is a mixture of predictive distributions, weighted by the posterior.
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

In our World Cup example: we mix over all possible values of $\lambda$ (weighted by the posterior) to predict future goal counts.

:::

##  <span style="font-size: 0.8em">Computing the PPD</span>

:::{style="font-size: .7em"}

To find the distribution of the number of goals $k$, we take:

1. $P(k \text{ goals} | \lambda) = \text{Poisson}_\lambda(k)$
2. Weight this by the posterior probability of that $\lambda$ (from the first part of the problem)

This gives us the **posterior predictive distribution** for goals.

```{python}
#| echo: true
# Create Poisson distributions for each value of lambda
pmf_table = pd.DataFrame([
    [poisson.pmf(goals, lam) for goals in range(10)]
    for lam in prior['lams']
])
pmf_table = (pmf_table.T / pmf_table.T.sum()).T

# Mix using posterior probabilities
pred_france = make_mixture(pmf_table, france['probs'])
pred_croatia = make_mixture(pmf_table, croatia['probs'])
```

:::

##  <span style="font-size: 0.8em">Posterior Predictive Distribution</span>

:::{style="font-size: .8em"}

```{python}
#| echo: false
#| fig-align: center
plt.plot(range(10), pred_france, 'o-', color='blue', lw=3)
plt.plot(range(10), pred_croatia, 'o-', color='red', lw=3)
plt.legend(['France', 'Croatia'], fontsize=14, loc='best')
plt.xlabel('Number of Goals', size=16)
plt.ylabel('Probability', size=16)
plt.title('Predicted Goals in a Rematch', size=18);
```

:::
##  <span style="font-size: 0.8em">Probability France Wins Rematch</span>

:::{style="font-size: .8em"}

```{python}
#| echo: true
prob_france_wins = prob_of_superiority(
    pd.DataFrame({'lams': range(10), 'probs': pred_france}),
    pd.DataFrame({'lams': range(10), 'probs': pred_croatia})
)
print(f"P(France wins rematch) = {prob_france_wins:.4f}")
```

There's about a 57% chance France would win a rematch!

**Important distinction:**

- 75% confident France has a higher $\lambda$ (is "better")
- 57% chance France wins next game (accounts for randomness)

:::

## Summary

:::{style="font-size: .8em"}

**Key Concepts:**

1. **Mixture distributions** weight multiple distributions by selection probabilities
2. **Poisson processes** model events occurring at a constant rate
3. **Posterior predictive distributions** predict future observations by mixing over parameter uncertainty
4. **Probability of superiority** quantifies confidence that one value is larger than another


:::

## Group Question

:::{style="font-size: .65em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>Football Strategy Problem:</b> Your opposing team uses one of three strategies:

<br>
• <b>Conservative:</b> Run 75% of the time, pass 25%<br>
• <b>Balanced:</b> Run 50%, pass 50%<br>
• <b>Aggressive:</b> Run 25%, pass 75%<br>
<br>
Recall: 
<br>
• Run plays gain 0, 5, or 10 yards (equal prob).<br>
• Pass plays gain -10, 0, 5, 10, 15, or 20 yards (equal prob)<br>
<br>
<b>a.</b> If they use the Conservative strategy, what is the probability they gain exactly 5 yards on the next play?
<br><br><br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Group Question (cont'd)

:::{style="font-size: .65em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>b.</b> You initially assume conservative/balanced/aggressive strategies are equally likely.  You then observe your opponent run the ball on their first 3 plays. Use Bayes' rule to compute the posterior probability of each strategy.
<br><br><br><br>
<b>c.</b> Given your answer to (b), what is the posterior predictive distribution for yards gained on their 4th play?
<br><br><br><br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Group Question: Solution

:::{style="font-size: .6em"}

**Part (a): P(5 yards | Conservative)**

Conservative strategy: 75% run, 25% pass

- Run plays: Gain 0, 5, or 10 yards with probability 1/3 each
- Pass plays: Gain -10, 0, 5, 10, 15, or 20 yards with probability 1/6 each

Using the law of total probability:

$$P(\text{5 yards} \mid \text{Conservative}) = P(\text{5 yards} \mid \text{Run}) \cdot P(\text{Run}) + P(\text{5 yards} \mid \text{Pass}) \cdot P(\text{Pass})$$

$$= \frac{1}{3} \cdot 0.75 + \frac{1}{6} \cdot 0.25 = 0.25 + 0.0417 = 0.2917$$

:::

## Group Question: Solution

:::{style="font-size: .6em"}


**Part (b): Posterior after 3 runs (Bayes' rule via table)**

| Strategy | Prior | Likelihood | Unnormalized Posterior | Posterior |
|----------|-------|------------|------------------------|-----------|
| Conservative | 1/3 | $(0.75)^3 = 0.4219$ | $\frac{1}{3} \times 0.4219 = 0.1406$ | $\frac{0.1406}{0.1875} = 0.75$ |
| Balanced | 1/3 | $(0.50)^3 = 0.125$ | $\frac{1}{3} \times 0.125 = 0.0417$ | $\frac{0.0417}{0.1875} = 0.22$ |
| Aggressive | 1/3 | $(0.25)^3 = 0.0156$ | $\frac{1}{3} \times 0.0156 = 0.0052$ | $\frac{0.0052}{0.1875} = 0.03$ |
| **Total** | **1** | | **0.1875** | **1.00** |

**Interpretation:** After observing 3 runs, we're 75% confident they're using a Conservative strategy!

:::

## Group Question: Solution 

:::{style="font-size: .65em"}

**Part (c): Posterior predictive for 4th play**

We need to compute: $P(\text{yards} \mid RRR)$ by averaging over strategies

For **5 yards gained** (as an example):

$$P(\text{5 yards} \mid RRR) = \sum_{\text{strategy}} P(\text{5 yards} \mid \text{strategy}) \cdot P(\text{strategy} \mid RRR)$$

$$= 0.2917 \cdot 0.75 + 0.2083 \cdot 0.22 + 0.1250 \cdot 0.03 = 0.268$$

Where:

- $P(\text{5 yards} \mid \text{Conservative}) = 0.2917$ (from part a)
- $P(\text{5 yards} \mid \text{Balanced}) = \frac{1}{3} \cdot 0.5 + \frac{1}{6} \cdot 0.5 = 0.2083$
- $P(\text{5 yards} \mid \text{Aggressive}) = \frac{1}{3} \cdot 0.25 + \frac{1}{6} \cdot 0.75 = 0.1250$


:::
