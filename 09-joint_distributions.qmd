---
title: Joint Distributions
author: "CDS DS-122<br>Boston University"
format: 
    revealjs:
        math: true
        css: 
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true 
---


## Dating Pool Illusion
:::{style="font-size: .8em"}
Imagine evaluating people in a dating pool based on two traits:

- Kindness (nice vs. mean)
- Appearance (attractive vs. unattractive)

You might notice that the more attractive someone is, the less kind they seem.
Or vice versa: the kinder someone is, the less attractive they appear.<br>
<br><br>

__Does this trade-off reflect a real link between attractiveness and kindness?__  

:::

:::{.notes}
Your friend sets you up on a blind date. You ask, “Are they attractive?”
They pause and say, “Well… they have a great personality.”

You already know what that means.

Now think about how we judge people in dating pools. We tend to notice those who are either very attractive, very kind, or ideally both. But we rarely notice the ones who are neither — they’re invisible to us.

This creates an illusion: it feels like attractive people are mean, and kind people are unattractive.

But is that really true?
:::

## Dating Pool Illusion
:::{.center-text}
```{python}
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# Create a figure and axis
fig, ax = plt.subplots(figsize=(6, 6))

# Draw the full grid (Great Square)
ax.add_patch(patches.Rectangle((0, 0), 1, 1, edgecolor='black', facecolor='lightgray', label='Complete Pool'))

# Draw the triangle of acceptable individuals (those who are nice, attractive, or both)
triangle = patches.Polygon([[0, 1], [1, 1], [1, 0]], closed=True, color='skyblue', label='Observed Subset')
ax.add_patch(triangle)

# Add labels to the axes
ax.set_xticks([0, 1])
ax.set_xticklabels(['Unattractive', 'Attractive'])
ax.set_yticks([0, 1])
ax.set_yticklabels(['Mean', 'Nice'])
plt.yticks(fontsize=18)
plt.xticks(fontsize=18)

# Add grid lines
ax.grid(True, linestyle='--', alpha=0.5)

# Add title and legend
# ax.set_title("Berkson's Fallacy: Selection Bias in Dating Pool")
# ax.legend(loc='upper left')
plt.legend(loc='best', prop={'size': 18})

# Set limits and aspect ratio
ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.set_aspect('equal')

# Save the figure
plt.savefig("berksons_fallacy_diagram.png")
plt.show()
```
:::

:::{.notes}
People are evenly distributed across both kindness and appearance. However, in practice, you only notice people who are either kind or attractive, or both.

A joint distribution of kindness and appearance.
:::




## Learning Objectives

:::{style="font-size: .8em"}

* Joint distributions (of discrete random variables)
* Marginal distributions
* Independent random variables
* Covariance 
* Correlation coefficient
* Application of above concepts to explore the relationship between heights of fathers and sons

:::

:::{.notes}
All the concepts can be extended to continuous random variables, but that's not required for this course.
:::


## Like Father, Like Son?
:::{style="font-size: .8em"}

A study was conducted to explore the relationship between the heights of fathers and their sons.

:::{.center-text}
```{python}
# Import necessary library
import pandas as pd

# Load the dataset
df = pd.read_csv("data/father_son_heights.csv")

# Display the first few rows
print(df.head(6))
```
:::

:::{.columns}
::: {.column width="25%"}
:::{.center-text}
<img src="images/joint distributions/father_son.png" width=400/>
:::
:::
::: {.column width="70%"}
:::{style="font-size: 1.0em"}
The properties of the dataset: <br>

- 100 rows/observations,
- 2 columns/variables,
- Heights are measured in inches
:::
:::
:::


__Question__: Do taller fathers usually have taller sons based on this dataset?
:::

:::{.notes}
regression to the mean
:::


## Joint Distributions
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Joint Distribution </span>
        <p>
       A joint distribution describes the probability of two or more random variables taking on specific values at the same time.
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

__Example__: We analyze a student population using two categorical variables:

- _StudyEffort_: {low, high}  
- _Grade_: {A, B, C}
<!-- 
To understand how these attributes relate, we define their _joint distribution_. -->

| |low|high|
|---|---|---|
|__A__| 0.07| 0.18|
|__B__| 0.28| 0.09|
|__C__|0.35| 0.03|

:::

## Marginal Distributions
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Marginal Distribution </span>
        <p>
       The marginal distribution of a discrete random variable is the probability distribution of that variable considered on its own. It is obtained by summing over the joint distribution with respect to the other variable(s).
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Remark__: Mathematically, we write: $\small{P(X) = \sum_y P(X, Y = y).}$

__Example__:
 <!-- We could also be interested in each characteristic on its own. For example, we might be interested in the distribution of _StudyEffort_ across our population. -->

<!-- This is called a _marginal_ distribution. -->

<!-- It gets this name because it corresponds to the __marginal sums__ of the probability distribution.   -->



| |low|high|marginal|
|---|---|---|---|
|__A__| 0.07| 0.18|0.25|
|__B__| 0.28| 0.09|0.37|
|__C__|0.35| 0.03|0.38 |
|__marginal__| 0.70 | 0.30| |
:::

## Marginal Distributions
:::{style="font-size: .8em"}
__Example__: A fair coin is tossed three times. Let $\small X$ denote _**the number of heads on the first toss**_ and $\small Y$ _**the total number of heads**_. The sample space is then equal to

$$\small  \Omega = \{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}.$$

From the sample space we see that the __joint probability mass__ (or __frequency__) __function__ of $\small  X$ and $\small  Y$ is as given in the following table:

| | $\small  y = 0$ |$\small  y = 1$| $\small  y = 2$| $\small  y = 3$|
| :---: | :---: | :---: | :---: | :---: |
| $\small  x = 0$ | $\small \frac{1}{8}$ | $\small \frac{2}{8}$ | $\small \frac{1}{8}$ | 0 |
| $\small x = 1$ | 0  | $\small \frac{1}{8}$ | $\small \frac{2}{8}$ | $\small \frac{1}{8}$ |

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
        What is the marginal PMF of \(\small  Y\)?
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
contingency table
:::

## Marginal Distributions
:::{style="font-size: .8em"}
Here is a set of observations of two random variables, $\small  X_1$ and $\small  X_2$:
:::
:::{.center-text}
```{python}
import seaborn as sns
import numpy as np
import pandas as pd

np.random.seed(4)
from scipy.stats import multivariate_normal
df1 = pd.DataFrame(multivariate_normal.rvs(mean = np.array([1, 1]), cov = np.array([[1, 0.8],[0.8, 1]]), 
                                           size = 600),
                  columns = ['X1', 'X2'])

plt.figure(figsize = (4.5,4.5))
sns.scatterplot(data = df1, x = 'X1', y = 'X2')
plt.xlabel(r'$X_1$', fontsize=18);
plt.ylabel(r'$X_2$', fontsize=18);
plt.yticks(fontsize=18)
plt.xticks(fontsize=18)
plt.axis('square');
plt.tight_layout()
```
:::
:::{style="font-size: .8em"}
We can calculate the marginals using Python.
:::

## Marginal Distributions
:::{.center-text}
```{python}
#| echo: true
g = sns.JointGrid(data = df1, x = 'X1', y = 'X2', height = 4.5)
g.plot(sns.histplot, sns.histplot)
g.ax_joint.set_xlabel(r'$X_1$', fontsize=16)
g.ax_joint.set_ylabel(r'$X_2$', fontsize=16);
```
:::

## Independence
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Independent Random Variables </span>
        <p>
       Random variables \(\small X\) and \(\small Y\) are independent if their joint cdf \(\small F\) factors into the product of their marginal cdf’s, \(\small F_{X}\) and \(\small F_{Y}\):

       $$\small{F(x,y) = F_{X}(x)F_{Y}(y)}$$ 

       or equivalently,

       $$\small{P(X\leq x,Y \leq y) = P(X\leq x)P(Y \leq y).}$$ 
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Remark__: Two random variables $\small X$ and $\small Y$ are independent if and only if
$$\small p_{XY}(x,y) = p_X(x)p_Y(y),$$
where $\small p_{XY}$ is their joint PMF and $\small p_{X}$ and $\small p_{Y}$ are their marginal PMFs.
:::

## Independence
:::{style="font-size: .8em"}
__Example__: Let us investigate whether _Grade_ and _StudyEffort_ are independent.

Starting from the marginals 
:::

:::{style="font-size: .7em"}

|low|high|
|---|---|
| 0.70| 0.30|

|A|B|C|
|---|---|---|
| 0.25| 0.37| 0.38|

:::

:::{style="font-size: .8em"}
we compute the new distribution based on the independence assumption:
:::

:::{style="font-size: .7em"}

| |low|high|
|---|---|---|
|A| 0.175| 0.075|
|B| 0.259| 0.111|
|C|0.266| 0.114|

:::
:::{style="font-size: .8em"}

The distribution is not equal to the original one, so they are not independent.
:::

:::{.notes}

| |low|high|
|---|---|---|
|__A__| 0.07| 0.18|
|__B__| 0.28| 0.09|
|__C__|0.35| 0.03|


:::

## Independence
:::{style="font-size: .8em"}
__Example (continued)__:
Since the resulting ditribution is not the same as the original joint distribution,  _Grade_ and _StudyEffort_ are not independent.

__Example__: <span style="color:rgb(1, 180, 180);">Are $X_1$ and $X_2$ independent?</span>

:::{.center-text}
```{python}
plt.figure(figsize = (4.5,4.5))
sns.scatterplot(data = df1, x = 'X1', y = 'X2')
plt.xlabel(r'$X_1$', fontsize=18);
plt.ylabel(r'$X_2$', fontsize=18);
plt.axis('square');
```
:::

:::

## Independence
:::{style="font-size: .8em"}
__Example__: The datasets below have approximately the same marginals. 

:::{.columns}
::: {.column width="50%"}
```{python}
np.random.seed(4)
from scipy.stats import multivariate_normal
df2 = pd.DataFrame(multivariate_normal.rvs(mean = np.array([1, 1]), cov = np.array([[1, 0],[0, 1]]), 
                                           size = 600),
                  columns = ['X1', 'X2'])
#plt.figure(figsize=(4.5, 4.5))
g = sns.JointGrid(data=df2, x='X1', y='X2', height=5)
g.plot(sns.scatterplot, sns.histplot)
g.ax_joint.set_xlabel(r'$X_1$', fontsize=18)
g.ax_joint.set_ylabel(r'$X_2$', fontsize=18)
g.figure.subplots_adjust(top=0.9)
plt.suptitle('$X_1$ and $X_2$: Independent', fontsize=20)
plt.show()

```
:::

::: {.column width="50%"}
```{python}
g = sns.JointGrid(data = df1, x = 'X1', y = 'X2', 
                   height = 5)
g.plot(sns.scatterplot, sns.histplot)
g.ax_joint.set_xlabel(r'$Y_1$', fontsize=18)
g.ax_joint.set_ylabel(r'$Y_2$', fontsize=18)
g.figure.subplots_adjust(top=0.9) # Reduce plot to make room 
plt.suptitle('$Y_1$ and $Y_2$: Not Independent', fontsize=20);
plt.show()
```
:::
:::
:::

## Covariance
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Covariance </span>
        <p>
       The covariance of \( \small X_1\) and \( \small X_2\), denoted \( \small \operatorname{Cov}(X_1, X_2)\) is defined as:
    
        $$\small{ \operatorname{Cov}(X_1, X_2) = E[(X_1 - \overline{X_1})(X_2 - \overline{X_2})] }$$

        where \(\small \overline{X_1}\) and \(\small \overline{X_2}\) are the means of \(\small X_1\) and \(\small X_2.\)
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Notation__: We often denote $\small \operatorname{Cov}(X,Y)$ as $\small \sigma_{XY}$.


We know how to find the expected value of a random variable. But how exactly do we compute 
$\small E[(X_1 - \overline{X_1})(X_2 - \overline{X_2})]?$
:::

## Covariance
:::{style="font-size: .8em"}
For discrete $\small X_1$ and $\small X_2$ with joint probability $\small P(X_1=x_1, X_2=x_2) = p(x_1,x_2)$ the covariance is computed as

$$ \small \operatorname{Cov}(X_1, X_2) = \sum_{(x_1,x_2)\in S} (x_1 - \overline{X_1})(x_2 - \overline{X_2}) p(x_1,x_2). $$

Here, $\small S$ is the __support__ of $\small X_1$ and $\small X_2$.

__Example__: Suppose that $\small X$ and $\small Y$ have the joint probability mass function given below.

| | $\small x = 5$ |$\small x = 6$| $\small x = 7$| 
| :---: | :---: | :---: | :---: | 
| $\small y = 8$ | 0 | 0.4 | 0.1 |
| $\small y = 9$ | 0.3  | 0 | 0.2 | 
:::

## Covariance
:::{style="font-size: .8em"}
__Example (continued)__: <br>
The support $S$ is the set $\{\small (5,8),(6,8),(7,8), (5,9),(6,9),(7,9)\}.$

The marginal probabilities are given by

| | $\small x = 5$ |$\small x = 6$| $\small x = 7$| 
| :---: | :---: | :---: | :---: | 
| $\small p_X(x)$ | 0.3 | 0.4 | 0.3 |

| | $\small y = 8$ |$\small y = 9$| 
| :---: | :---: | :---: | 
| $\small p_Y(y)$ | 0.5 | 0.5 | 

The mean of $\small X$ can be computed using $\small p_X(x)$ as follows

$$ \small \overline{X} = 5 \cdot 0.3 + 6 \cdot 0.4 + 7 \cdot 0.3 = 6.$$

Similarly, we find that $\small \overline{Y} = 8.5.$

:::
:::{.notes}
| | $\small x = 5$ |$\small x = 6$| $\small x = 7$| 
| :---: | :---: | :---: | :---: | 
| $\small y = 8$ | 0 | 0.4 | 0.1 |
| $\small y = 9$ | 0.3  | 0 | 0.2 | 
:::

## Covariance
:::{style="font-size: .8em"}
__Example (continued)__: Now it remains to compute the covariance itself:

$$\small{\operatorname{Cov}(X,Y) = (5-6)(8-8.5)(0) + (6-6)(8-8.5)(0.4) + (7-6)(8-8.5)(0.1)}$$

$$\small{+ (5-6)(9-8.5)(0.3) + (6-6)(9-8.5)(0) + (7-6)(9-8.5)(0.2) = -0.1.}$$

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Properties of Covariance </span>
        <p>
       $$\small \operatorname{Cov}(X,Y) = \operatorname{Cov}(Y,X),$$
       $$\small \operatorname{Var}(X) = \operatorname{Cov}(X,X).$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

We typically _**normalize**_ covariance by the standard deviations and obtain the __correlation coefficient__ (or __Pearson's correlation coefficient__ or __Pearson's__ $\rho.$)

:::

:::{.notes}
| | $\small x = 5$ |$\small x = 6$| $\small x = 7$| 
| :---: | :---: | :---: | :---: | 
| $\small y = 8$ | 0 | 0.4 | 0.1 |
| $\small y = 9$ | 0.3  | 0 | 0.2 | 

$$ \small \operatorname{Cov}(X_1, X_2) = \sum_{(x_1,x_2)\in S} (x_1 - \overline{X_1})(x_2 - \overline{X_2}) p(x_1,x_2). $$
:::

## Correlation
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Correlation </span>
        <p>
       If \(\small X\) and \(\small Y\) are jointly distributed random variables and the variances of both \(\small X\) and \(\small Y\) exist and the variances are nonzero, then the correlation of \(\small X\) and \(\small Y\) , denoted by \(\small \rho\), is

       $$\small \rho(X_1,X_2) = \frac{E\left[(X_1-\overline{X_1})(X_2-\overline{X_2})\right]}{\sigma_{X_1} \sigma_{X_2}}.$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

Because it is normalized, $\small \rho(X_1, X_2)$ takes on values between -1 and 1.   

If $\small \rho(X_1, X_2) = 0$ then $\small X_1$ and $\small X_2$ are **uncorrelated**.   <br>
If $\small \rho(X_1, X_2) > 0$ then $\small X_1$ and $\small X_2$ are **positively correlated**. <br>
if $\small \rho(X_1, X_2) < 0$ they are **negatively correlated** or (**anticorrelated**).
:::

## Correlation
:::{style="font-size: .8em"}
_**If $X_1$ and $X_2$ are uncorrelated, are they independent?**_

In general, __NO__.
<br><br>

If $X_1$ and $X_2$ are uncorrelated, that means the relationship between them can't be predicted by _**a linear model.**_

However, this does not mean that they are necessarily independent!
:::

## Like Father, Like Son?
:::{style="font-size: .8em"}
Let us look at the covariance and correlation of heights of fathers and their sons.
:::

:::{.center-text}
```{python}
#| echo: true
# Compute covariance between Father_Height and Son_Height
covariance = df['Father_Height'].cov(df['Son_Height'])
print(f"The covariance between the heights is {covariance:.4f}")

# Compute correlation between Father_Height and Son_Height
correlation = df['Father_Height'].corr(df['Son_Height'])
print(f"The correlation between the heights is {correlation:.4f}")
```
:::


:::{.columns}
::: {.column width="20%"}
:::{.center-text}
<img src="images/joint distributions/father_son.png" width=150/>
:::
:::
::: {.column width="80%"}
:::{style="font-size: .8em"}
The positive covariance indicates that taller fathers tend to have taller sons.

The correlation of 0.42 suggests a moderate positive linear relationship between father and son heights.
:::
:::
:::

:::{.notes}
cov() is a Pandas method

It computes the covariance between two Series (columns)
:::

## Group Question 1

:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<img src="images/joint distributions/PMF_table.png" width=900/><br>

\(\small X\) and \(\small Y\) are discrete random variables. Both \(\small X\) and \(\small Y\) can assume values 0, 1, and 2. The incomplete table above is meant to depict the joint distribution of \(\small X\) and \(\small Y\), and the marginal distributions of \(\small X\) and \(\small Y.\)<br>

a. Complete the table. <br>
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

## Group Question 1 

:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
b. Find the expected value of \(\small X\) and the expected value of \(\small Y\).<br>
    <br>
    <br>
    <br>
c. Find the covariance of \(\small X\) and \(\small Y\). <br>
    <br>
    <br>
    <br>
    <br>
    <br>
d. Are \(\small X\) and \(\small Y\) independent? Explain your answer.
    <br>
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
This is quesion 8 from the practice exam for Fall 24.
:::