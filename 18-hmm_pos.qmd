---
title: HMMs for Parts-of-Speech Tagging

author: "CDS DS-122<br>Boston University"
format: 
    revealjs:
        math: true
        css: 
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true 
---

## Parts-Of-Speech Tagging
:::{style="font-size: .8em"}
In Natural Language Processing (NLP), associating each word in a piece of text with a proper part of speech (POS) is known as _POS tagging_ or _POS annotation_. 

POS tags are also known as word classes, morphological classes, or lexical tags.

:::{.center-text}
<img src="images/hmm/POS_example.png" width=700/>
:::

Ambiguous words make POS tagging a nontrivial task.

:::

## Ambiguous Words
:::{style="font-size: .8em"}
In each of the following sentences the word well belongs to a different part of speech.

- He did not feel very well. (adjective)
- She took it well, all things considered. (adverb)
- Well, this homework took me forever to complete. (interjection)
- The well is dry. (noun)
- Tears were beginning to well in their eyes. (verb)


:::

## Parts-Of-Speech Tagging
:::{style="font-size: .8em"}
Back in the days, the POS annotation was done manually. Assigning POS tags manually within modern multibillion-word corpora is unrealistic and automatic tagging is used instead.

:::{.center-text}
<img src="images/hmm/POS_1.png" width=700/>
:::
:::

## Parts-Of-Speech Tagging
:::{style="font-size: .8em"}
Nowadays, manual annotation is typically used on a small portion of a text to create training data. After that, different techniques, such as transformation based tagging, deep learing models, and probabilistic tagging, can be used to develop an automated POS tagger. 

- Transformation based approaches combine handcrafted rules  and automatically induced rules that are generated during the training. 
- Deep learning tools assign POS tags to words based on the discovered patterns during the training. 
- Probabilistic tagging include simple frequency-based methods and more complex models like Hidden Markov Models.
:::

## Learning Objectives
:::{style="font-size: .8em"}

- The hidden Markov model behind Parts-Of-Speech Tagging
- Calculation of transition probabilities 
- Calculation of emission probabilities 
- Pre-processing
- Application of HMMs for POS-tagging for an 8-word corpus 

:::

## 8-Word Corpus
:::{style="font-size: .8em"}

Provided corpus:

- _Mary Jane can see Will._
- _Today Spot will see Mary._
- _Will Jane spot Mary?_
- _Mary will pat Spot._

Considered parts of speech:<br>
noun, modal verb, verb, and other POS tags.
<br>
<br>

__Question__: Use the provided corpus to annotate _Will can spot Mary._

:::

##  Hidden and Observable States
:::{style="font-size: .8em"}
When we use HMMs for POS tagging, the _**tags serve as the hidden states**_ and the _**words represent the observable states**_. If we distinquish only between<br> _**nouns**_ ($NN$), _**verbs**_
($VB$), and _**other**_ ($O$) tags, then $NN$, $VB$, and $O$ represent the hidden states. The observable states can be _**going, to, walk**_, etc.

:::{.center-text}
<img src="images/hmm/POS_diagram.svg" width=600/>
:::

:::

## Calculating Probabilities 
:::{style="font-size: .8em"}
How do we define the transition and emission probabilities for POS tagging? <br>
We will introduce the computation of the probabilities based on the following toy corpus.

:::{.center-text}
<img src="images/hmm/toy_corpus_plain.svg" width=500/>
:::

For simplicity we will distinquish only between _**nouns**_ ($NN$), _**verbs**_ ($VB$), and _**other**_ ($O$)  tags.

:::

## Transition Probabilities 
:::{style="font-size: .8em"}
We assign blue background color to verbs, pink to nouns, and green to other parts of speech:

:::{.center-text}
<img src="images/hmm/toy_corpus_color.svg" width=350/>
:::

The number of times that a green tag is followed by a blue tag is 2. At the same time, the number of all tag pairs starting with a green tag is 3. Therefore, the transition probability from a green tag to a blue one is $\frac{2}{3}$.
In other words, $P(VB|O) = \frac{2}{3}$ for this example.
:::

## Transition Probabilities 
:::{style="font-size: .8em"}
More formally, the transition probabilities are computed in two steps. <br>
__Step 1__: Count the occurences of tag pairs

$$\small{L(t_{i-1}, t_i) \quad \forall i \in \{1,\dots, N\}.}$$

Here, $N$ is the total number of POS tags.<br>
That is, for each pair of tags $(t_{i-1}, t_i)$, we count how many times tag $t_{i-1}$ is followed by tag $t_i$.<br>
__Step 2__: Calculate the conditional probabilities using the obtained counts

$$\small{P(X_n = t_i|X_{n-1} = t_{i-1}) = \frac{L(t_{i-1}, t_i)}{\sum_{j=1}^{N} L\left(t_{i-1}, t_j\right)}.}$$

That is, we normalize the number of times that tag $t_{i-1}$ is followed by tag $t_i$ by the total number of tag pairs that start with tag $t_{i-1}$.
:::

## Transition Probabilities 
:::{style="font-size: .8em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
   Compute \(P(NN|O)\) for the toy corpus. <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::{.center-text}
<img src="images/hmm/toy_corpus_color.svg" width=350/>
:::


:::

## Emission Probabilities 
:::{style="font-size: .8em"}

:::{.center-text}
<img src="images/hmm/toy_corpus_color.svg" width=350/>
:::

To compute the emission probabilities we need to count the co-occurrences of a part of speech tag with a specific word. For example, the word "We" occurs 2 times in our corpus, tagged with a green tag for $O$ for _other_ (i.e., not a verb nor a noun). In total, the green tag appears three times in this corpus. Thus, the probability that the hidden state $O$ emits the word "We", $P(We|O)$, is $\frac{2}{3}$.
:::

## Emission Probabilities 
:::{style="font-size: .8em"}
The emission probabilities are also computed in two steps.<br>
__Step 1__:  Count the co-occurences of tags and words

$$\small{L(t_{i}, w_k) \quad \forall i \in \{1,\dots, N\} \text{ and} \: \: \forall k \in \{1,\dots, K\}.}$$

Here, $K$ is the total number of words. <br>
That is, for each tag-word pair $(t_{i}, w_k)$, we count how many times the word $w_k$ is associated with the tag $t_i$. <br>
__Step 2__:  Calculate the conditional probabilities using the obtained counts

$$\small{P(O_n = w_k|X_n = t_{i}) = \frac{L(t_{i}, w_k)}{\sum_{j=1}^{K} L\left(t_{i}, w_j\right)}.}$$

That is, to find the conditional probabilities we divide $L(t_{i}, w_k)$ by the total number of words associated with the same tag.
:::

## Emission Probabilities 
:::{style="font-size: .8em"}


```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
   Find \(P(The|O)\) and \(P(algorithm|NN)\) for the toy corpus. <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::{.center-text}
<img src="images/hmm/toy_corpus_color.svg" width=350/>
:::


:::

## 8-Word Corpus
:::{style="font-size: .8em"}

### Pre-processing
Before we can construct the HMM, we need to identify the beginning of each sentence by adding the start of sentence tag ```<s>```. In addition, we replace capital letters by lower case letters. The punctuation stays intact. 

- ```<s>``` mary jane can see will.
- ```<s>``` today spot will see mary.
- ```<s>``` will jane spot mary?
- ```<s>``` mary will pat spot.

:::

## 8-Word Corpus
:::{style="font-size: .8em"}
### Manual annotation
Since the training set is relatively small, we can assign the tags manually, using color coding. Nouns are shown in red color, modal verbs in black, non-modal verbs in blue, and all other tags in green.

- ```<s>``` <font color='red'>mary jane</font> <font color='black'>can</font> <font color='blue'>see</font> <font color='red'>will</font><font color='green'>.</font>
- ```<s>``` <font color='green'>today</font> <font color='red'>spot</font> <font color='black'>will</font> <font color='blue'>see</font> <font color='red'>mary</font><font color='green'>.</font>
- ```<s>``` <font color='black'>will</font> <font color='red'>jane</font> <font color='blue'>spot</font> <font color='red'>mary</font><font color='green'>?</font>
- ```<s>``` <font color='red'>mary</font> <font color='black'>will</font> <font color='blue'>pat</font> <font color='red'>spot</font><font color='green'>.</font>
:::

## 8-Word Corpus
:::{style="font-size: .8em"}
### Transition probabilities
The below table shows the number of occurences of different tag pairs. It includes ```<s>``` tag that is needed to define the initial probabilities.


| 2nd tag\1st tag | ```<s>``` | $NN$ |$MD$ |$VB$ |$O$ |
| ---| --- |  --- |  --- |  --- | --- | 
|$NN$ | 2 | 1 | 1 | 4 | 1 |
|$MD$ | 1 | 3 | 0 | 0 | 0 |
|$VB$ | 0 | 1 | 3 | 0 | 0 |
|$O$  | 1 | 4 | 0 | 0 | 0 |
|__Total__| __4__ | __9__ | __4__ | __4__ | __1__ |
:::

## 8-Word Corpus
:::{style="font-size: .8em"}
Dividing the number of occurences of each tag pair $(t_{i-1},t_i)$ by the total number of tag pairs that starts with tag $t_{i-1}$ yields the transition matrix and the initial probabilities.

$$ A = \begin{bmatrix}
1/9 & 1/4 & 1 & 1\\
1/3 & 0 & 0 & 0 \\
1/9 & 3/4 & 0 & 0\\
4/9 & 0 & 0 & 0
\end{bmatrix} \: \text{ and } \:
\pi = \begin{bmatrix}
1/2 \\ 1/4 \\ 0 \\ 1/4
\end{bmatrix}.
$$

:::

## 8-Word Corpus
:::{style="font-size: .65em"}
### Emission probabilities
<!-- The following table shows that in the considered corpus two words, _will_ and _spot_, are used as different parts of speech.  -->
| word\tag | $NN$ |$MD$ |$VB$ |$O$ |
| ---| --- |  --- |  --- |  --- | 
|mary | 4 | 0 | 0 | 0 | 
|jane | 2 | 0 | 0 | 0 |
|will | 1 | 3 | 0 | 0 |
|spot | 2 | 0 | 1 | 0 |
|can  | 0 | 1 | 0 | 0 |
|see  | 0 | 0 | 2 | 0 |
|pat  | 0 | 0 | 1 | 0 |
|today| 0 | 0 | 0 | 1 |
|.    | 0 | 0 | 0 | 3 |
|?    | 0 | 0 | 0 | 1 |
|__Total__| __9__ | __4__ | __4__ | __5__ |
:::

## 8-Word Corpus
:::{style="font-size: .8em"}
Dividing the number of co-occurences of a POS tag with a certain word by the total number of times the tag is used provides us with the emission matrix.

$$\small{B = \begin{bmatrix}
4/9 & 0 & 0 & 0\\
2/9 & 0 & 0 & 0\\
1/9 & 3/4 & 0 & 0\\
2/9 & 0 & 1/4 & 0\\
0 & 1/4 & 0 & 0\\
0 & 0 & 1/2 & 0\\
0 & 0 & 1/4 & 0\\
0 & 0 & 0 & 1/5\\
0 & 0 & 0 & 3/5\\
0 & 0 & 0 & 1/5\\
\end{bmatrix}.}$$

Together $A, B$ and $\pi$ define the HMM.
:::

## 8-Word Corpus
:::{style="font-size: .8em"}
To assign POS tags to a sentence, we will use the Python implementation of the Viterbi algorithm.

```{python}
import numpy as np
```

```{python}
#| echo: true
def Viterbi(y, A, B, Pi):
    N = A.shape[1] # cardinality of the state space
    T = len(y) # length of the observed sequence
    # Initialize C & D
    C = np.empty((N, T), 'd') #'d' stands for type double
    D = np.empty((N, T), 'B') #'B' stands for type unsigned integer 

    # Initialization stage
    C[:, 0] = B[y[0], :] * Pi.T
    D[:, 0] = 0

    # Forward pass
    for i in range(1, T):
        C[:, i] = np.max(B[y[i], :, np.newaxis] * A * C[:, i - 1], 1)
        D[:, i] = np.argmax(B[y[i], :, np.newaxis] * A * C[:, i - 1], 1)
    D[:,1:] =  D[:,1:] + 1 # hidden states indices start with 1

    # Backward pass
    x = np.empty(T, 'B')
    x[-1] = np.argmax(C[:, T - 1]) + 1 # finds the value of s
    for i in reversed(range(1, T)): 
        x[i - 1] = D[x[i] - 1, i]

    return x, C, D
```
:::

## 8-Word Corpus
:::{style="font-size: .8em"}

```{python}
#| echo: true
# HMM: transition matrix, emission matrix, and initial probabilities
A = np.array([[1/9,1/4,1,1],[1/3,0,0,0],[1/9,3/4,0,0],[4/9,0,0,0]])
B = np.array([[4/9,0,0,0],[2/9,0,0,0],[1/9,3/4,0,0],[2/9,0,1/4,0],
              [0,1/4,0,0],[0,0,1/2,0],[0,0,1/4,0],[0,0,0,1/5],
              [0,0,0,3/5],[0,0,0,1/5]])
Pi = np.array([[1/2],[1/4],[0],[1/4]])

# Observed sequence: "Will can spot Mary."
O = np.array([2,4,3,0,8])
```

Note: The observed sequence is encoded in the same way as the emission matrix. For example, _mary_ corresponds with index 0, _jane_ with 1, etc.
:::

## 8-Word Corpus
:::{style="font-size: .8em"}
```{python}
#| echo: true
# Result
X, C, D = Viterbi(O,A,B,Pi)
print("Matrix C: \n", C)
print("Matrix D: \n", D)
print("Answer: \n", X)
```
:::

## 8-Word Corpus
:::{style="font-size: .8em"}
From the computation of the transition matrix, we know that
- 1 represents a noun,
- 2 represents a modal verb,
- 3 represents a non-modal verb,
- 4 represents other POS tags.

This implies that the answer that we obtained, $\begin{bmatrix} 1 & 2 & 3 & 1 & 4 \end{bmatrix}$, corresponds with the following sequesnce of POS tags:<br>
noun, modal verb, non-modal verb, noun, and other tag. <br>
The model assigned all the POS tags correctly, because the observed sequence was _Will can spot Mary._ 
:::

## Group Question 1
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
   Consider one of the poems that was written by Ezra Pound:<br><br>

In a Station of the Metro<br>
The apparition of these faces in the crowd:<br>
Petals on a wet, black bough.<br>  <br>

a. Construct the transition matrix for POS tagging based on this poem.
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
b. Is there anything unusual about this transition matrix?
    <br><br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
1/3
:::

<!-- ## Group Question 1
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
   Consider the following corpus that consists of one sentence (including the punctuation):<br>
<br>
<em>May the Force be with you.</em> <br>
<br>
Assume that we distinguish only between nouns (\(NN\)), verbs (\(VB\)), and other (\(O\)) tags. Find \(P(NN|O)\), the transition probability from the other tag to the noun tag.  <br>
    <br>
    <br>
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
1/3
::: -->

