---
title: Bayesian Testing & Decision Making
author: "CDS DS-122<br>Boston University"
format:
    revealjs:
        math: true
        css:
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom, randint
from IPython.core.display import HTML
from collections import Counter

```

## Learning Objectives
:::{style="font-size: .8em"}

- Understanding Bayes factors for hypothesis testing
- Applying Bayesian hypothesis testing to the Euro problem
- Using posterior predictive distributions for decision-making
- Implementing Thompson sampling for the multi-armed bandit problem

:::

## The Euro Coin Problem - Reminder

:::{style="font-size: .7em"}


From David MacKay's _Information Theory, Inference, and Learning Algorithms_:

> A statistical statement appeared in _The Guardian_ on Friday, January 4, 2002:
>
> "When spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110. 'It looks very suspicious to me,' said Barry Blight, a statistics lecturer at the London School of Economics. 'If the coin were unbiased, the chance of getting a result as extreme as that would be less than 7%.'"

**Question:** Do these data give evidence that the coin is biased rather than fair?

:::

## The Frequentist Answer

:::{style="font-size: .8em"}

We can compute the p-value: the probability of observing data this extreme if the coin were fair.

Under the null hypothesis ($p = 0.5$), the number of heads follows a binomial distribution. We want:

$$p\text{-value} = P(X \geq 140) + P(X \leq 110) $$

$$= \sum_{k=140}^{250} \binom{250}{k} (0.5)^{250} + \sum_{k=0}^{110} \binom{250}{k} (0.5)^{250} = 0.0664$$

So indeed, "if the coin were unbiased, the chance of getting a result as extreme as that would be less than 7%."

**But what's the Bayesian answer?**

:::

## Bayesian Estimate for p

:::{style="font-size: .7em"}

Previously, we estimated the probability of heads, $p$, using Bayesian updating:

```{python}
#| echo: true
def update(distribution, likelihood):
    '''Standard Bayesian update function'''
    distribution['probs'] = distribution['probs'] * likelihood
    prob_data = distribution['probs'].sum()
    distribution['probs'] = distribution['probs'] / prob_data
    return distribution

# Start with uniform prior
p_dist = pd.DataFrame(index=np.arange(101)/100)
p_dist['probs'] = 1/101  # uniform

# Compute likelihood: 140 heads in 250 flips
likelihood = [binom.pmf(140, 250, p) for p in p_dist.index]

update(p_dist, likelihood);
```

:::

## Posterior Distribution for p

:::{style="font-size: .7em"}

```{python}
#| echo: false
#| fig-align: center
ax = p_dist.plot(lw=3, legend=False, color='blue', figsize=(10, 4))
plt.axvline(0.5, color='red', linestyle='--', lw=2, label='Fair coin (p=0.5)')
plt.xlabel('Probability of Heads (p)', size=16)
plt.ylabel('Probability', size=16)
plt.title('Posterior Distribution of p', size=18)
plt.legend(fontsize=14);
```

The posterior mean (MMSE) is about 0.56, with a 90% credible interval from 0.51 to 0.61.

**But this doesn't directly answer:** Is the coin biased or fair?

:::

## Bayesian Hypothesis Testing

:::{style="font-size: .8em"}

Instead of estimating $p$, we can compare two **hypotheses**:

- **$H_{\text{fair}}$:** The coin is fair ($p = 0.5$)
- **$H_{\text{biased}}$:** The coin is biased toward heads ($p$ uniformly distributed between 0.55 and 0.95)

Since we observed more heads (140) than tails (110), if the coin is biased, it's likely biased toward heads.

```{python}
#| echo: true
# Create "biased toward heads" distribution
biased_heads = pd.DataFrame(index=np.arange(101)/100)
biased_heads['probs'] = 0
# Uniform distribution from 0.55 to 0.95
biased_heads.loc[0.55:0.95, 'probs'] = 1
biased_heads['probs'] = biased_heads['probs'] / biased_heads['probs'].sum()
```

:::

## Visualizing the Hypotheses

:::{style="font-size: .8em"}

```{python}
#| echo: false
#| fig-align: center
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))

# Fair hypothesis
ax1.bar([0.5], [1.0], width=0.01, color='red', alpha=0.7)
ax1.set_xlabel('Probability of Heads (p)', size=14)
ax1.set_ylabel('Probability', size=14)
ax1.set_title('Fair Hypothesis: p = 0.5', size=16)
ax1.set_xlim(0, 1)

# Biased hypothesis
biased_heads.plot(ax=ax2, lw=3, legend=False, color='blue')
ax2.set_xlabel('Probability of Heads (p)', size=14)
ax2.set_ylabel('Probability', size=14)
ax2.set_title('Biased Hypothesis: p ~ Uniform(0.55, 0.95)', size=16);
```

:::

## Computing Likelihoods

:::{style="font-size: .8em"}

**Likelihood under $H_{\text{fair}}$:** 

Probability of 140 heads in 250 flips with $p = 0.5$

```{python}
#| echo: true
likelihood_fair = binom.pmf(140, 250, p=0.5)
print(f"P(Data | Fair) = {likelihood_fair:.6f}")
```

**Likelihood under $H_{\text{biased}}$:** 

Average over all possible biased values

```{python}
#| echo: true
likelihood_vector = [binom.pmf(140, 250, p) for p in biased_heads.index]
likelihood_biased = np.sum(biased_heads['probs'] * likelihood_vector)
print(f"P(Data | Biased toward heads) = {likelihood_biased:.6f}")
```

:::

## Bayes Factors

:::{style="font-size: .8em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Bayes Factor</span>
        <p>
        The Bayes factor \(K\) is the ratio of likelihoods under two hypotheses:
        $$K = \frac{P(\text{Data} | H_A)}{P(\text{Data} | H_B)}$$

        It measures the strength of evidence in favor of hypothesis A over B.
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

In our case:

```{python}
#| echo: true
K = likelihood_fair / likelihood_biased
print(f"Bayes Factor (Fair/Biased) = {K:.2f}")
```

**Interpretation:** The Bayes factor will tell us which hypothesis the data supports more strongly.

:::

## Interpreting Bayes Factors

:::{style="font-size: .8em"}

If we start with equal prior probabilities (50% fair, 50% biased), we can convert the Bayes factor to a posterior probability:

$$P(\text{fair} | \text{data}) = \frac{K \cdot P(\text{fair})}{K \cdot P(\text{fair}) + P(\text{biased})}$$

```{python}
#| echo: true
prior_fair = 0.5
posterior_fair = (K * prior_fair) / (K * prior_fair + (1 - prior_fair))
print(f"P(Fair | Data) = {posterior_fair:.2f}")
```

The data moved us from 50% confidence to 56% confidence that the coin is fair.

**Conclusion:** The evidence is weak - we're actually *more* likely to think the coin is fair now.

:::

## Beyond Hypothesis Testing

:::{style="font-size: .8em"}

Is it really useful to make a binary decision about whether a coin is biased?

More useful questions might be:

1. **Prediction:** Based on what we know, what should we expect in the future?

2. **Decision-making:** How can we use predictions to make better decisions? (Whether to gamble with the coin or not)

We already saw prediction with the World Cup problem (posterior predictive distributions).

Next: An example of decision-making with **Bayesian Bandits**.

:::

##  <span style="font-size: 0.9em">Multi-Armed Bandit Problem</span>

:::{style="font-size: .7em"}

A slot machine is sometimes called a "one-armed bandit."

:::{.center-text}
<img src="images/bayesian_testing/slot_machines.jpeg" width=400/>
:::

**Scenario:** You have 4 slot machines. Each has a different (unknown) probability of winning. How do you maximize your winnings?

- Play all equally until you find the best? (Too much exploration)
- Pick the current best and play it exclusively? (Too much exploitation)

**Solution:** Balance exploration and exploitation!

:::

## Setting Up the Bandits

:::{style="font-size: .8em"}

We start with uniform priors for each machine (we know nothing about win probabilities):

```{python}
#| echo: true
p_prior = pd.DataFrame(index=np.arange(101)/100)
p_prior['probs'] = 1/101
beliefs = [p_prior.copy() for i in range(4)]
```

```{python}
#| echo: false
#| fig-align: center
fig, axes = plt.subplots(1, 4, figsize=(14, 3))
for i, pmf in enumerate(beliefs):
    pmf.plot(ax=axes[i], lw=3, legend=False, color='blue')
    axes[i].set_title(f'Machine {i}', size=12)
    axes[i].set_xlabel('Win Probability (p)', size=10)
    axes[i].set_ylabel('Probability', size=10)
plt.tight_layout();
```

:::

## Updating Beliefs

:::{style="font-size: .8em"}

After playing a machine, we update our beliefs based on the outcome:

- **Win:** Multiply by $p$ (higher $p$ values become more likely)
- **Loss:** Multiply by $(1-p)$ (lower $p$ values become more likely)

```{python}
#| echo: true
def update_bandit(belief, outcome):
    '''Update belief about a machine based on win/loss outcome'''
    if outcome == 'W':
        update(belief, belief.index)
    elif outcome == 'L':
        update(belief, 1 - belief.index)
```

:::

## Example: One Machine, 10 Plays

:::{style="font-size: .8em"}

Suppose we play one machine 10 times with outcomes: W, L, L, L, L, L, L, L, L, L

```{python}
#| echo: false
#| fig-align: center
bandit = p_prior.copy()
for outcome in 'WLLLLLLLLL':
    update_bandit(bandit, outcome)

bandit.plot(lw=3, legend=False, color='blue', figsize=(10, 4))
plt.xlabel('Win Probability (p)', size=16)
plt.ylabel('Probability', size=16)
plt.title('Posterior After 1 Win, 9 Losses', size=18);
```

The posterior shifts toward low win probabilities - this looks like a bad machine!

:::

## Let's Play! (Interactive Demo)

:::{style="font-size: .7em"}

See jupyter notebook

:::


## What Strategy Did We Use?

:::{style="font-size: .8em"}

After playing manually, we probably:
- Explored all machines early on
- Started favoring machines that seemed better
- Occasionally checked "worse" machines to see if we were wrong

**Question:** Is there an optimal strategy that automatically balances exploration and exploitation?

**Answer:** Yes! **Thompson Sampling**

:::

## Thompson Sampling Strategy

:::{style="font-size: .6em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Thompson Sampling Algorithm</span>
        <p>
        <b>Setup:</b> For each machine \(i\), maintain posterior distribution \(P(\theta_i | \text{data})\) over win probability \(\theta_i\).
        <br><br>
        <b>At each time step:</b><br>
        1. <b>Sample:</b> For each machine \(i\), draw one random value \(p_i \sim P(\theta | \text{data})\)<br>
        2. <b>Choose:</b> Play machine \(j = \arg\max_i p_i\) (the one with highest sampled value)<br>
        3. <b>Observe:</b> Get outcome \(x_j\) (Win or Loss)<br>
        4. <b>Update:</b> Update posterior \(P(\theta_j | \text{data}) \gets P(\theta_j | \text{data}, x_j)\) using Bayes' rule<br>
        5. <b>Repeat</b>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

**Key insight:** Machines with higher expected win rates get sampled more often, but wide posteriors (high uncertainty) give other machines a chance to be selected.

:::

## <span style="font-size: 0.8em">Implementing Thompson Sampling</span>

:::{style="font-size: .8em"}

```{python}
#| echo: true
def choose(beliefs):
    '''Use Thompson sampling to choose a machine'''
    ps = [np.random.choice(b.index, p=b['probs']) for b in beliefs]
    return np.argmax(ps)

# Simulate playing a machine
actual_probs = [0.10, 0.20, 0.30, 0.40]  # Unknown to us!
counter = Counter()

def play(i):
    '''Play machine i and return outcome'''
    counter[i] += 1
    p = actual_probs[i]
    if np.random.random() < p:
        return 'W'
    else:
        return 'L'
```

:::

## Running the Strategy

:::{style="font-size: .7em"}

Let's play 200 times using Thompson sampling:

```{python}
#| echo: true
# Reset beliefs and counter
beliefs = [p_prior.copy() for i in range(4)]
counter = Counter()
play_sequence = []
num_plays = 500

for _ in range(num_plays):
    machine = choose(beliefs)
    play_sequence.append(machine)
    outcome = play(machine)
    update_bandit(beliefs[machine], outcome)
```

```{python}
#| echo: false
#| fig-align: center
fig, axes = plt.subplots(1, 4, figsize=(14, 3))
for i, pmf in enumerate(beliefs):
    pmf.plot(ax=axes[i], lw=3, legend=False, color='blue')
    axes[i].set_title(f'Machine {i} (True p={actual_probs[i]:.1f})', size=12)
    axes[i].set_xlabel('Win Probability (p)', size=10)
    axes[i].set_ylabel('Probability', size=10)
plt.tight_layout();
```

:::

## <span style="font-size: 0.8em">How Often Did We Play Each Machine?</span>

:::{style="font-size: .8em"}

```{python}
#| echo: false
index = range(4)
columns = ['Actual P(win)', 'Times played']
df = pd.DataFrame(index=index, columns=columns)
for i in range(4):
    df.loc[i] = [actual_probs[i], counter.get(i, 0)]

df
```

:::

## Evolution of Play Strategy Over Time

:::{style="font-size: .8em"}

How did our strategy change as we learned? Let's look at selection frequency in 20-play windows:

```{python}
#| echo: false
#| fig-align: center
# Calculate selection frequency in bins using the actual play sequence
bin_size = 20
num_bins = num_plays // bin_size
selection_freq = np.zeros((4, num_bins))

for bin_idx in range(num_bins):
    start = bin_idx * bin_size
    end = start + bin_size
    bin_plays = play_sequence[start:end]
    for machine in range(4):
        selection_freq[machine, bin_idx] = bin_plays.count(machine) / bin_size

# Plot
fig, ax = plt.subplots(figsize=(10, 4))
x = np.arange(num_bins) * bin_size + bin_size/2
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
for machine in range(4):
    ax.plot(x, selection_freq[machine], 'o-', lw=2, label=f'Machine {machine} (p={actual_probs[machine]:.1f})',
            color=colors[machine], markersize=6)

ax.set_xlabel('Number of Plays', size=14)
ax.set_ylabel('Selection Frequency', size=14)
ax.set_title('How Often Each Machine Was Selected Over Time', size=16)
ax.legend(fontsize=11, loc='best')
ax.grid(alpha=0.3)
ax.set_ylim(0, 1);
```

**Key observation:** Early exploration (all machines tried) → Later exploitation (focus on Machine 3)!

:::

## Summary

:::{style="font-size: .8em"}

**Key Concepts:**

1. **Bayes factors** compare the evidence for two hypotheses by taking the ratio of likelihoods

2. **Bayesian hypothesis testing** provides an alternative to p-values, but depends on how you define hypotheses

3. **Thompson sampling** uses posterior distributions to make optimal decisions that balance exploration and exploitation

4. **Decision-making** often matters more than hypothesis testing - use predictions to guide actions!

:::


## Group Question 1

:::{style="font-size: .65em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>Quality Control Problem:</b> A factory claims their defect rate is 5%. You heard a rumor from the factory floor that the real rate is 20%. An inspector randomly samples 50 items and finds 8 defects.
<br><br>
Define two hypotheses:
<br>
• \(H_{\text{claimed}}\): p = 0.05<br>
• \(H_{\text{rumor}}\): p = 0.20<br>
<br>
<b>a.</b> Compute P(Data | \(H_{\text{claimed}}\)).
<br><br>
<b>b.</b> Compute P(Data | \(H_{\text{rumor}}\)).
<br><br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Group Question 1 (cont'd)

:::{style="font-size: .65em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>c.</b> Calculate the Bayes factor \(K = \frac{P(\text{Data} | H_{\text{claimed}})}{P(\text{Data} | H_{\text{rumor}})}\).
<br><br><br><br>
<b>d.</b> Which hypothesis does the data support more strongly?
<br><br><br><br><br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Group Question 2

:::{style="font-size: .55em"}

After playing for a while, you have these posterior distributions for 3 slot machines:

```{python}
#| echo: false
#| fig-align: center

# Create three machines with different histories
p_grid = pd.DataFrame(index=np.arange(101)/100)
p_grid['probs'] = 1/101

# Machine A: 7 plays, 6 wins
machine_a = p_grid.copy()
for _ in range(6):
    update(machine_a, machine_a.index)
for _ in range(1):
    update(machine_a, 1 - machine_a.index)

# Machine B: 2 plays, 2 wins
machine_b = p_grid.copy()
for _ in range(2):
    update(machine_b, machine_b.index)
for _ in range(0):
    update(machine_b, 1 - machine_b.index)

# Machine C: 30 plays, 21 wins
machine_c = p_grid.copy()
for _ in range(21):
    update(machine_c, machine_c.index)
for _ in range(9):
    update(machine_c, 1 - machine_c.index)

# Calculate means for display
mean_a = np.sum(machine_a.index * machine_a['probs'])
mean_b = np.sum(machine_b.index * machine_b['probs'])
mean_c = np.sum(machine_c.index * machine_c['probs'])

fig, axes = plt.subplots(1, 3, figsize=(13, 3))
machine_a.plot(ax=axes[0], lw=3, legend=False, color='blue')
axes[0].set_title(f'Machine A: 7 plays, 6 wins\n(mean = {mean_a:.2f})', size=11)
axes[0].set_xlabel('Win Probability (p)', size=10)
axes[0].set_ylabel('Probability', size=10)

machine_b.plot(ax=axes[1], lw=3, legend=False, color='orange')
axes[1].set_title(f'Machine B: 2 plays, 2 wins\n(mean = {mean_b:.2f})', size=11)
axes[1].set_xlabel('Win Probability (p)', size=10)
axes[1].set_ylabel('Probability', size=10)

machine_c.plot(ax=axes[2], lw=3, legend=False, color='green')
axes[2].set_title(f'Machine C: 30 plays, 21 wins\n(mean = {mean_c:.2f})', size=11)
axes[2].set_xlabel('Win Probability (p)', size=10)
axes[2].set_ylabel('Probability', size=10)

plt.tight_layout();
```

:::

## Group Question 2 (cont'd)

:::{style="font-size: .65em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>a.</b> Which machine has the highest expected win probability? The lowest?
<br><br><br>
<b>b.</b> Which machine is most likely to be selected next by Thompson sampling? 
<br><br><br>
<b>c.</b> If you only have <b>1 turn left to play</b>, which machine would you pick? What if you had <b>100 turns left</b>? Explain the difference.
<br><br><br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::
