---
title: Review - Bayesian Inference & Monte Carlo Methods
author: "CDS DS-122<br>Boston University"
format:
    revealjs:
        math: true
        css:
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import gamma, poisson, dirichlet, multinomial, beta, norm, binom
from IPython.core.display import HTML
```

## Overview

:::{style="font-size: .8em"}

This review covers lectures 28-36:

- Bayesian Comparison problems
- Bayesian bandits (Thompson sampling)
- Bayesian hypothesis testing
- Classification (Naive Bayes)
- Conjugate priors
- Monte Carlo integration
- Accept-reject sampling
- Markov Chain Monte Carlo (Metropolis-Hastings)

:::

## The Bayes' Table Method

:::{style="font-size: .75em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Grid Method for Bayesian Inference</span>
        <p>
        <b>Steps:</b><br>
        1. Create a grid of possible parameter values<br>
        2. Compute prior probability at each grid point<br>
        3. Compute likelihood at each grid point<br>
        4. Multiply: posterior \(\propto\) prior × likelihood<br>
        5. Normalize by dividing by the sum<br><br>

        <b>Pros:</b> Exact (given grid), works for any prior/likelihood<br>
        <b>Cons:</b> Curse of dimensionality (\(n^d\) points for \(d\) parameters)
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::


## Practice: Bayes Table (Poisson)

:::{style="font-size: .65em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="blue-box">
        <p>
        <b>Problem:</b> A website receives visitors at an unknown rate \(\lambda\) (visitors/hour). Based on similar websites, you believe \(\lambda\) could be 2, 4, or 6 visitors/hour with equal probability.<br><br>

        <b>Data:</b> In the first hour, you observe 5 visitors.<br><br>

        <b>Problem:</b> Complete the Bayes table to find the posterior distribution, MAP, and MMSE for \(\lambda\)<br>
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::


## Bayesian Hypothesis Testing

:::{style="font-size: .75em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Bayesian Hypothesis Testing</span>
        <p>
        To compare two hypotheses \(H_1\) and \(H_2\) given data:<br><br>

        <b>Bayes factor:</b> \(\text{K} = \frac{P(\text{data} \mid H_1)}{P(\text{data} \mid H_2)} = \frac{\text{likelihood} ( H_1)}{\text{likelihood} ( H_2)}\) <br><br>
        <b>Interpretation:</b> Bayes factor > 1 favors \(H_1\), < 1 favors \(H_2\) <br><br>
        <b>Also known as:</b> Likelihood ratio 
        </p>
    </div>
    """
display(HTML(generate_html()))
```


:::

## Practice: Bayesian Testing

:::{style="font-size: .65em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="blue-box">
        <p>
        <b>Problem:</b> You're testing whether a coin is fair (\(H_1: p = 0.5\)) or biased (\(H_2: p = 0.7\)).<br><br>

        <b>Prior:</b> You believe \(P(H_1) = 0.6\) and \(P(H_2) = 0.4\)<br>
        <b>Data:</b> You flip the coin 10 times and observe 7 heads<br><br>

        <b>Questions:</b><br>
        1. What is the Bayes factor?<br>
        2. What is the posterior probability of \(H_1\)?<br>
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Frequentist vs Bayesian

:::{style="font-size: .6em"}

| Aspect | Frequentist | Bayesian |
|--------|------------|----------|
| **Parameters** | Fixed but unknown | Random variables with distributions |
| **Probability** | Long-run frequency | Degree of belief |
| **Prior** | Not used | Explicitly incorporated |
| **Inference** | Confidence intervals, p-values | Posterior distributions, credible intervals |
| **Interpretation** | "95% of intervals contain true value" | "95% probability parameter is in interval" |

:::

## Classification: MAP Estimation

:::{style="font-size: .65em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Maximum A Posteriori (MAP) Classification</span>
        <p>
        To classify observation \(x\) into one of \(K\) classes:<br><br>

        <b>Classify to class \(k\) that maximizes:</b>
        $$P(C_k \mid x) \propto \text{prior}(C_k) \cdot \text{likelihood}(x \mid C_k) $$

        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Naive Bayes Classifier

:::{style="font-size: .65em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Naive Bayes</span>
        <p>
        For classification with features \(x_1, x_2, \ldots, x_n\):<br><br>

        <b>Naive Bayes assumption:</b> Features are conditionally independent given the class
        $$\text{lik}(x_1, x_2, \ldots, x_n \mid C_k) = \text{lik}(x_1 \mid C_k) \cdot \text{lik}(x_2 \mid C_k) \cdots \text{lik}(x_n \mid C_k)$$

        <b>Classify to class \(k\) that maximizes:</b>
        $$P(C_k \mid x) \propto \text{prior}(C_k) \cdot \text{lik}(x_1, x_2, \ldots, x_n \mid C_k) $$
        
        <b>"Naive" because:</b> Features are rarely truly independent, but this assumption works well in practice! <br><br>

        <b>Why?</b> We care about ranking, not exact probabilities, correlations may be similar across classes, and strength of the individual likelihood signals dominate.
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Practice: Naive Bayes

:::{style="font-size: .6em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="blue-box">
        <p>
        <b>Text classification problem:</b> Classify movie reviews as Positive or Negative.<br><br>

        <b>Training data:</b><br>
        • 60% of reviews are Positive, 40% are Negative<br>
        • Word "amazing" appears in 70% of Positive reviews, 10% of Negative reviews<br>
        • Word "boring" appears in 5% of Positive reviews, 60% of Negative reviews<br>
        • Word "plot" appears in 40% of Positive reviews, 50% of Negative reviews<br><br>

        <b>New review:</b> "The plot was amazing"<br>
        (contains: "plot" and "amazing", does not contain "boring")<br><br>

        <b>Questions:</b><br>
        1. What are the prior probabilities \(P(\text{Positive})\) and \(P(\text{Negative})\)?<br>
        2. Using the Naive Bayes assumption, calculate \(P(\text{``plot" AND "amazing"} \mid \text{Positive})\)<br>
        3. Calculate \(P(\text{``plot" AND "amazing"} \mid \text{Negative})\)<br>
        4. How would you classify this review?<br>
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Conjugate Priors: Overview

:::{style="font-size: .6em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Definition: Conjugate Prior</span>
        <p>
        A prior distribution is <b>conjugate</b> to a likelihood if the posterior has the same distributional family as the prior.
        <br><br>
        <b>Benefit:</b> Bayesian updating becomes simple arithmetic — no need for grids or numerical integration!
        <br><br>
        <b>General pattern:</b> Prior parameters + data -> updated parameters
        </p>
    </div>
    """
display(HTML(generate_html()))
```

**Common pairs:**

- Binomial likelihood = Beta prior
- Poisson likelihood = Gamma prior

NOT on the exam:

- Multinomial likelihood = Dirichlet prior
- Normal likelihood (known $\sigma$) = Normal prior

:::

## Beta-Binomial Conjugacy

:::{style="font-size: .75em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Beta-Binomial Conjugate Pair</span>
        <p>
        <b>Use case:</b> Estimating probability \(\theta \in [0,1]\) (proportions, rates)<br><br>

        <b>Prior:</b> \(\theta \sim \text{Beta}(\alpha, \beta)\)
        $$p(\theta) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}$$

        <b>Likelihood:</b> \(k\) successes in \(n\) trials
        $$P(k \mid \theta) \propto \theta^k(1-\theta)^{n-k}$$

        <b>Posterior:</b> \(\theta \mid \text{data} \sim \text{Beta}(\alpha + k, \beta + n - k)\)
        <br><br>
        <b>Update rule:</b> Add successes to \(\alpha\), add failures to \(\beta\)
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::


## Gamma-Poisson Conjugacy

:::{style="font-size: .75em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Gamma-Poisson Conjugate Pair</span>
        <p>
        <b>Use case:</b> Estimating rate parameter \(\lambda\) for count data<br><br>

        <b>Prior:</b> \(\lambda \sim \text{Gamma}(\alpha, \beta)\)
        $$p(\lambda) \propto \lambda^{\alpha-1} e^{-\lambda\beta}$$

        <b>Likelihood:</b> \(k\) events in time period \(t\)
        $$P(k \mid \lambda) \propto \lambda^k e^{-\lambda t}$$

        <b>Posterior:</b> \(\lambda \mid \text{data} \sim \text{Gamma}(\alpha + k, \beta + t)\)
        <br><br>
        <b>Update rule:</b> Add event count to \(\alpha\), add time to \(\beta\)
        <br>
        <b>Mean:</b> \(\frac{\alpha}{\beta}\) &nbsp; <b>Mode:</b> \(\frac{\alpha-1}{\beta}\) (when \(\alpha > 1\))
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Practice: Conjugate Priors

:::{style="font-size: .65em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="blue-box">
        <p>
        <b>Problem:</b> A website receives visits at an unknown rate \(\lambda\) visits/hour.<br><br>

        <b>Prior:</b> Historical data suggests Gamma(10, 2) — expecting around 5 visits/hour<br>
        <b>Data:</b> You observe 28 visits in 3 hours<br><br>

        <b>Questions:</b><br>
        1. What is the posterior distribution?<br>
        2. What is the posterior mean estimate of \(\lambda\)?<br>
        3. How does this compare to the MLE?<br>
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::


## What to Expect: Python on the Exam

:::{style="font-size: .6em"}

**You will be asked to complete a short Python implementation (fill-in-the-blank style)**

**Topics could include anything from the course, such as:**

- Bayes tables calculations, with MAP and MMSE
- Naive Bayes models (like HW8)
- Accept-reject sampling, Monte Carlo integration, MCMC

**What you need to know:**

- Basic Python syntax: `if` statements, `while` loops, `for` loops
- Basic operations: `min()`, `max()`, exponentiation `**`
- List operations: `append()`, `len()`

**You do NOT need to:**

- Write complete functions from scratch
- Know advanced NumPy or specialized libraries
- Specific NumPy commands, like defining and sampling from distributions

:::

## Programming Example: Bayes Table

:::{style="font-size: .5em"}

**Problem:** Complete the code to compute the posterior distribution using a Bayes table for a coin flip problem. We flip a coin 3 times and observe 2 heads. We're testing three hypotheses about the coin's bias: $p \in \{0.3, 0.5, 0.7\}$ with uniform prior.

```python
def bayes_table_coin(n_heads, n_flips, hypotheses, prior):
    """
    Compute posterior distribution using Bayes table.
    hypotheses: list of possible p values [0.3, 0.5, 0.7]
    prior: list of prior probabilities
    """
    n_tails = n_flips - n_heads

    # Compute likelihood for each hypothesis
    likelihoods = []
    for p in hypotheses:
        likelihood = p**n_heads * (1-p)**n_tails
        likelihoods.append(likelihood)

    # Compute unnormalized posterior
    unnormalized = []
    for i in range(len(hypotheses)):
        unnormalized.append(__________)

    # Normalize to get posterior
    total = __________
    posterior = []
    for u in unnormalized:
        posterior.append(__________)

    # Find MAP estimate
    map_index = posterior.index(__________)
    map_estimate = __________

    return posterior, map_estimate
```

:::

## Monte Carlo Simulation

:::{style="font-size: .75em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Monte Carlo Method</span>
        <p>
        <b>Core idea:</b> Use random sampling to solve problems that might be deterministic in principle.
        <br><br>
        <b>Applications:</b>
        <ul>
        <li><b>Numerical integration:</b> Estimate \(\int_a^b f(x) dx\)</li>
        <li><b>Distribution sampling:</b> Generate points \(x_i\) from a PDF \(p\).</li>
        <li><b>MCMC:</b> Sample from the steady-state distribution of a Markov chain.</li>
        </ul>

        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Monte Carlo Integration

:::{style="font-size: .75em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Monte Carlo Integration</span>
        <p>
        To estimate \(\int_a^b f(x) dx\):<br><br>

        1. Create bounding box: \([a,b] \times [0, M]\) where \(M \geq \max f(x)\)<br>
        2. Generate \(N\) random points uniformly in the box<br>
        3. Count how many fall under curve: \(N_{\text{under}}\)<br>
        4. Estimate: \(\int_a^b f(x) dx \approx (b-a) \times M \times \frac{N_{\text{under}}}{N}\)<br><br>

        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Accept-Reject Sampling

:::{style="font-size: .75em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Accept-Reject Sampling Algorithm</span>
        <p>
        To sample from target distribution \(p(x)\) using proposal \(g(x)\):<br><br>

        <b>Setup:</b> Find \(M\) such that \(M \cdot g(x) \geq p(x)\) for all \(x\)<br><br>

        <b>Algorithm:</b><br>
        1. Sample \(x\) from proposal distribution \(g(x)\)<br>
        2. Sample \(u \sim \text{Uniform}(0, 1)\)<br>
        3. If \(u \leq \frac{p(x)}{M \cdot g(x)}\), <b>accept</b> \(x\)<br>
        4. Otherwise, <b>reject</b> and return to step 1<br><br>

        Then accepted samples have distribution \(p(x)\)
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Practice: Accept-Reject Sampling

:::{style="font-size: .65em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="blue-box">
        <p>
        <b>Problem:</b> Sample from \(p(x) = 2x\) for \(x \in [0, 1]\) using uniform proposal \(g(x) = 1\).<br><br>

        <b>Questions:</b><br>
        1. What is the minimum value of \(M\) such that \(M \cdot g(x) \geq p(x)\) for all \(x \in [0,1]\)?<br>
        2. What is the expected acceptance rate?<br>
        3. If you sample \(x = 0.6\) from the uniform, what is the acceptance probability?<br>
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Markov Chain Monte Carlo (MCMC)

:::{style="font-size: .75em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">MCMC Core Idea</span>
        <p>
        <b>Goal:</b> Sample from complex posterior \(p(\theta \mid \text{data})\) when direct sampling is impossible.
        <br><br>
        <b>Strategy:</b> Construct a Markov chain whose stationary distribution equals the target posterior.
        <br><br>
        <b>Key advantages:</b>
        <ul>
        <li>Works in high dimensions</li>
        <li>Only need posterior ratios (unnormalized posteriors)</li>
        <li>Very general — works for complex models</li>
        </ul>

        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::

## Metropolis-Hastings Algorithm

:::{style="font-size: .7em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">Metropolis-Hastings Algorithm</span>
        <p>
        To sample from posterior \(p(\theta \mid \text{data})\) with a symmetric proposal function \(q\):<br><br>

        1. <b>Initialize:</b> Start with \(\theta_0\)<br>
        2. <b>Propose:</b> At iteration \(t\), propose \(\theta^*\) from \(q(\theta^* \mid \theta_t)\)<br>
        3. <b>Compute acceptance ratio:</b>
        $$\alpha = \min\left(1, \frac{p(\theta^* \mid \text{data}) }{p(\theta_t \mid \text{data}) }\right)$$
        4. <b>Accept/Reject:</b>
           <ul>
           <li>With probability \(\alpha\): set \(\theta_{t+1} = \theta^*\) (accept)</li>
           <li>Otherwise: set \(\theta_{t+1} = \theta_t\) (reject, stay put)</li>
           </ul> <br>
        5. <b>Repeat</b>
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::


## Practice: MCMC

:::{style="font-size: .6em"}

```{python}
#| echo: false
def generate_html():
    return r"""
    <div class="blue-box">
        <p>
        <b>Problem:</b> You're using Metropolis-Hastings to estimate \(\theta\) from data with a symmetric proposal distribution.<br><br>

        <b>Current state:</b> \(\theta_{\text{current}} = 0.5\)<br>
        <b>Proposed state:</b> \(\theta^* = 0.6\)<br>
        <b>Unnormalized posteriors:</b> \(p(0.5 \mid \text{data}) = 0.20\), \(p(0.6 \mid \text{data}) = 0.15\)<br><br>

        <b>Questions:</b><br>
        1. What is the acceptance probability \(\alpha\)?<br>
        2. If a random draw \(u = 0.8\), do you accept or reject?<br>
        3. If instead \(\theta^* = 0.55\) with \(p(0.55 \mid \text{data}) = 0.25\), what is \(\alpha\)?<br>
        4. Why do we sometimes accept moves to lower probability regions?<br>
        </p>
    </div>
    """
display(HTML(generate_html()))
```

:::
