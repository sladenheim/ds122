---
title: Common Discrete Distributions
author: "CDS DS-122<br>Boston University"
format: 
    revealjs:
        math: true
        css: 
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true 
---

## Why Common Discrete Distributions?
:::{style="font-size: .8em"}
Common discrete distributions help us answer questions like:

- Will a customer click on an ad? 
- How many users will open an email? 
- How long until a system fails? 
- How many phone calls will a company receive in one day? 
- What’s the chance of each outcome in a fair game? 
:::

:::{.notes}
A __discrete random variable__ is a random variable that can take on
only a finite or at most a countably infinite number of values.

I would replace this by a historical note on Bernoulli's 
:::


## Learning Objectives

:::{style="font-size: .8em"}
- Introduce common discrete distributions:

    - Bernoulli
    - Binomial
    - Geometric 
    - Poisson 
    - Uniform 

- For each, discuss intuition, PMF, mean, variance, and Python implementation

- Apply the knowledge to horse-kick fatalities problem

:::

:::{.notes}
No need to memorize the details, such as the PMF, the expected value, etc. They will be on the formula sheet.

However, you need to be able to recognize when to use which distribution!
:::

## Horse-Kick Fatalities 
:::{style="font-size: .8em"}

In 1898, Ladislaus Bortkiewicz analyzed deaths caused by horse kicks in the Prussian army. He found that in a certain year, a single corps experienced 4 such deaths.

| Number of Deaths     | Observed  Instances|
|----------------------|--------------------|
| 0          | 109      |
| 1          | 65       | 
| 2          | 22       | 
| 3          | 3        | 
| 4          | 1        |
| 5+         | 0        |

__Question__: Can the observed fatalities follow be described by a common discrete distribution? If so, which one?
:::



## Bernoulli Distribution

:::{.columns}
::: {.column width="35%"}
:::{.center-text}
<img src="images/common distributions/Coin_toss.jpg" width=400/>
:::
:::

::: {.column width="65%"}
:::{style="font-size: .8em"}
An experiment with only two possible outcomes:

- head or tail,
- success or failure,
- defective or nondefective component,
- patient recovers or does not recover, ...

These outcomes are typically decoded as 0 and 1.
:::
:::
:::

:::{style="font-size: .8em"}
Each distribution has at least one parameter that defines its behavior. The Bernoulli distribution has a single _**parameter**_
$p$, representing _**the probability that the random variable equals 1**_.
:::

:::{.notes}
In probability, the terms success and failure are purely mathematical labels with no social, moral, or economic meaning.
:::

## Bernoulli Distribution
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Bernoulli Distribution </span>
        <p>
       A random variable \(\small X\) has a Bernoulli distribution with parameter \(\small  p\) \((\small  0\leq p \leq 1)\) if \(\small X\) can take only the values 0 and 1 and the corresponding probabilities are 
       \(\small  P(X=1) = p\) and \(\small P(X=0) = 1-p.\)
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

__Remark__: The mean of a $\small X$ is $\small p$ and its variance is $\small p(1-p)$. <br>
__Example__: A canonical example is flipping a weighted coin. The coin comes up "heads" (aka "success", aka "1") with probability $p.$ If the coin is fair, $\small p=\frac{1}{2}.$ <br>
<span style="color:rgb(1, 180, 180);">Can you name another example of a Bernoulli random variable?</span>

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Bernoulli Trials </span>
        <p>
       Independent experiments that result in a success with
    probability \(\small p\) and a failure with probability \(\small 1−p\) are called Bernoulli trials.
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Binomial Distribution
:::{style="font-size: .8em"}
The binomial distribution considers $\small N$ Bernoulli trials. Each trial has the probability of a success equal to $\small p$. 
This distribution answers the question "_**What is the probability there will be $\small k$ successes in $\small N$ trials?**_"

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Bernoulli Trials </span>
        <p>
       If \(\small X\) is the number of successes that occur in \(\small N\) trials, then \(\small X\) has a binomial distribution with parameters \(\small N\) and \(\small p\) \((\small 0\leq p \leq 1),\) \(\small X \sim Bin(N,p).\) The PMF of a binomial random variable is given by
    $$\small p(k) = P(X=k) = \binom{N}{k}\; p^k\; (1-p)^{N-k} \: \text{ for } k=0,1,...,N.$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Remark__: The mean of $\small X$ is $\small pN$, and its variance is $\small p(1-p)N$. <br>
__Remark__: A Bernoulli random variable is just $\small Bin(1,p).$
:::

## Binomial Distribution
:::{style="font-size: .8em"}
__Example__: A biased coin that comes up heads 30% of the time is flipped 10 times. What is the probability of getting exactly 6 heads?<br>
Let $\small X$ represent the number of heads. Then $\small{X \sim Bin(10,0.3)}.$
$$\small{P(X=6) = \binom{10}{6}\; 0.3^6\; (1-0.3)^{10-6} = 0.0368.}$$
:::

```{python}
#| echo: true
from scipy.stats import binom

# Parameters
n = 10       # number of trials
p = 0.3      # probability of success

# Calculate the probability
probability = binom.pmf(6, n, p)
print(f"The probability of getting exactly 6 heads is {probability:.4f}")
```

:::{.notes}
 Import the binomial distribution from the scipy.stats library
:::


<!-- ## Binomial Distribution

:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    What is the most probable number of heads?
    </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.fragment .center-text}
```{python}
import numpy as np
import matplotlib.pyplot as plt

p = 0.3
x = np.arange(binom.ppf(0.01, 10, p), binom.ppf(0.9995, 10, p))
plt.figure(figsize=(10, 5))
plt.ylim([0, 0.3])
plt.xlim([-0.5, max(x)+0.5])
plt.plot(x, binom.pmf(x, 10, p), 'bo', ms=8, label = 'binom pmf')
plt.vlines(x, 0, binom.pmf(x, 10, p), colors='b', lw = 5, alpha=0.6)
plt.title(f'Binomial PMF, $p$ = {p}, $N$ = 10', size=20)
plt.yticks(fontsize=16)
plt.xticks(fontsize=16)
plt.xlabel(r'$k$', size=18)
plt.ylabel(r'$P[X = k]$', size=18);
```
::: -->

## Geometric Distribution
:::{style="font-size: .8em"}
The geometric distribution concerns Bernoulli trials as well. It has only one parameter $\small p$, the probability of success.

The geometric distribution answers the question: "_**What is the probability it takes $\small k$ trials to obtain the first success?**_"


```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Geometric Distribution </span>
        <p>
       A random variable \(\small X\) has a geometric distribution with parameter \(\small p\) \((\small 0\leq p \leq 1)\), \(\small X \sim Geo(p)\), if \(\small X\) has a discrete distribution with 
       $$ \small p(k) = P(X = k) = p(1-p)^{k-1} \: \:\text{for} \: k \geq 1.$$ 
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Remark__: The mean of $X$ is equal to $\small \frac{1}{p}$ and its variance is $\small \frac{1-p}{p^2}$. 
:::


## Geometric Distribution
:::{style="font-size: .8em"}
__Example__: A basketball player has a 30% chance of making a free throw. <br>
a. What is the probability that he makes his first successful shot on the 4th attempt?<br>
b. What is the expected number of attempts until he makes his first successful free throw?<br>

a. Let $\small X$ represent the trial number on which the first success occurs. Then 
$$\small P(X = 4) = 0.3(1-0.3)^{4-1} = 0.1029.$$

b. We also know that for a geometrically distributed random variable $\small E[X] = \frac{1}{p}.$ Hence,
$$\small E[X] = \frac{1}{0.3} \approx 3.33.$$

:::

:::{.notes}
$P(X = k) = p(1-p)^{k-1}$

$E[X]=\small \frac{1}{p}$
:::

## Geometric Distribution
:::{style="font-size: .8em"}

```{python}
#| echo: true
from scipy.stats import geom

# Parameters
p = 0.3  # probability of success

# Compute the probability
probability = geom.pmf(4, p)

# Calculate the expected value using the mean method
expected_value = geom.mean(p)
print(f"The probability of the 1st success on the 4th attempt is {probability:.4f}. The expected number of attempts until the 1st success is {expected_value:.2f}.")

```


:::{.center-text}
```{python}
import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import geom
p = 0.3
x = np.arange(geom.ppf(0.01, p), geom.ppf(0.995, p))
plt.figure(figsize=(10, 2))
plt.ylim([0, 0.35])
plt.xlim([0.5, 10.5])
plt.plot(x, geom.pmf(x, p), 'bo', ms=8, label = 'geom pmf')
plt.vlines(x, 0, geom.pmf(x, p), colors='b', lw = 5, alpha = 0.6)
plt.title(f'Geometric PMF, $p$ = {p}', size=20)
plt.yticks(fontsize=16)
plt.xticks(fontsize=16)
plt.xlabel(r'$k$', size=18)
plt.ylabel(r'$P[X = k]$', size=18);
```
:::

:::

<!-- ## Geometric Distribution
:::{.center-text}
```{python}
import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import geom
p = 0.3
x = np.arange(geom.ppf(0.01, p), geom.ppf(0.995, p))
plt.figure(figsize=(10, 6.5))
plt.ylim([0, 0.4])
plt.xlim([0.5, max(x)])
plt.plot(x, geom.pmf(x, p), 'bo', ms=8, label = 'geom pmf')
plt.vlines(x, 0, geom.pmf(x, p), colors='b', lw = 5, alpha = 0.6)
plt.title(f'Geometric PMF, $p$ = {p}', size=20)
plt.yticks(fontsize=16)
plt.xticks(fontsize=16)
plt.xlabel(r'$k$', size=18)
plt.ylabel(r'$P[X = k]$', size=18);
```
::: -->

## Poisson Distribution
:::{style="font-size: .8em"}
The Poisson distribution is not connected to Bernoulli trials. 
It arises from a _**Poisson process**_, which models _**the number of independent events occurring in fixed intervals at a constant average rate**_.

For example, the Poisson distribution can be used to model:

:::{.columns}
::: {.column width="60%"}
- people arriving to a bus stop;
- misprints on a page of a book;
- customers entering a post office on a given day;
- wrong telephone numbers dialed in a day.
:::

::: {.column width="40%"}
:::{.center-text}
<img src="images/common distributions/Bus_stop.png" width=400/>
:::
:::
:::
It answers the question: "_**How many successes occur in a fixed amount of time?**_"



:::

## Poisson Distribution
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Poisson Distribution </span>
        <p>
       A random variable \(\small X\) has a Poisson distribution with parameter \(\small \lambda > 0\), \(\small X \sim Pois(\lambda)\), if it has a discrete distribution with 
       \[
       \small P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}, \quad k = 0, 1, 2, \dots
       \]
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Remark__: The mean and variance of $X$ are both equal to $\lambda.$ <br>
__Example__: A small café receives an average of 5 online orders per hour. What is the probability that exactly 3 orders arrive in the next hour?<br>

Let $\small X \sim Pois(λ = 5).$ Then

$$
\small \small{P(X = 3) = \frac{e^{-5} 5^3}{3!} ≈ 0.1404.}
$$

:::

## Poisson Distribution
```{python}
#| echo: true
from scipy.stats import poisson

# Parameter
lambda_rate = 5

# Calculate the probability
probability = poisson.pmf(3, lambda_rate)
print(f"The probability of receiving exactly 3 orders in an hour is {probability:.4f}")
```
:::{.center-text}
```{python}
from scipy.stats import poisson
mu = 5
x = np.arange(poisson.ppf(0.01, mu), poisson.ppf(0.9995, mu))
# plt.ylim([0,1])
plt.figure(figsize=(10, 2.3))
plt.xlim([0.5, 12.5])
plt.ylim(ymax = 0.2)
plt.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')
plt.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.6)
plt.title(f'Poisson PMF,  $\lambda$ = {mu}', size=20)
plt.yticks(fontsize=16)
plt.xticks(fontsize=16)
plt.xlabel(r'$k$', size=18)
plt.ylabel(r'$P[X = k]$', size=18);
```
:::

## Uniform Distribution
:::{style="font-size: .8em"}
The uniform distribution models the case in which all outcomes are equally probable. This distribution _**can be discrete or continuous**_. We will introduce the continuous version of the uniform distribution in the next lecture.

We have already seen the discrete uniform distribution in the case of rolls of a fair die:
:::

:::{.center-text}
```{python}
x = np.arange(1, 7)
plt.figure(figsize=(10, 3))
plt.plot(x, 6*[1/6.], 'bo', ms=8)
plt.vlines(x, 0, 1/6., colors='b', lw=5, alpha=0.5)
plt.xlim([0.5, 6.5])
plt.ylim([0, 0.3])
plt.title(f'Discrete Uniform PMF on [1,6]', size=20)
plt.yticks(fontsize=16)
plt.xticks(fontsize=16)
plt.xlabel(r'x (Number of points showing)', size=18)
plt.ylabel(r'$P[X = x]$', size=18);
```
:::

## Uniform Distribution
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> (Discrete) Uniform Distribution</span>
        <p>
       A random variable is said to be uniformly distributed on interval \(\small [a,b]\), if it has the PMF equal to 

    $$\small p(x) = \frac{1}{n} \: \text{ with } n = b-a+1 \: \text{ and  } x = a, a+1,..., a+n-1.$$ 
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Remark__: $a$ and $b$ are the parameters of the uniform distribution.<br>
__Remark__: The expected value of a discrete uniform distribution is equal to $\small \frac{a+b}{2}$ and its variance is given by $\small \frac{n^2 - 1}{12}.$
:::

## Uniform Distribution
:::{style="font-size: .8em"}
__Example__: Compute the probability of rolling at most 4 points using a fair six-sided die using Python. 

```{python}
#| echo: true
from scipy.stats import randint

# Define a fair six-sided die
die = randint(1, 7)  # upper bound is exclusive

# Compute the probability of rolling at most 4 points
cdf_value = die.cdf(4)
print(f"The probability of rolling at most 4 points is {cdf_value}")

```
<br>

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    Which probability distribution can be used for modeling the number of horse-kick fatalities?
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}

imports the randint class from the scipy.stats module

creates a random variable die that represents a fair six-sided die.


:::

## Horse-Kick Fatalities 
:::{style="font-size: .8em"}

The Poisson distribution fits the data well when its parameter is set to 0.61. We will learn how to compute this value in Unit 3.
:::

:::{.center-text}

```{python}
import pandas as pd

horse_kicks = pd.DataFrame(
data = np.array([
[0, 108.67, 109],
[1, 66.29, 65],
[2, 20.22, 22],
[3, 4.11, 3],
[4, 0.63, 1],
[5, 0.08, 0],
[6, 0.01, 0]]),
columns = ["Number of Deaths Per Year","Predicted Instances (Poisson)","Observed Instances"])
horse_kicks[["Predicted Instances (Poisson)","Observed Instances"]].plot.bar()
plt.xlabel("Number of Deaths Per Year", size=18)
plt.ylabel("Count", size=18);
plt.legend(prop={'size': 18})
```
:::

:::{.notes}
cliff-hanger Maximum Likelihood Estimation
:::

<!-- ## Group Question 1
:::{style="font-size: .8em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
The probability of triplets in human births is approximately 0.001. We are considering 700 births in a large hospital. Which one of the below distributions best describes the total number of sets of triplets among these 700 births?<br>
    - Discrete uniform distribution <br>
    - Geometric distribution <br>
    - Binomial distribution <br>
    - Poisson distribution <br>
    - Bernoulli distribution
    <br>
    <br>
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
::: -->

## Group Question 1

:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    When three friends go for coffee, they decide who will pay the check by each flipping a coin and then letting the “odd person” pay. If all three flips produce the same result (so that there is no odd person), then they make a second round of flips, and they continue to do so until there is an odd person. <br>
    a. What distribution can be used to model the number of rounds? <br>
    <br>
    <br>
    b. What is the probability that exactly 3 rounds of flips are made?<br>
    <br>
    <br>
    c. What is the expectation of the number of rounds played? <br>
    <br>
    </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
Spring 25 HW 2

a. Probability of no odd person corresponds to the probability of getting HHH or TTT = 1/4. Let $X$ be the number of rounds. 
Then $X$ will follow geometric distribution with $p = 3/4$. The probability that exactly 3 rounds are made is equal to
$$P(X = 3) = (1/4)^2(3/4) = 3/64.$$\\
b. $$E[X] = \frac{1}{p} = 4/3.$$
:::

