---
title: Markov Chain Monte Carlo Part 1
author: "CDS DS-122<br>Boston University"
format:
    revealjs:
        math: true
        css:
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom, norm
from IPython.core.display import HTML
import networkx as nx

```

## Learning Objectives

:::{style="font-size: .8em"}

- Understand why we need **MCMC** for high-dimensional Bayesian inference
- Recall **Markov chain steady states** and apply them to sampling
- Learn the **Metropolis-Hastings algorithm** and its intuition
- See how MCMC constructs a Markov chain with desired steady-state distribution
- Apply MCMC to simple examples

:::

## Review: Accept-Reject Sampling (L34)

:::{style="font-size: .7em"}

**Last time:** We learned **accept-reject sampling** to sample from complex distributions

**Key idea:**

1. Find a proposal distribution $g(x)$ that's easy to sample from
2. Find constant $M$ such that $M \cdot g(x) \geq p(x)$ for all $x$
3. Sample $x$ from $g$ and $u$ from a uniform distribution $U([0,1])
4. **Accept** if $u \leq p(x)/ (Mg(x))$ (keep $x$), **reject** otherwise

**Two applications:**

- **Monte Carlo integration:** Estimate $\int_a^b f(x) \, dx$ by counting points under curve
- **Sampling from distributions:** Generate samples from target $p(x)$ using proposal $g(x)$


:::

## <span style="font-size: 0.8em">Group Question 2: Accept-Reject Sampling</span>

:::{style="font-size: .65em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>Problem:</b> You want to sample from \(p(x) = 3x^2\) for \(x \in [0,1]\) using accept-reject sampling with a linear proposal function \(g(x) = 2x\).<br><br>

<b>Tasks:</b><br>
1. What is the maximum value of the target PDF on [0,1]?<br>
2. What value of \(M\) do you need so that \(M \cdot g(x) \geq p(x)\) for all \(x\)?<br>
3. Describe the accept-reject algorithm steps for this specific problem<br>
4. What fraction of proposals would you expect to accept? <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::


## Solution 2: Accept-Reject Sampling

:::{style="font-size: .55em"}

**Problem:**  Sample from $p(x) = 3x^2$ for $x \in [0,1]$ using proposal $g(x) = 2x$

**1. Maximum value:** 
$p(x) = 3x^2$ is maximized at $x=1$, so $\max p(x) = 3$

**2. Finding M:** We need $M \cdot g(x) \geq p(x)$ for all $x$

   - Need: $M \cdot 2x \geq 3x^2$ for all $x \in [0,1]$, so  $M \geq (3/2)x$
   - Maximum at $x=1$: $M \geq 3/2$, so **M = 2**

**3. Algorithm steps:**

   1. Sample $x$ from $g(x) = 2x$
   2. Sample $u \sim \text{Uniform}(0,1)$
   3. Accept if $u \leq \frac{p(x)}{M \cdot g(x)} = \frac{2x^2}{2 \cdot x} = x$

**4. Acceptance rate:**

   - Area under target: $\int_0^1 3x^2 \, dx = (3/3-0) = 1$
   - Area under $M \cdot g(x) = 2 \cdot 2x = 4x$: $\int_0^1 4x \, dx = 2$
   - Acceptance rate: $\frac{1}{2} = \frac{1}{2}$

:::

## The Basketball Team Problem

:::{style="font-size: .65em"}

**Last class I started this analogy:** You want to generate data for a fake basketball team

- You have a "random person simulator" button
- But you only want people tall enough to play professional basketball

```{python}
#| echo: false
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, skewnorm

# Create x-axis for heights (in inches)
heights = np.linspace(55, 85, 300)

# General population: Normal distribution (mean=69", sd=3")
population_dist = norm.pdf(heights, loc=69, scale=3)

# Basketball players: Right-skewed distribution (taller, mean=78", less variance)
# Using skewnorm to create right-skewed distribution
basketball_dist = skewnorm.pdf(heights, a=5, loc=75, scale=3)

# Normalize basketball distribution to match population peak for visibility
basketball_dist = basketball_dist * (population_dist.max() / basketball_dist.max())

# Plot with minimal whitespace
fig, ax = plt.subplots(figsize=(8, 3.5))

ax.fill_between(heights, population_dist, alpha=0.4, color='blue',
                label='General Population (proposal)')
ax.plot(heights, population_dist, color='blue', linewidth=2)

ax.fill_between(heights, basketball_dist, alpha=0.4, color='orange',
                label='Basketball Players (target)')
ax.plot(heights, basketball_dist, color='orange', linewidth=2)

ax.set_xlabel('Height (inches)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('Height Distributions: General Population vs. Basketball Players', fontsize=13, fontweight='bold', pad=5)
ax.legend(fontsize=10, loc='upper left')
ax.grid(alpha=0.3)

# Aggressive margin reduction - minimize bottom space
plt.subplots_adjust(left=0.08, right=0.98, top=0.88, bottom=0)
plt.show()
```

:::{style="margin-top: -20px;"}
**The mismatch:** Most people are too short for basketball. Need a better strategy!
:::

:::

## The Accept-Reject Problem

:::{style="font-size: .6em"}

**Using accept-reject sampling:** M=50 is not even enough!

- Acceptance rate ≈ $\frac{\text{orange area}}{\text{blue area}}$ ≈ **1-2%**
- Need to press the "random person" button ~50-100 times to get one basketball player!
- **Waste 98-99% of samples!**

**Better idea:** What if, once we found someone tall enough, we could generate *similar* people more often?

```{python}
#| echo: false
#| fig-align: center

# Create the same distributions
heights = np.linspace(55, 85, 300)
population_dist = norm.pdf(heights, loc=69, scale=3)
basketball_dist = skewnorm.pdf(heights, a=5, loc=75, scale=3)

# Scale proposal by M to envelope target
M = 50
scaled_proposal = M * population_dist

# Plot with minimal whitespace
fig, ax = plt.subplots(figsize=(8, 3.5))

# Scaled proposal (the envelope)
ax.fill_between(heights, scaled_proposal, alpha=0.3, color='lightblue',
                label=f'M × Proposal (M={M})')
ax.plot(heights, scaled_proposal, color='blue', linewidth=2, linestyle='--')

# Target distribution
ax.fill_between(heights, basketball_dist, alpha=0.5, color='orange',
                label='Basketball Players (target)')
ax.plot(heights, basketball_dist, color='orange', linewidth=2)

ax.set_xlabel('Height (inches)', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('Accept-Reject: Massive Waste When Proposal Mismatches Target', fontsize=13, fontweight='bold', pad=5)
ax.legend(fontsize=10, loc='upper left')
ax.grid(alpha=0.3)
ax.set_ylim(0, scaled_proposal.max() * 1.1)

# Aggressive margin reduction - minimize bottom space
plt.subplots_adjust(left=0.08, right=0.98, top=0.88, bottom=0)
plt.show()
```



:::


## <span style="font-size: 0.8em">Motivation: Engineering a Therapeutic Virus</span>


:::: {.columns}

::: {.column width="60%"}

:::{style="font-size: .6em"}

**The problem:** Gene therapy - engineer a virus for medical applications

- Start with a viral genome (seq. of nucleotides: A, C, G, T)
- Have a model: sequence → predicted effectiveness score
- Goal: Generate 100,000 promising variants to test in the lab

**The challenge:**

- Sequences of ~600 amino acids = 20^600 = 1e78 possibilities
- Less than ~0.000...001% of random sequences are viable

:::

:::

::: {.column width="40%"}

<img src="images/mcmc/dyno.png" width="100%"/>

:::

::::


## Two Approaches to Exploration

:::{style="font-size: .75em"}

```{python}
#| echo: false
#| fig-align: center

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Left: Random dart throwing
np.random.seed(42)
n_random = 200
x_random = np.random.uniform(-3, 3, n_random)
y_random = np.random.uniform(-3, 3, n_random)

# Create a "fitness landscape" - only good in certain regions
xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))
fitness = (np.exp(-((xx-0.5)**2 + (yy-0.5)**2)/0.5) +
           0.5*np.exp(-((xx+1)**2 + (yy+1)**2)/0.8) +
           0.3*np.exp(-((xx+0.5)**2 + (yy-1.5)**2)/0.4))

ax1.contourf(xx, yy, fitness, levels=20, cmap='YlOrRd', alpha=0.6)
ax1.scatter(x_random, y_random, c='blue', s=20, alpha=0.6, edgecolors='black', linewidth=0.5)
ax1.set_title('Random Sampling:\nMost samples waste effort in low-fitness regions', size=13, color='darkred')
ax1.set_xlabel('Mutation dimension 1', size=11)
ax1.set_ylabel('Mutation dimension 2', size=11)
ax1.text(-2.5, 2.5, 'Hit rate: ~5%', fontsize=12, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

# Right: Evolutionary/MCMC approach
# Simulate a chain that explores high-fitness regions
chains = []
starts = [(-1, -1), (0.5, 0.5), (-0.5, 1.5)]
colors_chain = ['blue', 'green', 'purple']

for start, color in zip(starts, colors_chain):
    x, y = start
    chain_x, chain_y = [x], [y]

    for _ in range(100):
        # Propose nearby mutation
        x_prop = x + np.random.normal(0, 0.15)
        y_prop = y + np.random.normal(0, 0.15)

        # Accept based on fitness
        fitness_current = (np.exp(-((x-0.5)**2 + (y-0.5)**2)/0.5) +
                          0.5*np.exp(-((x+1)**2 + (y+1)**2)/0.8) +
                          0.3*np.exp(-((x+0.5)**2 + (y-1.5)**2)/0.4))
        fitness_prop = (np.exp(-((x_prop-0.5)**2 + (y_prop-0.5)**2)/0.5) +
                       0.5*np.exp(-((x_prop+1)**2 + (y_prop+1)**2)/0.8) +
                       0.3*np.exp(-((x_prop+0.5)**2 + (y_prop-1.5)**2)/0.4))

        if np.random.rand() < min(1, fitness_prop/fitness_current):
            x, y = x_prop, y_prop

        chain_x.append(x)
        chain_y.append(y)

    chains.append((chain_x, chain_y, color))

ax2.contourf(xx, yy, fitness, levels=20, cmap='YlOrRd', alpha=0.6)
for chain_x, chain_y, color in chains:
    ax2.plot(chain_x, chain_y, '-', alpha=0.4, linewidth=1.5, color=color)
    ax2.scatter(chain_x[::5], chain_y[::5], c=color, s=25, alpha=0.7, edgecolors='black', linewidth=0.5)

ax2.set_title('Evolutionary Approach (MCMC):\nExplore by branching from good sequences', size=13, color='darkgreen')
ax2.set_xlabel('Mutation dimension 1', size=11)
ax2.set_ylabel('Mutation dimension 2', size=11)
ax2.text(-2.5, 2.5, 'Hit rate: ~80%', fontsize=12, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

:::

## The Key Insight

:::{style="font-size: .75em"}

**Random sampling:** Throw darts blindly

- Most samples in low-fitness regions
- Wasteful in high dimensions

**Evolutionary approach:** Explore from what works

- Start with known sequence
- Make small mutations
- Keep good variants, sometimes keep "okay" variants
- Follow chains of successive improvements

**This is the heart of MCMC!**

:::

## A Tale of Good King Markov

:::{style="font-size: .6em"}

**A parable to build intuition** (adapted from Kruschke's *Doing Bayesian Data Analysis*):

Once upon a time, there was a kingdom with 10 islands arranged in a circle. On each island lived some number of people:

```{python}
#| echo: false
#| fig-align: center

# Island populations
populations = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
n_islands = len(populations)

fig, ax = plt.subplots(figsize=(6, 6))

# Draw islands in a circle
angles = np.linspace(0, 2*np.pi, n_islands, endpoint=False)
x = np.cos(angles)
y = np.sin(angles)

for i, (xi, yi, pop) in enumerate(zip(x, y, populations)):
    # Draw island
    circle = plt.Circle((xi, yi), 0.15, color='lightgreen', ec='black', linewidth=2)
    ax.add_patch(circle)
    # Label with population
    ax.text(xi, yi, str(pop), ha='center', va='center', size=14, weight='bold')
    # Island number outside
    ax.text(xi*1.4, yi*1.4, f'Island {i+1}', ha='center', va='center', size=10)

ax.set_xlim(-1.8, 1.8)
ax.set_ylim(-1.8, 1.8)
ax.set_aspect('equal')
ax.axis('off')
ax.set_title("Kingdom of 10 Islands\n(numbers show population)", size=16)
plt.show()
```

:::

## The King's Dilemma

:::{style="font-size: .75em"}

Good King Markov wanted to visit his people fairly - spending time on each island proportional to its population.

**The constraint:** 

The king could only travel between adjacent islands (clockwise or counterclockwise)

**The question:** 

How can the king visit islands proportional to population if he can only move to neighboring islands?


:::

## The King's Plan

:::{style="font-size: .75em"}

Each week:

1. Flip a coin: 

- heads = propose clockwise
- tails = propose counterclockwise

2. Compare populations: current island vs. proposed island

3. Decide whether to move based on a special rule...

:::

## The King's Decision Rule

:::{style="font-size: .75em"}

**When the king proposes to move from island $i$ to island $j$:**

$$P(\text{move}) = \min\left(1, \frac{\text{population}_j}{\text{population}_i}\right)$$

**In words:**

- If proposed island has **more** people: definitely move (probability = 1)
- If proposed island has **fewer** people: move with probability = ratio of populations

**Example:** Currently on Island 8 (pop. 80), propose Island 9 (pop. 90)

- $P(\text{move}) = \min(1, 9/8) = 1$ -> Always move!

**Example:** Currently on Island 9 (pop. 90), propose Island 8 (pop. 80)

- $P(\text{move}) = \min(1, 8/9) = 0.889$ -> Move 88.9% of the time

:::

## Simulating the King's Journey

:::{style="font-size: .7em"}

Let's simulate the king's travels for 10,000 weeks:

```{python}
#| echo: true
np.random.seed(42)

populations = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])
n_islands = len(populations)
n_weeks = 10000

# Start on a random island
current = np.random.randint(n_islands)
history = [current]

for week in range(n_weeks):
    # Propose: move clockwise or counterclockwise
    if np.random.rand() < 0.5:
        proposed = (current + 1) % n_islands  # clockwise
    else:
        proposed = (current - 1) % n_islands  # counterclockwise

    # Decide whether to move
    prob_move = min(1, populations[proposed] / populations[current])

    if np.random.rand() < prob_move:
        current = proposed  # Accept: move to new island
    # else: stay on current island (reject)

    history.append(current)
```

:::

## Step 1: Week 1

:::{style="font-size: .7em"}


```{python}
#| echo: false
#| fig-align: center

# Run simulation to get steps
np.random.seed(42)
populations = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])
n_islands = len(populations)

# Initial position
current = np.random.randint(n_islands)

# Week 1 proposal
coin1 = np.random.rand()
if coin1 < 0.5:
    proposed = (current + 1) % n_islands
    direction = "clockwise"
else:
    proposed = (current - 1) % n_islands
    direction = "counter-clockwise"

prob_move = populations[proposed] / populations[current]
accept_coin = np.random.rand()

# Draw the islands
fig, ax = plt.subplots(figsize=(4, 3))
theta = np.linspace(0, 2*np.pi, n_islands, endpoint=False)
x = np.cos(theta)
y = np.sin(theta)

# Draw all islands
for i in range(n_islands):
    color = 'gold' if i == current else ('lightgreen' if i == proposed else 'lightblue')
    circle = plt.Circle((x[i], y[i]), 0.12, color=color, ec='black', linewidth=1.5, zorder=2)
    ax.add_patch(circle)
    ax.text(x[i], y[i], f'{populations[i]}', ha='center', va='center',
            fontsize=7, fontweight='bold', zorder=3)

# Draw arrow from current to proposed
dx = x[proposed] - x[current]
dy = y[proposed] - y[current]
ax.arrow(x[current], y[current], dx*0.6, dy*0.6, head_width=0.06, head_length=0.06,
         fc='red', ec='red', linewidth=2, zorder=1, length_includes_head=True)

ax.set_xlim(-1.2, 1.2)
ax.set_ylim(-1.2, 1.2)
ax.set_aspect('equal')
ax.axis('off')

# Add legend
ax.text(0, -1.35, f'Current island: {current+1} (Pop: {populations[current]})',
        fontsize=9, ha='center', bbox=dict(boxstyle='round', facecolor='gold', alpha=0.8))
ax.text(0, -1.52, f'Proposal: Move {direction} to island {proposed+1} (Pop: {populations[proposed]})',
        fontsize=9, ha='center', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

plt.subplots_adjust(left=0, right=1, top=1, bottom=0)
plt.show()
```

**Calculate:** $\alpha = \min\left(1, \frac{{\text{{Pop}}_{{proposed}}}}{{\text{{Pop}}_{{current}}}}\right) = \min\left(1,60/70\right) = 0.85$

**Random number:** 0.11

**Decision:** Move!

:::

## Step 2: Week 2

:::{style="font-size: .7em"}



```{python}
#| echo: false
#| fig-align: center

# Update position from week 1
if accept_coin < min(1, prob_move):
    current = proposed

# Week 2 proposal
coin2 = np.random.rand()
if coin2 < 0.5:
    proposed = (current + 1) % n_islands
    direction = "clockwise"
else:
    proposed = (current - 1) % n_islands
    direction = "counter-clockwise"

prob_move = populations[proposed] / populations[current]
accept_coin = np.random.rand()

# Draw the islands
fig, ax = plt.subplots(figsize=(4, 3))

# Draw all islands
for i in range(n_islands):
    color = 'gold' if i == current else ('lightgreen' if i == proposed else 'lightblue')
    circle = plt.Circle((x[i], y[i]), 0.12, color=color, ec='black', linewidth=1.5, zorder=2)
    ax.add_patch(circle)
    ax.text(x[i], y[i], f'{populations[i]}', ha='center', va='center',
            fontsize=7, fontweight='bold', zorder=3)

# Draw arrow
dx = x[proposed] - x[current]
dy = y[proposed] - y[current]
ax.arrow(x[current], y[current], dx*0.6, dy*0.6, head_width=0.06, head_length=0.06,
         fc='red', ec='red', linewidth=2, zorder=1, length_includes_head=True)

ax.set_xlim(-1.2, 1.2)
ax.set_ylim(-1.2, 1.2)
ax.set_aspect('equal')
ax.axis('off')

ax.text(0, -1.35, f'Current island: {current+1} (Pop: {populations[current]})',
        fontsize=9, ha='center', bbox=dict(boxstyle='round', facecolor='gold', alpha=0.8))
ax.text(0, -1.52, f'Proposal: Move {direction} to island {proposed+1} (Pop: {populations[proposed]})',
        fontsize=9, ha='center', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

plt.subplots_adjust(left=0, right=1, top=1, bottom=0)
plt.show()
```

**Calculate:** $\alpha = \min\left(1, \frac{{\text{{Pop}}_{{proposed}}}}{{\text{{Pop}}_{{current}}}}\right) = \min\left(1,50/60\right) = 0.83$

**Random number:** 0.92

**Decision:** Stay!

:::

## Step 3: Week 3

:::{style="font-size: .7em"}


```{python}
#| echo: false
#| fig-align: center

# Don't update - faking it
# if accept_coin < min(1, prob_move):
#     current = proposed

# Week 3 proposal
coin3 = np.random.rand()
if coin3 < 0.5:
    proposed = (current + 1) % n_islands
    direction = "clockwise"
else:
    proposed = (current - 1) % n_islands
    direction = "counter-clockwise"

prob_move = populations[proposed] / populations[current]
accept_coin = np.random.rand()

# Draw the islands
fig, ax = plt.subplots(figsize=(4, 3))

# Draw all islands
for i in range(n_islands):
    color = 'gold' if i == current else ('lightgreen' if i == proposed else 'lightblue')
    circle = plt.Circle((x[i], y[i]), 0.12, color=color, ec='black', linewidth=1.5, zorder=2)
    ax.add_patch(circle)
    ax.text(x[i], y[i], f'{populations[i]}', ha='center', va='center',
            fontsize=7, fontweight='bold', zorder=3)

# Draw arrow
dx = x[proposed] - x[current]
dy = y[proposed] - y[current]
ax.arrow(x[current], y[current], dx*0.6, dy*0.6, head_width=0.06, head_length=0.06,
         fc='red', ec='red', linewidth=2, zorder=1, length_includes_head=True)

ax.set_xlim(-1.2, 1.2)
ax.set_ylim(-1.2, 1.2)
ax.set_aspect('equal')
ax.axis('off')

ax.text(0, -1.35, f'Current island: {populations[current]}',
        fontsize=9, ha='center', bbox=dict(boxstyle='round', facecolor='gold', alpha=0.8))
ax.text(0, -1.52, f'Proposal: Move {direction} to island {populations[proposed]}',
        fontsize=9, ha='center', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

plt.subplots_adjust(left=0, right=1, top=1, bottom=0)
plt.show()
```

**Calculate:** $\alpha = \min\left(1, \frac{{\text{{Pop}}_{{proposed}}}}{{\text{{Pop}}_{{current}}}}\right) = \min\left(1,70/60\right) = 1.0$

**Random number:** 0.59

**Decision:** Move!

:::


## The King's Long-Run Distribution

:::{style="font-size: .7em"}

**Amazing!** The king visits each island proportional to its population, even though he could only move to adjacent islands!

```{python}
#| echo: false
#| fig-align: center

# Throw out first 100 samples (burn-in)
burn_in = 100
samples = np.array(history[burn_in:])

# Count visits to each island
visits = np.bincount(samples, minlength=n_islands)
visit_proportions = visits / len(samples)

# Expected proportions (proportional to population)
total_pop = populations.sum()
expected_proportions = populations / total_pop

fig, ax = plt.subplots(figsize=(10, 5))

x = np.arange(n_islands)
width = 0.35

ax.bar(x - width/2, visit_proportions, width, label='King\'s actual visits', alpha=0.8)
ax.bar(x + width/2, expected_proportions, width, label='Expected (∝ population)', alpha=0.8)

ax.set_xlabel('Island', size=14)
ax.set_ylabel('Proportion of time', size=14)
ax.set_title('King Markov\'s Visit Distribution (after 10,000 weeks)', size=14)
ax.set_xticks(x)
ax.set_xticklabels([f'{i+1}' for i in range(n_islands)])
ax.legend(fontsize=12)
ax.grid(axis='y', alpha=0.3)

plt.show()
```


:::

## The Markov Chain View

:::{style="font-size: .75em"}

**What just happened?**

The king's algorithm created a **Markov chain**:

- **States:** Islands (parameter values in general)
- **Proposals:** Move to adjacent island (explore nearby parameters)
- **Acceptance rule:** Based on population ratio (based on posterior ratio)
- **Steady state:** Proportional to population (equal to posterior!)

**This is MCMC!** Specifically, the **Metropolis-Hastings algorithm**

:::

## <span style="font-size: 0.8em">From King Markov to Bayesian Inference</span>

:::{style="font-size: .7em"}

**Replace the metaphor with statistics:**

| King's Kingdom | Bayesian Inference |
|----------------|-------------------|
| Islands | Parameter values $\theta$ |
| Population | Posterior probability $p(\theta \mid \text{data})$ |
| King's position | Current sample |
| Move between islands | Propose new parameter value |
| Acceptance rule | Metropolis-Hastings criterion |
| Long-run visits | Samples from posterior |

**Key feature:** We only need the *ratio* of posteriors, so normalizing constants cancel out!

$$\frac{p(\theta_{\text{new}} \mid \text{data})}{p(\theta_{\text{current}} \mid \text{data})} = \frac{p(\text{data} \mid \theta_{\text{new}}) p(\theta_{\text{new}})}{p(\text{data} \mid \theta_{\text{current}}) p(\theta_{\text{current}})}$$

:::

## The Metropolis Algorithm

:::{style="font-size: .75em"}

```{python}
#| echo: false
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label">The Metropolis Algorithm</span>
        <p>
        To sample from posterior \(p(\theta \mid \text{data})\):<br><br>

        1. Start with initial value \(\theta_0\)<br>
        2. At step \(t\), propose new value \(\theta^*\) "near" \(\theta_t\)<br>
        3. Compute acceptance probability:
        $$\alpha = \min\left(1, \frac{p(\theta^* \mid \text{data})}{p(\theta_t \mid \text{data})}\right)$$
        4. With probability \(\alpha\), set \(\theta_{t+1} = \theta^*\) (accept)<br>
        5. Otherwise, set \(\theta_{t+1} = \theta_t\) (reject, stay put)<br>
        6. Repeat steps 2-5<br><br>

        After enough iterations, samples \(\theta_t\) come from the posterior!
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Why Does This Work?

:::{style="font-size: .75em"}

**The mathematical reason:** Detailed balance

If we set transition probabilities via the Metropolis-Hastings rule, then:

$$\pi_i P_{ji} = \pi_j P_{ij}$$

where $\pi$ is the target posterior distribution.

**Detailed balance implies steady state!**

**The Intuition:**

- We always accept moves to higher-probability regions
- We sometimes accept moves to lower-probability regions
- The balance is exactly right so we spend time proportional to probability

:::

## Example: Estimating a Coin's Bias

:::{style="font-size: .7em"}

**Problem:** Flip a coin 20 times, get 14 heads. What's $p$ (probability of heads)?

**Traditional approach:** Grid method with 100 points (or conjugate prior!)

**MCMC approach:**

- States: possible values of $p \in [0, 1]$
- Target: posterior $P(p \mid 14H, 6T) \propto p^{14}(1-p)^6$
- Proposal: add random noise to current value of $p$
<br>

```{python}
#| echo: true
def posterior_unnormalized(p, heads, tails):
    """Posterior up to a constant (uniform prior)"""
    if p < 0 or p > 1:
        return 0
    return p**heads * (1-p)**tails

n_heads, n_tails = 14, 6
```

:::

## Metropolis-Hastings for Coin Problem

:::{style="font-size: .65em"}

```{python}
#| echo: true
np.random.seed(42)

n_samples = 10000
current = 0.5 # Start at p = 0.5
samples = [current]

for i in range(n_samples):
    # Propose: add random noise (symmetric proposal)
    proposed = current + np.random.normal(0, 0.1)

    # Compute acceptance probability
    if proposed < 0 or proposed > 1:
        alpha = 0  # Reject proposals outside [0,1]
    else:
        posterior_current = posterior_unnormalized(current, n_heads, n_tails)
        posterior_proposed = posterior_unnormalized(proposed, n_heads, n_tails)
        alpha = min(1, posterior_proposed / posterior_current)

    # Accept or reject
    if np.random.rand() < alpha:
        current = proposed  # Accept
    # else: current stays the same (reject)

    samples.append(current)

samples = np.array(samples)
```

:::

## MCMC Results vs Grid Method

:::{style="font-size: .7em"}

```{python}
#| echo: false
#| fig-align: center

# Throw out burn-in
burn_in = 1000
mcmc_samples = samples[burn_in:]

# Compare with analytical posterior (Beta distribution)
from scipy.stats import beta
p_range = np.linspace(0, 1, 100)
# With uniform prior, posterior is Beta(heads+1, tails+1)
analytical_posterior = beta.pdf(p_range, n_heads+1, n_tails+1)

fig, ax = plt.subplots(figsize=(10, 5))

# MCMC histogram
ax.hist(mcmc_samples, bins=50, density=True, alpha=0.6,
        color='skyblue', label=f'MCMC samples ({len(mcmc_samples)} samples)')

# Analytical posterior
ax.plot(p_range, analytical_posterior, 'r-', linewidth=3,
        label='True posterior (analytical)')

ax.set_xlabel('p (probability of heads)', size=14)
ax.set_ylabel('Density', size=14)
ax.set_title('MCMC vs True Posterior Distribution', size=14)
ax.legend(fontsize=12)
ax.grid(alpha=0.3)

plt.show()

print(f"MCMC mean: {mcmc_samples.mean():.3f}")
print(f"True mean: {(n_heads+1)/(n_heads+n_tails+2):.3f}")
```

MCMC successfully samples from the posterior!

:::

## Visualizing the MCMC Journey

:::{style="font-size: .7em"}

Let's watch the chain explore the parameter space:

```{python}
#| echo: false
#| fig-align: center

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 6))

# Trace plot
ax1.plot(samples[:500], alpha=0.7, linewidth=1)
ax1.axhline(y=0.7, color='r', linestyle='--', linewidth=2, label='True mean')
ax1.set_xlabel('Iteration', size=12)
ax1.set_ylabel('p value', size=12)
ax1.set_title('Trace Plot: MCMC Chain over Time (first 500 iterations)', size=14)
ax1.legend()
ax1.grid(alpha=0.3)

# Running mean
running_mean = np.cumsum(samples) / np.arange(1, len(samples)+1)
ax2.plot(running_mean, linewidth=2)
ax2.axhline(y=0.7, color='r', linestyle='--', linewidth=2, label='True mean')
ax2.set_xlabel('Iteration', size=12)
ax2.set_ylabel('Running mean', size=12)
ax2.set_title('Convergence: Running Mean of Samples', size=14)
ax2.legend()
ax2.grid(alpha=0.3)

plt.tight_layout()
plt.show()
```

:::

## The Metropolis-Hastings Extension

:::{style="font-size: .75em"}

**Metropolis algorithm** (1953): Requires *symmetric* proposals

- The chance of proposing q' from q is the same as proposing q from q' 
- $q(\theta' \mid \theta) = q(\theta \mid \theta')$
- Example: $\theta' = \theta + \text{Normal}(0, \sigma)$

**Metropolis-Hastings algorithm** (1970): Allows *asymmetric* proposals

$$\alpha = \min\left(1, \frac{p(\theta' \mid \text{data}) \cdot q(\theta \mid \theta')}{p(\theta \mid \text{data}) \cdot q(\theta' \mid \theta)}\right)$$

**In reality** Most people say Metropolis-Hastings regardless of proposal symmetry.

:::

## Comparing Posterior Calculation Methods

:::{style="font-size: .7em"}

| Method | Pros | Cons |
|------|---------|---------|
| **Grid** | • Exact<br>• Easy to understand | • Depends on granularity<br>• Infeasible for 4+ parameters |
| **Conjugate Priors** | • Analytical solution<br>• No approximation | • Only works for special cases |
| **Accept-Reject** | • Direct sampling<br>• No burn-in | • Low acceptance in high-D<br>• Need good proposal |
| **MCMC** | • Works in high-D<br>• Only need posterior ratio<br>• Very general | • Burn-in required<br>• Convergence issues |

**MCMC is often the only practical option for complex models!**

:::


## Group Question 1: MCMC Intuition

:::{style="font-size: .65em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>Scenario:</b> You're using MCMC to estimate a parameter \(\theta\) from data. The posterior has a clear peak at \(\theta = 5\).<br><br>

<b>Current state:</b> \(\theta_{\text{current}} = 4.0\)<br>
<b>Proposed state:</b> \(\theta_{\text{proposed}} = 4.5\)<br><br>

You calculate unnormalized posterior values:<br>
• \(p(\theta=4.0 \mid \text{data}) = 0.10\)<br>
• \(p(\theta=4.5 \mid \text{data}) = 0.15\)<br><br>

<b>Questions:</b><br>
1. What is the acceptance probability \(\alpha\)?<br>
2. If instead you proposed \(\theta = 3.5\) with posterior probability 0.08, what would your probability of acceptance be?<br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Group Question 1: Solution

:::{style="font-size: .7em"}

**1. Acceptance probability:**

$$\alpha = \min\left(1, \frac{p(\theta_{\text{proposed}} \mid \text{data})}{p(\theta_{\text{current}} \mid \text{data})}\right) = \min\left(1, \frac{0.15}{0.10}\right) = 1$$

We definitely accept ($\alpha$ = 1).

**Intuition:** We're moving toward a region of higher posterior probability (closer to the peak at $\theta$=5), so we should always accept moves in the right direction.

**2. For $\theta$ = 3.5:**

$$\alpha = \min\left(1, \frac{0.08}{0.10}\right) = 0.8$$

We'd accept with 80% probability. Even though it's worse, we sometimes accept moves to lower probability regions to ensure we explore the full distribution!

:::

## Group Question 2: The Robot's Search

:::{style="font-size: .6em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
<b>The Robot's Search:</b> A robot is searching a 3×3 grid for the warmest spot. Each cell has a temp:<br>

<table style="margin: 10px auto; border-collapse: collapse;">
<tr><td style="border: 1px solid black; padding: 8px; text-align: center;">20°</td><td style="border: 1px solid black; padding: 8px; text-align: center;">40°</td><td style="border: 1px solid black; padding: 8px; text-align: center;">30°</td></tr>
<tr><td style="border: 1px solid black; padding: 8px; text-align: center;">50°</td><td style="border: 1px solid black; padding: 8px; text-align: center;">80°</td><td style="border: 1px solid black; padding: 8px; text-align: center;">60°</td></tr>
<tr><td style="border: 1px solid black; padding: 8px; text-align: center;">40°</td><td style="border: 1px solid black; padding: 8px; text-align: center;">70°</td><td style="border: 1px solid black; padding: 8px; text-align: center;">50°</td></tr>
</table>


The robot can only move to adjacent cells (up, down, left, right - no diagonals). It uses Metropolis-Hastings to visit cells in proportion to their temperature.<br><br>

<b>Questions:</b><br>
1. If the robot is at the center and proposes the cell above it, what's the probability of moving?<br>
2. If the robot is at top-left and proposes the cell to its right, what's the probability of moving?<br>
3. In the long run, what fraction of time should the robot spend at the warmest cell?<br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::

## Group Question 2: Solution

:::{style="font-size: .6em"}

**1. Moving from center (80°) to above (40°):**

$$\alpha = \min\left(1, \frac{40}{80}\right) = 0.5$$

The robot accepts with 50% probability - moving to a colder spot is sometimes accepted

**2. Moving from top-left (20°) to right (40°):**

$$\alpha = \min\left(1, \frac{40}{20}\right) = \min(1, 2) = 1$$

Moving to warmer areas is always accepted.

**3. Fraction of time at warmest cell:**

Total temperature: 20+40+30+50+80+60+40+70+50 = 440°

Fraction at center: $\frac{80}{440} = \frac{2}{11} \approx 18.2\%$

:::
