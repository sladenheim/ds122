---
title: Sums of Random Variables 
author: "CDS DS-122<br>Boston University"
format: 
    revealjs:
        math: true
        css: 
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true 
---

## Brain Cancer
:::{style="font-size: .8em"}

_**Which states have the biggest brain cancer problem?**_

:::{.center-text}
<img src="images/sums of random variables/USA_map.png" width=350/>
:::


- Most deaths: California, Texas, New York, Florida. 

- Highest death rates per 100,000 people: South Dakota, Nebraska, Alaska, Delaware, Maine.<br>
(Lowest death rates: Wyoming, Vermont, North Dakota, Hawaii, D.C.)
:::

:::{.notes}
_What can we conclude?_

- Rates per capita are more meaningful: States like California and Texas have the most brain cancer deaths simply because they have the largest populations.
- Small states dominate both extremes in terms of rates.


One of today's topics, the _Law of Large Numbers_, explains why small populations show more fluctuation.


:::

## Learning Objectives

:::{style="font-size: .8em"}
:::{.columns}
::: {.column width="70%"}
* Law of the Unconscious Statistician (LOTUS)
* Linearity of Expectation (LOE)
* Additional properties of variance and covariance:
    - New expression for variance and covariance
    - Variance of a linear function 
    - Variance of a sum
* The Weak Law of Large Numbers
* An Introduction to the Central Limit Theorem 
:::
::: {.column width="30%"}
:::{.center-text}
<img src="images/sums of random variables/white_lotus.png" width=500/>
:::
:::
:::
* Application of above concepts to song classification
:::

:::{.notes}

All the rules and concepts can be extended to continuous random variables.
:::

## Classifying Songs
:::{style="font-size: .8em"}
:::{.columns}
::: {.column width="70%"}
A data science student Anush is building a machine learning model to classify music genres from audio files. He has a playlist of 500 songs, each with a variable processing time. The processing times are independent and follow a common distribution with:

- Mean: 3 minutes
- Standard deviation: 0.5 minutes
:::

::: {.column width="30%"}
:::{.center-text}
<img src="images/sums of random variables/Anush.jpeg" width=300/>
:::
:::
:::

__Question__: What distribution can be used to approximate the amount of time that Anush needs to process the first 45 songs?
<!-- Anush has 100 minutes available before class starts. 

__Question__: Approximate the probability that Anush will be able to process at least 45 songs before class. -->

:::


## LOTUS
:::{style="font-size: .8em"}
To study a function $\small g(X)$ of a random variable $\small  X$, we use the __Law of the Unconscious Statistician (LOTUS)__. 
<!-- Examples of $g(x)$ include $X^2, e^X$, or $\log X.$   -->



```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Law of the Unconscious Statistician (LOTUS) </span>
        <p>
    For a discrete random variable \(\small  X \) and a function \(\small  g(x) \) defined for all values that \(\small  X \) can take,
     \[
    \small{E[g(X)] = \sum_{x=-\infty}^{\infty} g(x) p(x).}
     \]
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

__Remark__: The same principle extends to continuous random variables using integration.

__Examples__: $\small{E[X^2] = \sum_{x=-\infty}^{\infty} x^2 p(x) \quad \text{ and } \quad E[e^X] = \sum_{x=-\infty}^{\infty} e^x p(x).}$
:::

:::{.notes}

NOTATION:

Write g(X)

Sum over x
:::

## LOTUS 
:::{style="font-size: .8em"}
Let us use LOTUS to find $E[X+b]$ where $b$ is a number.

$$\small{\begin{align*}
E[X+b] &= \sum_x (x + b) p(x) & \text{(LOTUS)} \\
&= \sum_x x p(x) + \sum_x b p(x) & \text{(break $(x + b) p(x)$ into $xp(x) + bp(x)$)} \\
&= \sum_x x p(x) + b\sum_x p(x) & \text{(factor constant outside the sum)} \\
&= E[X] + b & \text{(definitions of expected value, p.m.f.)}
\end{align*}}$$

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    What is \(E[aX]\) where \(a\) is a number?
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

## Linearity of Expectation

:::{style="font-size: .75em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Linearity of Expectation (LOE) </span>
        <p>
    Let \(\small{X}\) and \(\small{Y}\) be discrete random variables. Then 

    $$\begin{equation}
    \small{E[X+Y] = E[X] + E[Y].}
    \end{equation}
    $$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

__Proof__: Suppose that the joint distribution of $\small{X}$ and $\small{Y}$ is $\small{p(x, y)}$. Then 
$$\small{
\begin{align*}
E[X + Y] &= \sum_x \sum_y (x + y) p(x, y) \: \quad \qquad \qquad \qquad \text{(LOTUS)} \\
 &= \sum_x \sum_y x p(x, y) + \sum_x \sum_y y p(x, y) \qquad \text{(distribute $p(x, y)$ over the sum)} \\
&= \sum_x x \sum_y p(x, y) + \sum_y y \sum_x p(x, y) \: \: \quad \text{(move term outside the inner sum)} \\
&= \sum_x x p_X(x) + \sum_y y p_Y(y) \qquad \qquad \quad \text{(definition of marginal distribution)} \\
&= E[X] + E[Y]. \qquad \qquad \qquad \qquad \qquad\text{(definition of expected value)}
\end{align*}}
$$
:::

## Linearity of Expectation

:::{style="font-size: .8em"}
__Remark__: Suppose we have $n$ random variables with the same distribution. Then 

$$ E\left[\sum_i X_i\right] = E[X_1] + E[X_2] +\dots + E[X_n] = n E[X_1] $$

That is, if you have $n$ random variables, and each has mean $\mu$, then the mean of the sum is $n\mu$.
:::

## Linearity of Expectation

:::{style="font-size: .8em"}
__Example__: Suppose two people are playing Roulette. Note that in Roulette, 18 of the 38 numbers are red. They each first bet on red three times in a row. The first player leaves, but the second player bets two more times on red. How many more times is player 2 expected to win than player one?

Note that the number of times player 2 wins is not independent from the number of times player 1 wins, because every time player 1 wins, player 2 also wins.

:::{.center-text}
<img src="images/sums of random variables/roulette.png" width=300/>
:::

:::

## Linearity of Expectation

:::{style="font-size: .8em"}
__Example (continued)__:
Let $X$ be the number of times player one wins and $Y$ be the number of times player 2 wins, we want to calculate $E[Y-X].$ 

Then 

$$\small{E[Y−X]=E[Y]+E[−1⋅X]=E[Y]+(−1)E[X]=E[Y]−E[X].}$$

$X$ is Binomial with $N=3$ and $p=18/38.$ Similarly, $Y$ is Binomial with $N=5$ and $p=18/38.$

We saw in an earlier lecture that the expect value of a Binomail is $Np$ so putting it all together: 

$$\small{E[Y−X]=E[Y]−E[X]=5*18/38−3*18/38=2*18/38 = 18/19.}$$

<span style="color:rgb(1, 180, 180);">Is it possible to solve this problem in a different way?</span>
:::

## Covariance

:::{style="font-size: .8em"}
Using linearity of expectation we can prove some useful equations for calculating variance and covariance.

$$ 
\begin{align*} 
\operatorname{Cov}(X, Y) & = E[(X−E[X])(Y−E[Y])] \: \: \qquad \qquad \qquad \qquad  \text{(definition)}\\
& \\
 & = E[XY−XE[Y]−E[X]Y+E[X]E[Y]] \qquad \quad \text{(expansion)}\\
 & \\
 & = E[XY]−E[XE[Y]]−E[E[X]Y]+ E[E[X]E[Y]]  \quad  \: \text{(LOE)} \\
  & \\
 & = E[XY]−E[X]E[Y]−E[X]E[Y]+E[X]E[Y] \: \: \: \: \: \: \text{(LOTUS)}\\
  & \\
 & = E[XY]−E[X]E[Y].
\end{align*} 
$$

:::

## Covariance

:::{style="font-size: .8em"}
__Example__: Recall the group question from last lecture. $\small X$ and $\small Y$ are discrete random variables. Both $\small X$ and $\small Y$ can assume values 0, 1, and 2. The incomplete table above is meant to depict the joint distribution of $\small X$ and $\small Y$, and the marginal distributions of $\small X$ and $\small Y.$ Let us compute $\small \operatorname{Cov}(X,Y)$.

:::{.center-text}
<img src="images/sums of random variables/covariance_table.png" width=700/>
:::

- $\small E[X] = E[Y] = (0+1+2)\cdot\frac{1}{3} = 1.$
- $\small E[XY] = 1 \cdot 1 \cdot \frac{4}{36} + 1\cdot 2 \cdot \frac{8}{36} + 2 \cdot 1 \cdot \frac{2}{36} + 2 \cdot 2 \cdot \frac{3}{36} = 1.$
- $\small \operatorname{Cov}(X,Y) = E[XY]−E[X]E[Y] = 1 - 1 = 0.$

:::

:::{.notes}
Earlier $\operatorname{Cov}(X_1, X_2) = E[(X_1 - \overline{X_1})(X_2 - \overline{X_2})]=$
$= \sum_{(x_1,x_2)\in S} (x_1 - \overline{X_1})(x_2 - \overline{X_2}) p(x_1,x_2)$
:::

## Variance and Covariance

:::{style="font-size: .8em"}
From $\small \operatorname{Cov}(X, Y) = E[XY]−E[X]E[Y]$ and $\small \operatorname{Cov}(X, X) = \operatorname{Var}(X),$ it follows that $$ \small{\operatorname{Var}(X) = E[X^2] - E[X]^2.} $$

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> Properties of Variance and Covariance </span>
        <p>
       $$\small{\operatorname{Cov}(X, Y) = E[XY]−E[X]E[Y],}$$
       $$\small{\operatorname{Var}(X) = E[X^2] - E[X]^2,}$$
       $$\small{\operatorname{Var}(aX+b) = a^2\operatorname{Var}(X), \text{ where } a \text{ and } b \text{ are numbers},}$$
       $$\small{\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2 \operatorname{Cov}(X, Y).}$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
__Remark__: We will prove the last property as a group exercise.
:::

## Weak Law of Large Numbers
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> The Weak Law of Large Numbers </span>
        <p>
        Let \(\small{X_1,X_2,...,X_i, ...}\) be a sequence of independent random variables with \(\small{E[X_i] = \mu}\) and \(\operatorname{Var}(X_i) = \sigma^2.\) Let \(\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i.\) Then, for any \(\varepsilon >0,\)

    $$\small{P(|\overline{X}_n - \mu| \geq \varepsilon) \to 0 \quad \text{ as } n \to \infty.}$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
<!-- __Example__: It is commonly believed that if a fair coin is tossed many times and the proportion of heads is calculated, that proportion will be close to $\frac{1}{2}$.   -->

__Example__: It is commonly believed that if a fair coin is tossed many times and the proportion of heads is calculated, that proportion will be close to 50%. 

 Imagine we are repeatedly flipping a certain number of coins at time and keep track of the number of head counts. The results can look as follows.
:::

:::{.notes}
The Weak Law of Large Numbers says that for any small non-negative number 
$\varepsilon,$ the probability that the average $X_n$
differs from the true mean $\mu$
by more than $\varepsilon$ goes to zero as $n$ becomes very large.

Mathematician John Kerrich tested this belief empirically while detained as a prisoner during World War II. He tossed a coin 10,000 times and observed heads 5067 times
:::

## Weak Law of Large Numbers
:::{style="font-size: .8em"}

__Example (continued)__:

- **10 coins** per trial:  
  `4, 4, 5, 6, 5, 4, 3, 3, 4, 5, 5, 9, 3, 5, 7, 4, 5, 7, 7, 9`  
  → Wide variability (30%–90% heads)

- **100 coins** per trial:  
  `46, 54, 48, 45, 52, 49, 47, 60, 40, 57, 46, 45, 51, 52...`  
  → Narrower range (40%–60%)

- **1000 coins** per trial:  
  `486, 501, 489, 472, 537, 474, 508, 510, 478, 508, 493,...`  
  → Tight clustering near 500 heads (47.2%–53.7%)


As the number of coins per trial increases, the proportion of heads converges toward 50%.  


:::

:::{.notes}
Connect to brain cancer!
:::

## Central Limit Theorem
:::{style="font-size: .8em"}


The Weak Law of Large Numbers tells us that the average of many random values tends to get close to the true mean.

The Central Limit Theorem (CLT) goes further: it describes how the average fluctuates around the mean.



```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> The Central Limit Theorem (CLT) </span>
        <p>
        Let \(\small X_1, X_2, ...\) be a sequence of independent and identically distributed random variables, each having mean \(\small \mu\) and variance \( \small \sigma^2.\) Let \( \small S_n = \sum_{i=1}^n X_i.\) Then 

    $$\small \frac{S_n - n\mu}{\sigma\sqrt{n}}$$

    tends to the standard normal distribution as \(\small n \to \infty.\)
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

:::


## Classifying Songs
:::{style="font-size: .8em"}
:::{.columns}
::: {.column width="70%"}
Let $X_i$ be the time that it takes to process song $i$, then 

$$\small{S_{45} = \sum_{i=1}^{45} X_i}$$

is the time it takes to classify the first 45 exams. 
:::

::: {.column width="30%"}
:::{.center-text}
<img src="images/sums of random variables/Anush.jpeg" width=300/>
:::
:::
:::

From CLT it follows that $S_{45}$ is approximately normally distributed with 
$$\small{E[S_{45}] = E\left[\sum_{i=1}^{45} X_i \right] = 45\cdot 3 = 135,}$$

$$\small{\text{Var}[S_{45}] = \text{Var}\left(\sum_{i=1}^{45} X_i \right) = 45\cdot 0.5^2 = 11.25.}$$

:::

## Group Question 1
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    You roll a 4-sided die and a 6-sided die.  <br>
    a. What is the expected value of the sum of two die rolls?
    <br>
    <br>
    <br>
    <br> 
    b. What is the variance of the sum of the two die rolls?
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.notes}
Q5 HW 3 Fall 25
:::

## Group Question 2
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
    Use \(\small{\operatorname{Var}(X) = E[X^2] - E[X]^2}\) to show that \(\small{\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2 \operatorname{Cov}(X, Y).}\)
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

