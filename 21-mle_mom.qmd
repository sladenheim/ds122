---
title: MLE and Method of Moments
author: "CDS DS-122<br>Boston University"
format: 
    revealjs:
        math: true
        css: 
        - styles.css
        html-math-method: mathjax
        highlight-style: github
        slide-number: true
        show-slide-number: all
        chalkboard: true 
---


## Learning Objectives
:::{style="font-size: .8em"}

- Applying MLE to the horse-kicking example
- The definition of the $k$th moment
- The definition of the $k$th sample moment
- The method of moments (for one variable)

:::

## Train Problem
:::{style="font-size: .8em"}
A railroad gives each train a number starting from 1 up to some number ùëÅ. One day, you see a train with the number 60. 

:::{.center-text}
<img src="images/train/train60.jpeg" width=400/>
:::

__Question__: Estimate how many trains the railroad has using the method of moments.
:::

## MLE
:::{style="font-size: .8em"}
__Example__: Recall that Bortkiewicz studied deaths by horse-kick in the Prussian army over 200 years to see if they occurred at a constant rate. He modeled the data using a Poisson distribution, estimating the parameter $\lambda.$  

```{python}
import pandas as pd
import numpy as np

# note that this data is available in 'data/HorseKicks.txt'
horse_kicks = pd.DataFrame(
data = np.array([
[0, 108.67, 109],
[1, 66.29, 65],
[2, 20.22, 22],
[3, 4.11, 3],
[4, 0.63, 1],
[5, 0.08, 0],
[6, 0.01, 0]]),
columns = ["Deaths Per Year","Predicted Instances (Poisson)","Observed Instances"])
horse_kicks["Deaths Per Year"] = horse_kicks["Deaths Per Year"].astype('int')
horse_kicks["Observed Instances"] = horse_kicks["Observed Instances"].astype('int')
horse_kicks[["Deaths Per Year","Observed Instances"]].style.hide(axis='index')
```
:::

## MLE
:::{style="font-size: .8em"}
__Example (continued)__: We are interested in one-year intervals. The parameter $\lambda$ is the rate of deaths per year.

Thus, the likelihood of a particular number of deaths $x_i$ in year $i$ is:

$$ p(x_i; \lambda) = \lambda^{x_i} \frac{e^{- \lambda}}{x_i!}$$

The likelihood function then becomes

$$p(X_s;\lambda) = \prod_{i=1}^{m} \lambda^{x_i} \frac{e^{- \lambda}}{x_i!}.$$

:::

## MLE
:::{style="font-size: .8em"}
__Example (continued)__: Note that the log-likelihood of a particular number of deaths $x_i$ in year $i$ is equal to

$$ \log p(x_i; \lambda) = \log \left(\lambda^{x_i} \frac{e^{- \lambda}}{x_i!}\right) = x_i \log \lambda - \lambda - \log x_i! $$

Using this we can express the log-likelihood of the data as
    
$$ \log p(X_s; \lambda) = \sum_{i=0}^m \left( x_i \log \lambda - \lambda - \log x_i!\right) $$
:::

## MLE
:::{style="font-size: .8em"}
__Example (continued)__: Which looks like this as we vary $\lambda$:

```{python}
from scipy.stats import poisson
import matplotlib.pyplot as plt

# assumes data is a list of counts for various values starting at zero
def ll(data, lam):
    return np.sum(data * poisson.logpmf(range(len(data)), lam))

xvals = np.linspace(0.01, 3, 1000)
ll_vals = [ll(horse_kicks['Observed Instances'], xval) for xval in xvals]

fig, ax = plt.subplots(1, 1, figsize = (14,3))
plt.plot(xvals, ll_vals, lw = 3, color = 'blue')
plt.ylabel('Log-Likelihood', size = 16)
plt.xlabel(r'$\lambda$', size = 16)
ax.tick_params(axis='both', which='major', labelsize=14)
plt.title('Log-Likelihood of Poisson Model for Horse-Kick Data', size = 18);
```
To find the maximum of the log-likelihood of the data as a function of $\lambda$, we need to compute the derivative w.r.t. $\lambda$.
:::

## MLE
:::{style="font-size: .8em"}
__Example (continued)__: 
$$\small{\frac{\partial}{\partial\lambda}\log p(X_s; \lambda) = \frac{\partial}{\partial\lambda} \sum_{i=0}^m \left( x_i \log \lambda - \lambda - \log x_i!\right)}$$

$$\small{=\sum_{i=0}^m \left(x_i \frac{1}{\lambda} - 1 \right)
= \frac{1}{\lambda}\sum_{i=0}^m x_i - m.}$$

Setting the derivative to zero, we get the expression for the ML estimator 

$$\small{\frac{1}{{\lambda}}\sum_{i=0}^m x_i - m = 0,}$$

$$\small{{\hat{\lambda}} = \frac{1}{m}\sum_{i=0}^m x_i.}$$

:::

## MLE
:::{style="font-size: .8em"}
__Example (continued)__:  This is just the mean of the data, that is, the average number of deaths per year! From the available data we can compute that the mean is 0.61. The plot confirms that 0.61 is indeed a maximum. Therefore, the MLE of $\lambda$, $\hat{\lambda}$ = 0.61.

```{python}
# assumes data is a list of counts for various values starting at zero
def ll(data, lam):
    return np.sum(data * poisson.logpmf(range(len(data)), lam))

xvals = np.linspace(0.01, 3, 1000)
ll_vals = [ll(horse_kicks['Observed Instances'], xval) for xval in xvals]

mle = np.sum((horse_kicks['Observed Instances'] 
    * np.array(range(len(horse_kicks['Observed Instances'])))) /
       np.sum(horse_kicks['Observed Instances']))

fig, ax = plt.subplots(1, 1, figsize = (14,3))
plt.plot(xvals, ll_vals, lw = 3, color = 'blue')

ymin = np.min(ll_vals)
ax.set_ylim(ymin = ymin)
mle_ll = np.sum(horse_kicks['Observed Instances'] *  
                     poisson.logpmf(range(len(horse_kicks['Observed Instances'])), mle))
plt.vlines(x = mle, ymin = ymin, ymax = mle_ll, linestyles = 'dashed', color = 'g')
plt.plot(mle, ymin, 'o', color = 'g', markersize = 14, clip_on = False)
plt.text(mle+0.05, ymin+20, r'$\hat{\lambda} = 0.61$', size = 16, ha = 'left', va = 'bottom')

plt.ylabel('Log-Likelihood', size = 16)
plt.xlabel(r'$\lambda$', size = 16)
plt.title('MLE of $\lambda$ for Poisson Model applied to Horse-Kick Data', size = 18);
```

The plot confirms that 0.61 is indeed a maximum. Therefore, the MLE for $\lambda$, $\hat{\lambda}$ = 0.61.

:::

## MLE
:::{style="font-size: .8em"}
__Example (continued)__: 
Using this estimate for $\lambda$, we can ask what the expected number of deaths per year would be, if deaths by horse-kick really followed the assumptions of the Poisson distribution (ie, happening at a fixed, constant rate):

:::{.center-text}
```{python}
# note that this data is available in 'data/HorseKicks.txt'
horse_kicks = pd.DataFrame(
data = np.array([
[0, 108.67, 109],
[1, 66.29, 65],
[2, 20.22, 22],
[3, 4.11, 3],
[4, 0.63, 1],
[5, 0.08, 0],
[6, 0.01, 0]]),
columns = ["Deaths Per Year","Predicted Instances (Poisson)","Observed Instances"])
horse_kicks["Deaths Per Year"] = horse_kicks["Deaths Per Year"].astype('int')
horse_kicks["Observed Instances"] = horse_kicks["Observed Instances"].astype('int')
horse_kicks[["Deaths Per Year","Observed Instances"]].style.hide(axis='index')
horse_kicks[["Predicted Instances (Poisson)","Observed Instances"]].plot.bar(figsize = (10,4))
plt.xlabel("Number of Deaths Per Year", size=14)
plt.ylabel("Count", size=14);
```
:::
Which shows that the Poisson model is indeed a very good fit to the data!
:::

## MLE
:::{style="font-size: .8em"}
__Example (continued)__: 
From this, Bortkeiwicz concluded that there was nothing particularly unusual about the years when there were many deaths by horse-kick.  They could be just what is expected if deaths occurred at a constant rate.
:::

## Moments
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> \(k\)th Moment </span>
        <p>
        
    Let \(X\) be a random variable. Then, the \(k\)th moment of \(X\) is defined as

    $$\mu_k = E[X^k].$$ 
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
The $k$th moment of $X$ about scalar $c$ is equal to

$$E[(X-c)^k].$$

Usually, we are interested in the first moment of $X$: $\mu_1 = E[X]$, and the second moment of $X$ about $\mu_1$: $\operatorname{Var}(X) = E[(X ‚àí \mu_1)^2.$
:::

## Moments
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="purple-box">
     <span class="label"> \(k\)th Sample Moment </span>
        <p>
        
    Let \(X\) be a random variable and \(x_1, x_2, ..., x_m\) be i.i.d. realizations from \(X\). Then, the \(k\)th sample moment of \(X\) is defined as

    $$\hat{\mu}_k = \frac{1}{m} \sum_{i=1}^m x_i^k.$$
        </p>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```

The $k$th sample moment of $X$ about scalar $c$ is equal to

$$\frac{1}{m} \sum_{i=1}^m (x_i-c)^k.$$

For example, the first sample moment is just the sample mean. The second sample moment about the sample mean is the sample variance.
:::

## Method of Moments
:::{style="font-size: .8em"}
Suppose we only need to estimate one parameter $\theta$. 

The method of moments assumes that to find a good estimator, we should have the true and sample moments match as best we can. That is, we should choose the parameter $\theta$ such that the first true moment $\mu_1$ is equal to the first sample moment $\hat{\mu}_1$.

__Example__: Let $x_1, x_2, ..., x_m$ be an i.i.d. sample from a continuous uniform distribution on $(0,\theta)$. What is the method of moments estimate of $\theta$?

Let $X$ be continuous random variable that is uniformly distributed on $(0,\theta)$. Then $x_1, x_2, ..., x_m$ are i.i.d. realizations from $X$. 
:::

## Method of Moments
:::{style="font-size: .8em"}
__Example (continued)__:
We know that $E[X] = \frac{0+\theta}{2} = \frac{\theta}{2}.$ Thus, $\frac{\theta}{2}$ is the first true moment. 

The method of moments approach requires us to match this value with the first sample moment:

$$\frac{\theta}{2} = \frac{1}{m} \sum_{i=1}^m x_i.$$

Solving for $\theta$ we obtain 

$$\hat{\theta} = \frac{2}{m} \sum_{i=1}^m x_i.$$

:::

## Method of Moments
:::{style="font-size: .8em"}
__Example__: A data science team is analyzing user engagement in a mobile app. To understand how frequently users interact with a specific feature (e.g., a notification panel), they collect data from 23 randomly selected users. For each user, they count how many times the feature was accessed over the course of one week. The weekly counts are:

\begin{align*}
& 31 \quad   29  \quad   19  \quad   18  \quad    31  \quad   28 \\
& 34 \quad   27  \quad   34  \quad   30  \quad    16  \quad   18 \\
& 26 \quad   27  \quad   27  \quad   18  \quad    24   \quad  22 \\
& 28  \quad  24  \quad   21  \quad   17  \quad   24
\end{align*}

The Poisson distribution would be a plausible model for describing the variability from grid square to grid square in this situation and could be used to characterize the inherent variability in future measurements.

:::

## Method of Moments
:::{style="font-size: .8em"}
__Example (continued)__: Since for the Poisson distribution with parameter $\theta$ the expected value is also $\theta$, we find that 

$$\mu_1 = E[X] = \theta.$$

The method of moments tells us that $\theta = \mu_1 = \hat{\mu}_1$, where $\hat{\mu}_1$ is the sample mean. That is, to find the method of moments estimate of $\theta$ we simply need to find the mean of the counts.

```{python}
#| echo: true
counts = [31, 29, 19, 18, 31, 28, 34, 27, 34, 30, 16, 18, 26, 27, 27, 18,
          24, 22, 28, 24, 21, 17, 24]
mean_counts = np.mean(counts)

print(f"The mean of the given counts is {mean_counts:.3g}.")
```
Thus, the estimate of $\theta$ for this particular dataset is simply 24.9.
:::

## Train Problem
:::{style="font-size: .8em"}
Let $X$ represent the train number. Then $X$ has a discrete uniform distribution on $[1,N]$. 

- True moment:  $\mu = E[X] = \frac{1 + N}{2}$

- Sample moment:  $\hat{\mu} = \frac{60}{1} = 60$

- Match moments: $\frac{1 + N}{2} = 60 \Rightarrow N = 119$

_Estimated number of trains: 119_


<!-- $\mu = E[X] = \frac{N+1}{2}.$ 
$\hat{\mu} = \frac{60}{1} = 60.$
$\frac{N+1}{2} = 60$. Thus, $\hat{N} = 119.$\\ -->
:::

## Group Quesiton 1
:::{style="font-size: .8em"}

| $x$ | $1$ | $2$ |
| :---: | :---: | :---: |
| $p(x;\theta)$ | $\theta$ | $1-\theta$|
```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
   Suppose that \(X\) is a discrete random variable with the probability mass function given by the table above.<br>

Three independent oservations are made from this distribution:
\(x_1 = 1, x_2 = 2, x_3 = 2.\)<br>

    a. Find the first true moment.<br>
    <br>
    <br>
    <br>
    b. Find the first sample moment.
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::

## Group Quesiton 1
:::{style="font-size: .8em"}

```{python}
from IPython.core.display import HTML

def generate_html():
    return r"""
    <div class="blue-box">
        <p>
   c. Find the method of moments estimate of \(\theta.\) 
       <br>
    <br>
    <br>
        <br>
    <br>
    <br>
    </div>
    """
html_content = generate_html()
display(HTML(html_content))
```
:::
